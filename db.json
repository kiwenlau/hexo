{"meta":{"version":1,"warehouse":"1.0.3"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0},{"_id":"source/image/151128/single-kubernetes-docker.png","path":"image/151128/single-kubernetes-docker.png","modified":0},{"_id":"source/image/150918/kiwenlau_single_mesos.graffle","path":"image/150918/kiwenlau_single_mesos.graffle","modified":0},{"_id":"source/image/150918/hello.png","path":"image/150918/hello.png","modified":0},{"_id":"source/image/150918/architecuture.png","path":"image/150918/architecuture.png","modified":0},{"_id":"source/image/150918/Mesos.png","path":"image/150918/Mesos.png","modified":0},{"_id":"source/image/150918/Marathon.png","path":"image/150918/Marathon.png","modified":0},{"_id":"source/image/150626/grep_word.png","path":"image/150626/grep_word.png","modified":0},{"_id":"source/image/150623/tr-sed-newline.png","path":"image/150623/tr-sed-newline.png","modified":0},{"_id":"source/image/150621/python-substring.png","path":"image/150621/python-substring.png","modified":0},{"_id":"source/image/150620/ssh-copy-id.png","path":"image/150620/ssh-copy-id.png","modified":0},{"_id":"source/image/150608/image architecture.jpg","path":"image/150608/image architecture.jpg","modified":0},{"_id":"themes/light/source/kiwenlau.jpg","path":"kiwenlau.jpg","modified":0},{"_id":"themes/light/source/kiwenlau.ico","path":"kiwenlau.ico","modified":0},{"_id":"themes/light/source/js/jquery.imagesloaded.min.js","path":"js/jquery.imagesloaded.min.js","modified":0},{"_id":"themes/light/source/js/gallery.js","path":"js/gallery.js","modified":0},{"_id":"themes/light/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0},{"_id":"themes/light/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0},{"_id":"themes/light/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0},{"_id":"themes/light/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0},{"_id":"themes/light/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0},{"_id":"themes/light/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0},{"_id":"themes/light/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0},{"_id":"themes/light/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0},{"_id":"themes/light/source/css/style.styl","path":"css/style.styl","modified":0},{"_id":"themes/light/source/css/font/fontawesome-webfont.woff","path":"css/font/fontawesome-webfont.woff","modified":0},{"_id":"themes/light/source/css/font/fontawesome-webfont.ttf","path":"css/font/fontawesome-webfont.ttf","modified":0},{"_id":"themes/light/source/css/font/fontawesome-webfont.svg","path":"css/font/fontawesome-webfont.svg","modified":0},{"_id":"themes/light/source/css/font/fontawesome-webfont.eot","path":"css/font/fontawesome-webfont.eot","modified":0}],"Cache":[{"_id":"source/_posts/150620-ssh-copy-id-error.md","shasum":"4011d5afe85c583c0fb564639c541ef41c3da0bb","modified":1442554778000},{"_id":"source/_posts/150608-hadoop-cluster-docker.md","shasum":"b03799f1c93041d334614672f312e4189ba69afc","modified":1443167508000},{"_id":"source/_posts/150626-grep-multiple-word.md","shasum":"c87b595b9b3450392e48ad8852d378fd23e171e1","modified":1443167735000},{"_id":"source/_posts/150623-tr-sed-newline.md","shasum":"36930cd381eafdad62989932ed3e612c2995ac10","modified":1443167710000},{"_id":"source/_posts/150621-python-get-substring.md","shasum":"32c39e8c770c3df18603f9b5983d8200dc34a57e","modified":1443167690000},{"_id":"source/CNAME","shasum":"abcdc15ac03b8fdb087e2ae5c120ef20dd63dcd1","modified":1433732308000},{"_id":"source/_posts/151105-compile-hadoop-ubuntu.md","shasum":"4f039d3410636892f126871a8e884f02a2f8ff56","modified":1446881339000},{"_id":"source/_posts/150918-single-mesos-docker.md","shasum":"edba8c5a27c473c84f46e22adf28c3059be1399a","modified":1443167326000},{"_id":"source/_posts/151104-macbook-virtualbox-ubuntu-share-fold.md","shasum":"e0f9894a04526c2f41376487cd393c6f6a5bddb5","modified":1446568416000},{"_id":"source/_posts/151104-docker-registry-in-docker-container.md","shasum":"c0d80b8dc5b95f45bdb1d63537cdf7cd23c76c07","modified":1446635994000},{"_id":"source/_posts/151107-macbook-install-openmpi.md","shasum":"f57bf0d1b5c44bc032ba62af68a90c68427f8f17","modified":1446885397000},{"_id":"source/_posts/151107-macbook-ssh-virtualbox-ubuntu.md","shasum":"11bbc4508206443b22ea69863796b1bdd5c063da","modified":1446885420000},{"_id":"source/_posts/151107-install-docker-macbook.md","shasum":"9a0a62d6ade773018343c096a4c6ceb784883959","modified":1446885413000},{"_id":"source/_posts/151105-install-docker-registry-python-ubuntu.md","shasum":"9c4999e3fe674edc47c9ac548d91fc1d73d2bd95","modified":1446725307000},{"_id":"source/_posts/151109-ssh-ubuntu-root.md","shasum":"62557eaa7ceee3a93f62c9da06bd74c11ee3fc38","modified":1446999450000},{"_id":"source/_posts/151109-ubuntu-static-ip.md","shasum":"eb62d0c704dbb7e697f4d8971420b3ed574ee489","modified":1447063069000},{"_id":"source/_posts/151128-single-kubernetes-docker.md","shasum":"5b5347d638fff2646184fa77b11f4cd3dc075ed9","modified":1448624919000},{"_id":"source/google6c2fe14874348911.html","shasum":"13b1c75c0ffea7dd64b61e99c75e328e81812700","modified":1433934386000},{"_id":"source/image/150620/ssh-copy-id.png","shasum":"7cfa4736885fd48039e14eb57130c1d7278f7254","modified":1434791520000},{"_id":"source/image/150621/python-substring.png","shasum":"fe43b66a9024cc27c634badcb783b03fbbf5fc9b","modified":1434872276000},{"_id":"source/image/150608/image architecture.jpg","shasum":"450f2dffe1127f102221a9b3c934eb6054876d85","modified":1433822708000},{"_id":"source/image/150626/grep_word.png","shasum":"3db90e0fc83c8e1e2aad60b3a09d72ccd35513c8","modified":1435279172000},{"_id":"source/image/150623/tr-sed-newline.png","shasum":"ef261eaa115e62ae593299bd54b004b52302c9a9","modified":1435053026000},{"_id":"source/image/150918/architecuture.png","shasum":"43b51d2b17bb54c7c11530d40f1068a558ebde10","modified":1442408879000},{"_id":"source/image/150918/kiwenlau_single_mesos.graffle","shasum":"3fcf8e0540f83077eac1114e5b431e64886f19ca","modified":1442407988000},{"_id":"source/image/151128/single-kubernetes-docker.png","shasum":"75ee08265d42589d56dd36ac764d40521af2fe8c","modified":1448502361000},{"_id":"source/image/150918/Marathon.png","shasum":"58abd8a1ba30eedef47649093a37291e29f21e00","modified":1442402866000},{"_id":"source/image/150918/hello.png","shasum":"732ba80324b66a3ff47fafbde89f4c61dfb04a08","modified":1442471270000},{"_id":"themes/light/source/css/_base/utils.styl","shasum":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1433597960000},{"_id":"source/image/150918/Mesos.png","shasum":"c21fb25a6da8801ecb9b6b25888cb88da16790b4","modified":1442402816000},{"_id":"themes/light/README.md","shasum":"aa189c7ff03c60d8fceb009f5fca1a61d8a0ecdf","modified":1433597960000},{"_id":"themes/light/_config.yml","shasum":"c3729dc713e5b3dc3a1d5bf0ac1be76bf997a8bf","modified":1433929122000},{"_id":"themes/light/LICENSE","shasum":"c6f301bc722f0af3a55267a36c1c147aeddc6e46","modified":1433597960000},{"_id":"themes/light/languages/de.yml","shasum":"e076c7f2eb29ebcfb04d94861bf3063c4b08078c","modified":1433597960000},{"_id":"themes/light/languages/default.yml","shasum":"fd7397be7789b43c1c163ab4faf106318811c2a8","modified":1433597960000},{"_id":"themes/light/languages/es.yml","shasum":"de273af604b27812cfd4195e7b7f28ceff2734b3","modified":1433597960000},{"_id":"themes/light/languages/ru.yml","shasum":"35aadf8fdd28aaff8a1c8f50e80201dcf8ce0604","modified":1433597960000},{"_id":"themes/light/languages/zh-CN.yml","shasum":"ca0118e9081b54cc0fca8596660bd6acf4c0308f","modified":1433597960000},{"_id":"themes/light/languages/zh-TW.yml","shasum":"6141b4c7a094c74bd9df7c08908d92b561c1a0c0","modified":1433597960000},{"_id":"themes/light/layout/_partial/archive.ejs","shasum":"7e4f7c2909b1b90241424ea2ff8e7b4761d8360f","modified":1433597960000},{"_id":"themes/light/layout/_partial/article.ejs","shasum":"bcae2ea030e69242d938c33094fc60207bf25e22","modified":1433609506000},{"_id":"themes/light/layout/_partial/comment.ejs","shasum":"c5409a6920f96816506b4e0d5dfae13449021654","modified":1433609464000},{"_id":"themes/light/layout/_partial/footer.ejs","shasum":"61224149335ae515b0c23e27070c11b05d78749d","modified":1434024998000},{"_id":"themes/light/layout/_partial/facebook_comment.ejs","shasum":"3fdc1d0ce9177825e7417635fbc545a35d528d04","modified":1433597960000},{"_id":"themes/light/layout/_partial/google_analytics.ejs","shasum":"7cf0d1f93051bda510d49dab7f684b9d7c6ba58f","modified":1433597960000},{"_id":"themes/light/layout/_partial/header.ejs","shasum":"d9a99aca97d8b41ed907fbf5b25df05da3ffa4f6","modified":1433597960000},{"_id":"themes/light/layout/_partial/head.ejs","shasum":"d892420cbd253bf8707da7affb8f489cebaae6fd","modified":1433941148000},{"_id":"themes/light/layout/_partial/pagination.ejs","shasum":"1206b630a07444e8744365f14ddb26095c925ae1","modified":1433597960000},{"_id":"themes/light/layout/_partial/post/category.ejs","shasum":"be740939c5c2d4ffdbed9557b4e63a590058b476","modified":1433597960000},{"_id":"themes/light/layout/_partial/post/gallery.ejs","shasum":"fafc2501d7e65983b0f5c2b58151ca12e57c0574","modified":1433597960000},{"_id":"themes/light/layout/_partial/post/share.ejs","shasum":"24c04b319f1b19e887c42db961b90a7e0ab26fdc","modified":1433597960000},{"_id":"themes/light/layout/_partial/post/tag.ejs","shasum":"095418df66a27a28cbab16d7cb0d16001b0e23f1","modified":1433597960000},{"_id":"themes/light/layout/_partial/post/title.ejs","shasum":"d7fbc575d35ae68f9045a382c651450e4131f335","modified":1433597960000},{"_id":"themes/light/layout/_partial/sidebar.ejs","shasum":"caf351797a18d03d8ee945ceb9f83785c50c09f9","modified":1433597960000},{"_id":"themes/light/layout/_widget/category.ejs","shasum":"8a2b90dc29661371f060f710668929c3588e15e4","modified":1433597960000},{"_id":"themes/light/layout/_widget/link.ejs","shasum":"a7e48cd1e18b4a0608dc81a6a50674d9cd30e858","modified":1442558797000},{"_id":"themes/light/layout/_widget/recent_posts.ejs","shasum":"8f2f3963bd568c681d7585bb8099fb1b3e1d4c81","modified":1442555459000},{"_id":"themes/light/layout/_widget/search.ejs","shasum":"55c707f3aa7453c305c41898ad22556edd213830","modified":1433597960000},{"_id":"themes/light/layout/_widget/photo.ejs","shasum":"a985f891ccad6a54352c38de34e90e14005c3e04","modified":1442554487000},{"_id":"themes/light/layout/_widget/tagcloud.ejs","shasum":"673b598903e1b2b9d9ea31dc79bab65ef3984348","modified":1433929542000},{"_id":"themes/light/layout/archive.ejs","shasum":"a18842e3d719fe3ca9b977a6995f8facc75c8673","modified":1433597960000},{"_id":"themes/light/layout/_widget/tag.ejs","shasum":"1914db78bea49c333067d79fe7ad9567d2b08d00","modified":1433597960000},{"_id":"themes/light/layout/index.ejs","shasum":"e569d8fe0741a24efb89e44781f9e616da17e036","modified":1433597960000},{"_id":"themes/light/layout/category.ejs","shasum":"9b740fc33f6f028df60f0bc4312bf3ebd03aa8ea","modified":1433597960000},{"_id":"themes/light/layout/layout.ejs","shasum":"72da76881ebf00e71d7cc196f377e37a17ec7a6f","modified":1433597960000},{"_id":"themes/light/layout/page.ejs","shasum":"70cbc9854655773cc6ba84eecaaf330fed430465","modified":1433597960000},{"_id":"themes/light/layout/post.ejs","shasum":"70cbc9854655773cc6ba84eecaaf330fed430465","modified":1433597960000},{"_id":"themes/light/layout/tag.ejs","shasum":"45150a2365768b6b67880193c9264ad2bb4814db","modified":1433597960000},{"_id":"themes/light/source/css/_base/layout.styl","shasum":"1b58c21aa48a8f9f7f811af681ac182dd058e23d","modified":1433597960000},{"_id":"themes/light/source/css/_base/variable.styl","shasum":"c4e07cb7f4ec980553cd19d9d2cd8a1a44d4cd82","modified":1434027874000},{"_id":"themes/light/source/css/_partial/archive.styl","shasum":"072e9b8c5ee9acf95ac7cce9c34706d41e412229","modified":1433597960000},{"_id":"themes/light/source/css/_partial/article.styl","shasum":"09e00a528f2524093660cf96647282aa2b98abbf","modified":1434030084000},{"_id":"themes/light/source/css/_partial/comment.styl","shasum":"a74254a6e713a522134cca4c644fde681c502823","modified":1434028066000},{"_id":"themes/light/source/css/_partial/footer.styl","shasum":"1757872dbdbd09295a625f13e356aa798a8bb308","modified":1433597960000},{"_id":"themes/light/source/css/_partial/index.styl","shasum":"7a8c0ec6ab99a9f8e00c9687aca29d31752424a2","modified":1433597960000},{"_id":"themes/light/source/css/_partial/header.styl","shasum":"0f932c9514d13fea70fe109242e17ee633a2b28a","modified":1434021576000},{"_id":"themes/light/layout/_partial/after_footer.ejs","shasum":"e3fc00ab06a8e41051602b65ef8a4f968a4bf2bb","modified":1433941216000},{"_id":"themes/light/source/css/_partial/sidebar.styl","shasum":"1bdd787d9dc40829dcab26e0e4543c0d2325b5b8","modified":1434022784000},{"_id":"themes/light/source/css/font/fontawesome-webfont.eot","shasum":"d775f599ff3f23be082e6a9604b4898718923a37","modified":1433597960000},{"_id":"themes/light/source/css/_partial/syntax.styl","shasum":"031903fe83aa7d9960bd084f4bb77a6671abcbfa","modified":1443169038000},{"_id":"themes/light/source/css/style.styl","shasum":"c03b2520e4a85b981e29516cadc0a365e6500e3d","modified":1433597960000},{"_id":"themes/light/source/css/font/fontawesome-webfont.woff","shasum":"0612cddf2f835cceffccc88fd194f97367d0b024","modified":1433597960000},{"_id":"themes/light/source/fancybox/blank.gif","shasum":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1433597960000},{"_id":"themes/light/source/fancybox/fancybox_overlay.png","shasum":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1433597960000},{"_id":"themes/light/source/fancybox/fancybox_loading@2x.gif","shasum":"273b123496a42ba45c3416adb027cd99745058b0","modified":1433597960000},{"_id":"themes/light/source/fancybox/fancybox_loading.gif","shasum":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1433597960000},{"_id":"themes/light/source/fancybox/jquery.fancybox.css","shasum":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1433597960000},{"_id":"themes/light/source/fancybox/fancybox_sprite@2x.png","shasum":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1433597960000},{"_id":"themes/light/source/fancybox/fancybox_sprite.png","shasum":"17df19f97628e77be09c352bf27425faea248251","modified":1433597960000},{"_id":"themes/light/source/js/jquery.imagesloaded.min.js","shasum":"4109837b1f6477bacc6b095a863b1b95b1b3693f","modified":1433597960000},{"_id":"themes/light/source/fancybox/jquery.fancybox.pack.js","shasum":"53360764b429c212f424399384417ccc233bb3be","modified":1433597960000},{"_id":"themes/light/source/js/gallery.js","shasum":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed","modified":1433597960000},{"_id":"themes/light/source/css/font/fontawesome-webfont.ttf","shasum":"a9468f6a1fe965fbcaf5a1bd6c11705e2fc5f84c","modified":1433597960000},{"_id":"themes/light/source/css/font/fontawesome-webfont.svg","shasum":"d162419c91b8bab3a4fd327c933a0fcf3799c251","modified":1433597960000},{"_id":"themes/light/source/kiwenlau.ico","shasum":"987d9c1658c3779037d96f49d8779b9bab18440b","modified":1433759880000},{"_id":"themes/light/source/kiwenlau.jpg","shasum":"e163949f9617ee5a0b92ef131f6e2bb57a2c0e36","modified":1433690104000},{"_id":"public/CNAME","modified":1448624341405,"shasum":"abcdc15ac03b8fdb087e2ae5c120ef20dd63dcd1"},{"_id":"public/image/151128/single-kubernetes-docker.png","modified":1448624341411,"shasum":"75ee08265d42589d56dd36ac764d40521af2fe8c"},{"_id":"public/image/150918/kiwenlau_single_mesos.graffle","modified":1448624341415,"shasum":"3fcf8e0540f83077eac1114e5b431e64886f19ca"},{"_id":"public/image/150918/hello.png","modified":1448624341421,"shasum":"732ba80324b66a3ff47fafbde89f4c61dfb04a08"},{"_id":"public/image/150918/architecuture.png","modified":1448624341423,"shasum":"43b51d2b17bb54c7c11530d40f1068a558ebde10"},{"_id":"public/image/150918/Mesos.png","modified":1448624341428,"shasum":"c21fb25a6da8801ecb9b6b25888cb88da16790b4"},{"_id":"public/image/150918/Marathon.png","modified":1448624341432,"shasum":"58abd8a1ba30eedef47649093a37291e29f21e00"},{"_id":"public/image/150626/grep_word.png","modified":1448624341433,"shasum":"3db90e0fc83c8e1e2aad60b3a09d72ccd35513c8"},{"_id":"public/image/150623/tr-sed-newline.png","modified":1448624341435,"shasum":"ef261eaa115e62ae593299bd54b004b52302c9a9"},{"_id":"public/image/150621/python-substring.png","modified":1448624341440,"shasum":"fe43b66a9024cc27c634badcb783b03fbbf5fc9b"},{"_id":"public/image/150620/ssh-copy-id.png","modified":1448624341443,"shasum":"7cfa4736885fd48039e14eb57130c1d7278f7254"},{"_id":"public/image/150608/image architecture.jpg","modified":1448624341447,"shasum":"450f2dffe1127f102221a9b3c934eb6054876d85"},{"_id":"public/kiwenlau.jpg","modified":1448624341451,"shasum":"e163949f9617ee5a0b92ef131f6e2bb57a2c0e36"},{"_id":"public/kiwenlau.ico","modified":1448624341456,"shasum":"987d9c1658c3779037d96f49d8779b9bab18440b"},{"_id":"public/js/jquery.imagesloaded.min.js","modified":1448624341460,"shasum":"4109837b1f6477bacc6b095a863b1b95b1b3693f"},{"_id":"public/js/gallery.js","modified":1448624341462,"shasum":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed"},{"_id":"public/fancybox/jquery.fancybox.pack.js","modified":1448624341463,"shasum":"53360764b429c212f424399384417ccc233bb3be"},{"_id":"public/fancybox/jquery.fancybox.css","modified":1448624341465,"shasum":"5f163444617b6cf267342f06ac166a237bb62df9"},{"_id":"public/fancybox/fancybox_sprite@2x.png","modified":1448624341469,"shasum":"30c58913f327e28f466a00f4c1ac8001b560aed8"},{"_id":"public/fancybox/fancybox_sprite.png","modified":1448624341471,"shasum":"17df19f97628e77be09c352bf27425faea248251"},{"_id":"public/fancybox/fancybox_overlay.png","modified":1448624341473,"shasum":"b3a4ee645ba494f52840ef8412015ba0f465dbe0"},{"_id":"public/fancybox/fancybox_loading@2x.gif","modified":1448624341477,"shasum":"273b123496a42ba45c3416adb027cd99745058b0"},{"_id":"public/fancybox/fancybox_loading.gif","modified":1448624341479,"shasum":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c"},{"_id":"public/fancybox/blank.gif","modified":1448624341481,"shasum":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a"},{"_id":"public/css/style.css","modified":1448624342244,"shasum":"df9188017cc02f2f3eab54caf4ca3f185f756efd"},{"_id":"public/css/font/fontawesome-webfont.woff","modified":1448624342412,"shasum":"0612cddf2f835cceffccc88fd194f97367d0b024"},{"_id":"public/css/font/fontawesome-webfont.ttf","modified":1448624342416,"shasum":"a9468f6a1fe965fbcaf5a1bd6c11705e2fc5f84c"},{"_id":"public/css/font/fontawesome-webfont.svg","modified":1448624342419,"shasum":"d162419c91b8bab3a4fd327c933a0fcf3799c251"},{"_id":"public/css/font/fontawesome-webfont.eot","modified":1448624342422,"shasum":"d775f599ff3f23be082e6a9604b4898718923a37"},{"_id":"public/google6c2fe14874348911.html","modified":1448624342425,"shasum":"12e5058baee67bcef931133e10cf113179e7d1eb"},{"_id":"public/2015/11/28/151128-single-kubernetes-docker/index.html","modified":1448624678214,"shasum":"46d666bb080070ed82e3fcca30a40221fe3cd709"},{"_id":"public/2015/11/09/151109-ubuntu-static-ip/index.html","modified":1448624678229,"shasum":"b05dd3d87947950a78bef723ff67b4212a226ad4"},{"_id":"public/2015/11/09/151109-ssh-ubuntu-root/index.html","modified":1448624678246,"shasum":"0c7aca046164a54c7a4a7adf1b1461a21f319a1b"},{"_id":"public/2015/11/07/151107-macbook-ssh-virtualbox-ubuntu/index.html","modified":1448624678267,"shasum":"36fbb7ea579e9196819cba2c8ac972e706c70c86"},{"_id":"public/2015/11/07/151107-install-docker-macbook/index.html","modified":1448624678282,"shasum":"11861618187564f7e1acdf67c7794a43cfbd091b"},{"_id":"public/2015/11/07/151107-macbook-install-openmpi/index.html","modified":1448624678293,"shasum":"5be9731c8d4012fc8e0869e77a8b0a377ccba055"},{"_id":"public/2015/11/05/151105-compile-hadoop-ubuntu/index.html","modified":1448624678305,"shasum":"7b6b7266d064b6c8e7d5ebbe79c161fabbac7a63"},{"_id":"public/2015/11/05/151105-install-docker-registry-python-ubuntu/index.html","modified":1448624678319,"shasum":"88b0ba70dd6d9cefac2fa47734b63b6979c9128c"},{"_id":"public/2015/11/04/151104-docker-registry-in-docker-container/index.html","modified":1448624678329,"shasum":"cbc4e17d89a526288a2163edee1841713e6274cf"},{"_id":"public/2015/11/04/151104-macbook-virtualbox-ubuntu-share-fold/index.html","modified":1448624678339,"shasum":"5966bfd92c66bbbe8080034f9e662c67ff021397"},{"_id":"public/2015/09/18/150918-single-mesos-docker/index.html","modified":1448624678350,"shasum":"300d299681b5b7b8e86a149b5d83fa280efec993"},{"_id":"public/2015/06/26/150626-grep-multiple-word/index.html","modified":1448624678362,"shasum":"550a506b40b1b4a3cf02b766167e86019964dc00"},{"_id":"public/2015/06/23/150623-tr-sed-newline/index.html","modified":1448624678378,"shasum":"c7bf824250ab57223532dd9f47890e0662b62afe"},{"_id":"public/2015/06/21/150621-python-get-substring/index.html","modified":1448624678389,"shasum":"2f082d15eee580b48c9fa8123a7b6242cb7f1a59"},{"_id":"public/2015/06/20/150620-ssh-copy-id-error/index.html","modified":1448624678400,"shasum":"6a7445ddea979f83af25f882912b8ef86126f7ee"},{"_id":"public/2015/06/08/150608-hadoop-cluster-docker/index.html","modified":1448624678412,"shasum":"783d722931fdf2681e5c1d19c1eccaf63274c0ed"},{"_id":"public/index.html","modified":1448624678513,"shasum":"41688fa3df610d98bae5a5e60d359ab88b3948f3"},{"_id":"public/page/2/index.html","modified":1448624678532,"shasum":"649199f28559ba69e07c9993ca8f588f1302c03b"},{"_id":"public/archives/index.html","modified":1448624678422,"shasum":"a3b3c3832aa6e0ded070af22e407281d601b309b"},{"_id":"public/archives/page/2/index.html","modified":1448624678433,"shasum":"ae4fb85dd1b2a9ce7d4be66efa0301a24a8bd393"},{"_id":"public/archives/2015/index.html","modified":1448624678445,"shasum":"9d7de4997a218ee5c9df2e298422adcefe4742a8"},{"_id":"public/archives/2015/page/2/index.html","modified":1448624678455,"shasum":"a68fa6fa5d4605fc74c63b8a250ab6941b059a95"},{"_id":"public/archives/2015/06/index.html","modified":1448624678465,"shasum":"b16d908f2336a336183dbf1952fe169ae0f4467b"},{"_id":"public/archives/2015/09/index.html","modified":1448624678476,"shasum":"5ae4aa46505e70dd89d824886f0e5acc4ce27cc0"},{"_id":"public/archives/2015/11/index.html","modified":1448624678484,"shasum":"bdcd6334d4439a7702ae6021e7994e95396829a1"},{"_id":"public/tags/Linux/index.html","modified":1448624678543,"shasum":"08bace6499e3a3620ec8b009692ee552de04f123"},{"_id":"public/tags/Kubernetes/index.html","modified":1448624678553,"shasum":"7b90eb529f60fc1cfd23068afdcb9eaa47baf92e"},{"_id":"public/tags/VirtualBox/index.html","modified":1448624678562,"shasum":"f76e6161767434b81c0c18d09ebf4742737a78b6"},{"_id":"public/tags/MPI/index.html","modified":1448624678571,"shasum":"48d1d09ec340e3e13a3edb96f23d34a3a508f686"},{"_id":"public/tags/Docker/index.html","modified":1448624678578,"shasum":"c2fc9589da7951a325be691e73f1013a7d9e4abc"},{"_id":"public/tags/Hadoop/index.html","modified":1448624678589,"shasum":"72a6702100a625024ce692c1dfcf1db14f4c0af4"},{"_id":"public/tags/Mesos/index.html","modified":1448624678599,"shasum":"ca3f3db8a3c345b4f0e93f2c855a835b5322f50c"},{"_id":"public/tags/Marathon/index.html","modified":1448624678611,"shasum":"e11e04b441ca9cce2f8e6971d43a537cb8165a57"},{"_id":"public/tags/Python/index.html","modified":1448624678619,"shasum":"04e8dbdfb4df57537d1c1c5b295c21ba9a99e93a"},{"_id":"public/atom.xml","modified":1448624678622,"shasum":"7da358776fa5132768b35382985dc9f3bc7e4c8a"},{"_id":"public/sitemap.xml","modified":1448624678625,"shasum":"37119b9352c9bfc2b6aa80efbbe182b6e213628f"}],"Category":[],"Data":[],"Page":[{"layout":"false","_content":"google-site-verification: google6c2fe14874348911.html","source":"google6c2fe14874348911.html","raw":"layout: false\n---\ngoogle-site-verification: google6c2fe14874348911.html","date":"2015-10-18T08:02:22.000Z","updated":"2015-06-10T11:06:26.000Z","path":"google6c2fe14874348911.html","title":"","comments":1,"_id":"cihhlj2zj00000x8zbi6xv32e"}],"Post":[{"title":"基于Docker快速搭建单机版Kuberntes","date":"2015-11-28T00:00:00.000Z","_content":"\n**GitHub地址:** [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n\n##1. Kubernetes简介\n\n2006年，Google工程师Rohit Seth发起了Cgroups内核项目。Cgroups是容器实现CPU，内存等资源隔离的基础，由此可见Google其实很早就开始涉足容器技术。而事实上，Google内部使用容器技术已经长达十年，目前谷歌所有业务包括搜索，Gmail，MapReduce等均运行在容器之中。Google内部使用的集群管理系统--Borg，堪称其容器技术的瑞士军刀。\n\n2014年，Google发起了开源容器集群管理系统--Kubernetes，其设计之初就吸取了Borg的经验和教训，并原生支持了Docker。因此，Kubernetees与较早的集群管理系统Mesos和YARN相比，对容器技术尤其是Docker的支持更加原生，同时提供了更强大的机制实现资源调度，负载均衡，高可用等底层功能，使开发者可以专注于开发应用。\n\n与其他集群系统一致，Kubernetes也采用了Master/Slave结构。下表显示了Kubernetes的各个组件及其功能。\n\n| 角色     | 组件               | 功能                                           |\n| ------- |:-----------------: | :--------------------------------------------:|\n| Master  | apiserver          | 提供RESTful接口                                |\n| Master  | scheduler          | 负责调度，将pod分配到Slave节点                   |\n| Master  | controller-manager | 负责Master的其他功能                           |\n| Master  | etde               | 储存配置信息，节点信息，pod信息等                 |\n| Slave   | kubelet            | 负责管理Pod、容器和容器镜像                       |\n| Slave   | proxy              | 将访问Service的请求转发给对应的Pod，做一些负载均衡  |\n| 客户端   | kubectl            | 命令行工具，向apiserver发起创建Pod等请求          |\n\n\n##2. kiwenlau/kubernetes镜像简介\n\n下图显示了我在Ubuntu主机上运行单机版Kubernetes的架构。可知，我一共运行了7个容器，分别运行Kubernetes的各个组件。事实上，Kuberenetes未来的开发目标正是将Kubernetes的各个组件运行到容器之中，这样可以方便Kubernetes的部署和升级。现在我将Kubernetes的各个组件全部运行在容器中必然存在很多问题且很多问题是未知的，因此这个项目仅做学习测试而不宜部署到生产环境中。Kubernetes各个组件容器之间的通信通过docker link实现，其中apiserver与ectd的4001端口进行通信，scheduler，controller-manager，kubelet，proxy以及kubectl与apiserver的8080端口进行通信。\n\n![](/image/151128/single-kubernetes-docker.png)\n\n集群的大致运行流程是这样的: 用户通过kubectl命令向apiserver发起创建Pod的请求; scheduler将创建Pod的任务分配给kubelet；kubelet中包含了一个docker命令行工具，该工具会向Docker deamon发起创建容器的请求; Docker deamon负责下载镜像然后创建容器。\n\n我将Docker deamon运行在Ubuntu主机上，因此Docker daemon所创建的应用容器与Kubernetes各个组件的容器均运行在Ubuntu主机上。docker socket采用volume的形式挂载到kubelet容器内，因此kubelet中的docker命令行工具可以直接与主机上的Docker daemon进行通信。\n\n我是直接将kubernetes发布的各个组件的二进制可执行文件安装在/usr/local/bin目录下，因此，修改Dockerfile中的Kubernetes下载链接的版本号，就可以快速构建其他版本的Kubernetes镜像。另外，仅需修改网络配置，就可以很方便地在多个节点上部署Kubernetes。\n\nkiwenlau/kubernetes:1.0.7镜像版本信息:\n\n- ubuntu: 14.04\n- Kubernetes: 1.0.7\n- ectd: 2.2.1\n\nUbuntu主机版本信息:\n\n- ubuntu: 14.04.3 LTS\n- kernel: 3.16.0-30-generic\n- docker: 1.9.1\n\n\n\n##3. 运行步骤\n\n**1. 安装Docker**\n\nubuntu 14.04上安装Docker: \n\n```\ncurl -fLsS https://get.docker.com/ | sh\n```\n\n其他系统请参考: [https://docs.docker.com/](https://docs.docker.com/)\n\n**2. 下载Docker镜像**\n\n我将kiwenlau/kubernetes:1.07以及其他用到的Docker镜像都放在[灵雀云](http://www.alauda.cn/)\n\n```\nsudo docker pull index.alauda.cn/kiwenlau/kubernetes:1.0.7\nsudo docker pull index.alauda.cn/kiwenlau/etcd:v2.2.1\nsudo docker pull index.alauda.cn/kiwenlau/nginx:1.9.7\nsudo docker pull index.alauda.cn/kiwenlau/pause:0.8.0\n```\n\n**3. 启动Kubernetes**\n\n```sh\ngit clone https://github.com/kiwenlau/single-kubernetes-docker\ncd single-kubernetes-docker/\nsudo chmod +x start-kubernetes-alauda.sh stop-kubernetes.sh\nsudo ./start-kubernetes-alauda.sh\n```\n\n运行结束后进入kubectl容器。容器主机名为kubeclt。可以通过\"exit\"命令退出容器返回到主机，然后可以通过\"sudo docker exec -it kubectl bash\"命令再次进入kubectl容器。\n\n\n**4. 测试Kubernetes**\n\n运行测试脚本，该脚本会启动一个nginx pod。\n\n```\nchmod +x test-kubernetes-alauda.sh\n./test-kubernetes-alauda.sh \n```\n\n输出\n\n```\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n<style>\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n</style>\n</head>\n<body>\n<h1>Welcome to nginx!</h1>\n<p>If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.</p>\n\n<p>For online documentation and support please refer to\n<a href=\"http://nginx.org/\">nginx.org</a>.<br/>\nCommercial support is available at\n<a href=\"http://nginx.com/\">nginx.com</a>.</p>\n\n<p><em>Thank you for using nginx.</em></p>\n</body>\n</html>\n```\n\n\n##3. 参考\n1. [meteorhacks/hyperkube](https://github.com/meteorhacks/hyperkube)\n2. [meteorhacks/kube-init](https://github.com/meteorhacks/kube-init)\n3. [Kubernetes: The Future of Cloud Hosting](https://meteorhacks.com/learn-kubernetes-the-future-of-the-cloud)\n4. [Kubernetes 架构浅析](http://weibo.com/p/1001603912843031387951?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io)\n5. [An Introduction to Kubernetes](https://www.digitalocean.com/community/tutorials/an-introduction-to-kubernetes)\n\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/](http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/)\n***\n\n\n\n\n\n\n\n\n","source":"_posts/151128-single-kubernetes-docker.md","raw":"title: 基于Docker快速搭建单机版Kuberntes\n\ndate: 2015-11-28 09:00:00\n\ntags: [Docker, Kubernetes]\n\n---\n\n**GitHub地址:** [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n\n##1. Kubernetes简介\n\n2006年，Google工程师Rohit Seth发起了Cgroups内核项目。Cgroups是容器实现CPU，内存等资源隔离的基础，由此可见Google其实很早就开始涉足容器技术。而事实上，Google内部使用容器技术已经长达十年，目前谷歌所有业务包括搜索，Gmail，MapReduce等均运行在容器之中。Google内部使用的集群管理系统--Borg，堪称其容器技术的瑞士军刀。\n\n2014年，Google发起了开源容器集群管理系统--Kubernetes，其设计之初就吸取了Borg的经验和教训，并原生支持了Docker。因此，Kubernetees与较早的集群管理系统Mesos和YARN相比，对容器技术尤其是Docker的支持更加原生，同时提供了更强大的机制实现资源调度，负载均衡，高可用等底层功能，使开发者可以专注于开发应用。\n\n与其他集群系统一致，Kubernetes也采用了Master/Slave结构。下表显示了Kubernetes的各个组件及其功能。\n\n| 角色     | 组件               | 功能                                           |\n| ------- |:-----------------: | :--------------------------------------------:|\n| Master  | apiserver          | 提供RESTful接口                                |\n| Master  | scheduler          | 负责调度，将pod分配到Slave节点                   |\n| Master  | controller-manager | 负责Master的其他功能                           |\n| Master  | etde               | 储存配置信息，节点信息，pod信息等                 |\n| Slave   | kubelet            | 负责管理Pod、容器和容器镜像                       |\n| Slave   | proxy              | 将访问Service的请求转发给对应的Pod，做一些负载均衡  |\n| 客户端   | kubectl            | 命令行工具，向apiserver发起创建Pod等请求          |\n\n\n##2. kiwenlau/kubernetes镜像简介\n\n下图显示了我在Ubuntu主机上运行单机版Kubernetes的架构。可知，我一共运行了7个容器，分别运行Kubernetes的各个组件。事实上，Kuberenetes未来的开发目标正是将Kubernetes的各个组件运行到容器之中，这样可以方便Kubernetes的部署和升级。现在我将Kubernetes的各个组件全部运行在容器中必然存在很多问题且很多问题是未知的，因此这个项目仅做学习测试而不宜部署到生产环境中。Kubernetes各个组件容器之间的通信通过docker link实现，其中apiserver与ectd的4001端口进行通信，scheduler，controller-manager，kubelet，proxy以及kubectl与apiserver的8080端口进行通信。\n\n![](/image/151128/single-kubernetes-docker.png)\n\n集群的大致运行流程是这样的: 用户通过kubectl命令向apiserver发起创建Pod的请求; scheduler将创建Pod的任务分配给kubelet；kubelet中包含了一个docker命令行工具，该工具会向Docker deamon发起创建容器的请求; Docker deamon负责下载镜像然后创建容器。\n\n我将Docker deamon运行在Ubuntu主机上，因此Docker daemon所创建的应用容器与Kubernetes各个组件的容器均运行在Ubuntu主机上。docker socket采用volume的形式挂载到kubelet容器内，因此kubelet中的docker命令行工具可以直接与主机上的Docker daemon进行通信。\n\n我是直接将kubernetes发布的各个组件的二进制可执行文件安装在/usr/local/bin目录下，因此，修改Dockerfile中的Kubernetes下载链接的版本号，就可以快速构建其他版本的Kubernetes镜像。另外，仅需修改网络配置，就可以很方便地在多个节点上部署Kubernetes。\n\nkiwenlau/kubernetes:1.0.7镜像版本信息:\n\n- ubuntu: 14.04\n- Kubernetes: 1.0.7\n- ectd: 2.2.1\n\nUbuntu主机版本信息:\n\n- ubuntu: 14.04.3 LTS\n- kernel: 3.16.0-30-generic\n- docker: 1.9.1\n\n\n\n##3. 运行步骤\n\n**1. 安装Docker**\n\nubuntu 14.04上安装Docker: \n\n```\ncurl -fLsS https://get.docker.com/ | sh\n```\n\n其他系统请参考: [https://docs.docker.com/](https://docs.docker.com/)\n\n**2. 下载Docker镜像**\n\n我将kiwenlau/kubernetes:1.07以及其他用到的Docker镜像都放在[灵雀云](http://www.alauda.cn/)\n\n```\nsudo docker pull index.alauda.cn/kiwenlau/kubernetes:1.0.7\nsudo docker pull index.alauda.cn/kiwenlau/etcd:v2.2.1\nsudo docker pull index.alauda.cn/kiwenlau/nginx:1.9.7\nsudo docker pull index.alauda.cn/kiwenlau/pause:0.8.0\n```\n\n**3. 启动Kubernetes**\n\n```sh\ngit clone https://github.com/kiwenlau/single-kubernetes-docker\ncd single-kubernetes-docker/\nsudo chmod +x start-kubernetes-alauda.sh stop-kubernetes.sh\nsudo ./start-kubernetes-alauda.sh\n```\n\n运行结束后进入kubectl容器。容器主机名为kubeclt。可以通过\"exit\"命令退出容器返回到主机，然后可以通过\"sudo docker exec -it kubectl bash\"命令再次进入kubectl容器。\n\n\n**4. 测试Kubernetes**\n\n运行测试脚本，该脚本会启动一个nginx pod。\n\n```\nchmod +x test-kubernetes-alauda.sh\n./test-kubernetes-alauda.sh \n```\n\n输出\n\n```\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n<style>\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n</style>\n</head>\n<body>\n<h1>Welcome to nginx!</h1>\n<p>If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.</p>\n\n<p>For online documentation and support please refer to\n<a href=\"http://nginx.org/\">nginx.org</a>.<br/>\nCommercial support is available at\n<a href=\"http://nginx.com/\">nginx.com</a>.</p>\n\n<p><em>Thank you for using nginx.</em></p>\n</body>\n</html>\n```\n\n\n##3. 参考\n1. [meteorhacks/hyperkube](https://github.com/meteorhacks/hyperkube)\n2. [meteorhacks/kube-init](https://github.com/meteorhacks/kube-init)\n3. [Kubernetes: The Future of Cloud Hosting](https://meteorhacks.com/learn-kubernetes-the-future-of-the-cloud)\n4. [Kubernetes 架构浅析](http://weibo.com/p/1001603912843031387951?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io)\n5. [An Introduction to Kubernetes](https://www.digitalocean.com/community/tutorials/an-introduction-to-kubernetes)\n\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/](http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/)\n***\n\n\n\n\n\n\n\n\n","slug":"151128-single-kubernetes-docker","published":1,"updated":"2015-11-27T11:48:39.000Z","_id":"cihhlj2zr00010x8z68kdaa5d","comments":1,"layout":"post","photos":[],"link":""},{"title":"Ubuntu静态IP配置","date":"2015-11-09T04:00:00.000Z","_content":"\n##1. 修改Ubuntu网络配置文件\n\n```\nsudo vim /etc/network/interfaces\n```\n\n将eth0的配置改为：\n\n```\niface eth0 inet static\naddress 136.187.81.160\nnetmask 255.255.248.0\nnetwork 136.187.80.0\nbroadcast 136.187.87.255\ngateway 136.187.80.1\n```\n\n这些值要根据实际情况配置，本人所在的局域网范围是：\n\n```\n136.187.80.*~136.187.87.*\n```\n\n##2. 配置域名服务器\n\n不配置域名服务器的话Ubuntu不能联网\n\n```\nsudo vim /etc/resolvconf/resolv.conf.d/base\n```\n\n增加以下内容：\n\n```\nnameserver 8.8.8.8\nnameserver 8.8.4.4\n```\n\n##3. 重启Ubuntu, 查看IP\n\n```\nifconfig\n```\n\n##4. 测试网络连接\n\n```\nping google.com\n```\n\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/11/09/151109-ubuntu-static-ip/](http://kiwenlau.com/2015/11/09/151109-ubuntu-static-ip/)\n***\n\n\n\n\n\n\n\n\n","source":"_posts/151109-ubuntu-static-ip.md","raw":"title: Ubuntu静态IP配置\n\ndate: 2015-11-09 13:00:00\n\ntags: [Linux]\n\n---\n\n##1. 修改Ubuntu网络配置文件\n\n```\nsudo vim /etc/network/interfaces\n```\n\n将eth0的配置改为：\n\n```\niface eth0 inet static\naddress 136.187.81.160\nnetmask 255.255.248.0\nnetwork 136.187.80.0\nbroadcast 136.187.87.255\ngateway 136.187.80.1\n```\n\n这些值要根据实际情况配置，本人所在的局域网范围是：\n\n```\n136.187.80.*~136.187.87.*\n```\n\n##2. 配置域名服务器\n\n不配置域名服务器的话Ubuntu不能联网\n\n```\nsudo vim /etc/resolvconf/resolv.conf.d/base\n```\n\n增加以下内容：\n\n```\nnameserver 8.8.8.8\nnameserver 8.8.4.4\n```\n\n##3. 重启Ubuntu, 查看IP\n\n```\nifconfig\n```\n\n##4. 测试网络连接\n\n```\nping google.com\n```\n\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/11/09/151109-ubuntu-static-ip/](http://kiwenlau.com/2015/11/09/151109-ubuntu-static-ip/)\n***\n\n\n\n\n\n\n\n\n","slug":"151109-ubuntu-static-ip","published":1,"updated":"2015-11-09T09:57:49.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cihhlj30800060x8zu75kfmdn"},{"title":"SSH远程登陆Ubuntu的root用户","date":"2015-11-09T03:00:00.000Z","_content":"\n##1.\t关于Ubuntu的root用户\n\nubuntu默认是不开启root用户的。当执行管理权限的命令时，需要在命令前添加sudo，然后输入密码才能执行命令。使用passwd命令可以为root用户设置密码，使用su命令可以切换到root用户。\n\n但是，默认情况下，不能使用SSH远程登陆Ubuntu的root用户\n\n\n##2.\tSSH远程登陆Ubuntu的root用户\n\n```\nsudo vim /etc/ssh/sshd_config\n```\n\n将\n\n```\nPermitRootLogin without-password\n```\n\n改为:\n\n```\nPermitRootLogin yes\n```\n\n重启SSH服务\n\n```\nsudo service ssh restart\n```\n\n这样就可以使用SSH远程登陆Ubuntu的root用户了。\n\n\n***\n\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n\n[http://kiwenlau.com/2015/11/09/151109-ssh-ubuntu-root/](http://kiwenlau.com/2015/11/09/151109-ssh-ubuntu-root/)\n***","source":"_posts/151109-ssh-ubuntu-root.md","raw":"title: SSH远程登陆Ubuntu的root用户\n\ndate: 2015-11-09 12:00:00\n\ntags: [Linux]\n\n---\n\n##1.\t关于Ubuntu的root用户\n\nubuntu默认是不开启root用户的。当执行管理权限的命令时，需要在命令前添加sudo，然后输入密码才能执行命令。使用passwd命令可以为root用户设置密码，使用su命令可以切换到root用户。\n\n但是，默认情况下，不能使用SSH远程登陆Ubuntu的root用户\n\n\n##2.\tSSH远程登陆Ubuntu的root用户\n\n```\nsudo vim /etc/ssh/sshd_config\n```\n\n将\n\n```\nPermitRootLogin without-password\n```\n\n改为:\n\n```\nPermitRootLogin yes\n```\n\n重启SSH服务\n\n```\nsudo service ssh restart\n```\n\n这样就可以使用SSH远程登陆Ubuntu的root用户了。\n\n\n***\n\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n\n[http://kiwenlau.com/2015/11/09/151109-ssh-ubuntu-root/](http://kiwenlau.com/2015/11/09/151109-ssh-ubuntu-root/)\n***","slug":"151109-ssh-ubuntu-root","published":1,"updated":"2015-11-08T16:17:30.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cihhlj30d00080x8zam9lsqr3"},{"title":"MacBook通过SSH登陆VirtualBox中的Ubuntu","date":"2015-11-07T05:00:00.000Z","_content":"\n##1. 虚拟机网卡配置\n\n网卡1：NAT\n\n网卡2：Host-only，界面名称为vboxnet0\n\n##2. 获取vboxnet0的IP地址\n\t\n**在MacBook的终端中执行:**\n\n```\t\nifconfig\n```\n\t\n**运行结果：**\n\n```\nvboxnet0: flags=8943<UP,BROADCAST,RUNNING,PROMISC,SIMPLEX,MULTICAST> mtu 1500\nether 0a:00:27:00:00:00 \ninet 192.168.59.3 netmask 0xffffff00 broadcast 192.168.59.255\n```\n\n可知vboxnet0的IP地址为192.168.59.3\n\t\n##3. 配置Ubuntu的eth1\n\n```\nsudo vim /etc/network/interfaces\n```\n\t \n**增加以下内容：192.168.59.1与vboxnet0的IP地址192.168.59.3对应**\n\n```\nauto eth1\niface eth1 inet static\naddress 192.168.59.1\nnetmask 255.255.255.0\n```\n\t\t\t \n**激活eth1**\n\n```\nsudo ifup eth1\n```\n\t \n**重启虚拟机:**\n\n```\nsudo reboot\n```\n\t \n##4. 在MacBook终端中通过SSH登陆虚拟机Ubuntu\n\n```\nssh liu@192.168.59.1\n```\n\t\n***\n\n##5. 通过配置端口转发\n\n放弃设置端口转发的方法，因为有多个虚拟机时不方便。\n\n在MacBook的终端设置端口转发，把本机的2222端口转发到虚拟机Ubuntu的22端口，也就是ssh的端口（虚拟机Ubuntu需要关机）\n\n```\nVBoxManage modifyvm \"Ubuntu\" --natpf1 \"guestssh,tcp,,2222,,22\"\n```\n\n在MacBook终端中通过SSH连接虚拟机Ubuntu （虚拟机Ubuntu必须开机）\n\n```\nssh -p 2222 liu@127.0.0.1\n```\n\n***\n\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n\n[http://kiwenlau.com/2015/11/07/151107-macbook-ssh-virtualbox-ubuntu/](http://kiwenlau.com/2015/11/07/151107-macbook-ssh-virtualbox-ubuntu/)\n***","source":"_posts/151107-macbook-ssh-virtualbox-ubuntu.md","raw":"title: MacBook通过SSH登陆VirtualBox中的Ubuntu\n\ndate: 2015-11-07 14:00:00\n\ntags: [VirtualBox]\n\n---\n\n##1. 虚拟机网卡配置\n\n网卡1：NAT\n\n网卡2：Host-only，界面名称为vboxnet0\n\n##2. 获取vboxnet0的IP地址\n\t\n**在MacBook的终端中执行:**\n\n```\t\nifconfig\n```\n\t\n**运行结果：**\n\n```\nvboxnet0: flags=8943<UP,BROADCAST,RUNNING,PROMISC,SIMPLEX,MULTICAST> mtu 1500\nether 0a:00:27:00:00:00 \ninet 192.168.59.3 netmask 0xffffff00 broadcast 192.168.59.255\n```\n\n可知vboxnet0的IP地址为192.168.59.3\n\t\n##3. 配置Ubuntu的eth1\n\n```\nsudo vim /etc/network/interfaces\n```\n\t \n**增加以下内容：192.168.59.1与vboxnet0的IP地址192.168.59.3对应**\n\n```\nauto eth1\niface eth1 inet static\naddress 192.168.59.1\nnetmask 255.255.255.0\n```\n\t\t\t \n**激活eth1**\n\n```\nsudo ifup eth1\n```\n\t \n**重启虚拟机:**\n\n```\nsudo reboot\n```\n\t \n##4. 在MacBook终端中通过SSH登陆虚拟机Ubuntu\n\n```\nssh liu@192.168.59.1\n```\n\t\n***\n\n##5. 通过配置端口转发\n\n放弃设置端口转发的方法，因为有多个虚拟机时不方便。\n\n在MacBook的终端设置端口转发，把本机的2222端口转发到虚拟机Ubuntu的22端口，也就是ssh的端口（虚拟机Ubuntu需要关机）\n\n```\nVBoxManage modifyvm \"Ubuntu\" --natpf1 \"guestssh,tcp,,2222,,22\"\n```\n\n在MacBook终端中通过SSH连接虚拟机Ubuntu （虚拟机Ubuntu必须开机）\n\n```\nssh -p 2222 liu@127.0.0.1\n```\n\n***\n\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n\n[http://kiwenlau.com/2015/11/07/151107-macbook-ssh-virtualbox-ubuntu/](http://kiwenlau.com/2015/11/07/151107-macbook-ssh-virtualbox-ubuntu/)\n***","slug":"151107-macbook-ssh-virtualbox-ubuntu","published":1,"updated":"2015-11-07T08:37:00.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cihhlj30f000a0x8zphlej4o2"},{"title":"MacBook上安装OpenMPI","date":"2015-11-07T03:00:00.000Z","_content":"\nOS X版本：10.10.1\n\nOpenMPI版本： 1.8.4\n\n\n##1. 下载源文件包\n\n```\ncurl -O http://www.open-mpi.org/software/ompi/v1.8/downloads/openmpi-1.8.4.tar.gz\n```\n\n##2.\t解压源文件包\n\n```\ntar zxvf openmpi-1.8.4.tar.gz\n```\n\n##3.\t进入OpenMPI文件夹\n\n```\ncd openmpi-1.8.4\n```\n\n##4.\t生成Makefile。\n\n```\n./configure --prefix=/usr/local\n```\n\n##5.\t编译\n\n```\nMACOSX_DEPLOYMENT_TARGET=10.9 make all\n```\n\n注意，直接使用make all命令编译会产生以下错误：\n\n```\ncouldn't understand kern.osversion '14.0.0\n```\n\n##6.\t安装\n\n```\nsudo make install\n```\n\n##7. 测试：\n\n**代码: mpi_hello.c**\n\n```\n#include <mpi.h>\n#include <stdio.h>\n \nint main (int argc, char* argv[])\n{\n  int rank, size;\n \n  MPI_Init (&argc, &argv);      /* starts MPI */\n  MPI_Comm_rank (MPI_COMM_WORLD, &rank);        /* get current process id */\n  MPI_Comm_size (MPI_COMM_WORLD, &size);        /* get number of processes */\n  printf( \"Hello world from process %d of %d\\n\", rank, size );\n  MPI_Finalize();\n  return 0;\n}\n```\n\n\n**编译：**\n\n```\nmpicc mpi_hello.c -o hello\n```\n\n**执行：**\n\n```\nmpirun -np 2 ./hello\n```\n\n**运行结果：**\n\n```\nHello world from process 0 of 2\nHello world from process 1 of 2\n```\n\n***\n\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n\n[http://kiwenlau.com/2015/11/07/151107-macbook-install-openmpi/](http://kiwenlau.com/2015/11/07/151107-macbook-install-openmpi/)\n***","source":"_posts/151107-macbook-install-openmpi.md","raw":"title: MacBook上安装OpenMPI\n\ndate: 2015-11-07 12:00:00\n\ntags: [MPI]\n\n---\n\nOS X版本：10.10.1\n\nOpenMPI版本： 1.8.4\n\n\n##1. 下载源文件包\n\n```\ncurl -O http://www.open-mpi.org/software/ompi/v1.8/downloads/openmpi-1.8.4.tar.gz\n```\n\n##2.\t解压源文件包\n\n```\ntar zxvf openmpi-1.8.4.tar.gz\n```\n\n##3.\t进入OpenMPI文件夹\n\n```\ncd openmpi-1.8.4\n```\n\n##4.\t生成Makefile。\n\n```\n./configure --prefix=/usr/local\n```\n\n##5.\t编译\n\n```\nMACOSX_DEPLOYMENT_TARGET=10.9 make all\n```\n\n注意，直接使用make all命令编译会产生以下错误：\n\n```\ncouldn't understand kern.osversion '14.0.0\n```\n\n##6.\t安装\n\n```\nsudo make install\n```\n\n##7. 测试：\n\n**代码: mpi_hello.c**\n\n```\n#include <mpi.h>\n#include <stdio.h>\n \nint main (int argc, char* argv[])\n{\n  int rank, size;\n \n  MPI_Init (&argc, &argv);      /* starts MPI */\n  MPI_Comm_rank (MPI_COMM_WORLD, &rank);        /* get current process id */\n  MPI_Comm_size (MPI_COMM_WORLD, &size);        /* get number of processes */\n  printf( \"Hello world from process %d of %d\\n\", rank, size );\n  MPI_Finalize();\n  return 0;\n}\n```\n\n\n**编译：**\n\n```\nmpicc mpi_hello.c -o hello\n```\n\n**执行：**\n\n```\nmpirun -np 2 ./hello\n```\n\n**运行结果：**\n\n```\nHello world from process 0 of 2\nHello world from process 1 of 2\n```\n\n***\n\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n\n[http://kiwenlau.com/2015/11/07/151107-macbook-install-openmpi/](http://kiwenlau.com/2015/11/07/151107-macbook-install-openmpi/)\n***","slug":"151107-macbook-install-openmpi","published":1,"updated":"2015-11-07T08:36:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cihhlj30i000d0x8zgqss6i3z"},{"title":"MacBook上安装Docker","date":"2015-11-07T04:00:00.000Z","_content":"\n## 1.\t下载并安装Docker for OS X Installer \n\n**下载地址：**\n\n[https://github.com/boot2docker/osx-installer/releases/download/v1.4.1/Boot2Docker-1.4.1.pkg](https://github.com/boot2docker/osx-installer/releases/download/v1.4.1/Boot2Docker-1.4.1.pkg)\n\n\n##2.\t配置环境变量\n\n```\nvim ~/.bash_profile\n```\n\n**在.bash_profile文件中增加以下环境变量**\n\n```\nexport DOCKER_CERT_PATH=/Users/nii/.boot2docker/certs/boot2docker-vm\nexport DOCKER_TLS_VERIFY=1\nexport DOCKER_HOST=tcp://192.168.59.103:2376\n```\n\n**执行以下命令使设置的环境变量生效**\n\n```\nsource ~/.bash_profile\n```\n\n##3.\t测试Docker\n\n**开启boot2docker虚拟机**\n\n```\nboot2docker start\n```\n\n**运行hello-world程序**\n\n```\ndocker run hello-world\n```\n\n***\n\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n\n[http://kiwenlau.com/2015/11/07/151107-install-docker-macbook/](http://kiwenlau.com/2015/11/07/151107-install-docker-macbook/)\n***","source":"_posts/151107-install-docker-macbook.md","raw":"title: MacBook上安装Docker\n\ndate: 2015-11-07 13:00:00\n\ntags: [Docker]\n\n---\n\n## 1.\t下载并安装Docker for OS X Installer \n\n**下载地址：**\n\n[https://github.com/boot2docker/osx-installer/releases/download/v1.4.1/Boot2Docker-1.4.1.pkg](https://github.com/boot2docker/osx-installer/releases/download/v1.4.1/Boot2Docker-1.4.1.pkg)\n\n\n##2.\t配置环境变量\n\n```\nvim ~/.bash_profile\n```\n\n**在.bash_profile文件中增加以下环境变量**\n\n```\nexport DOCKER_CERT_PATH=/Users/nii/.boot2docker/certs/boot2docker-vm\nexport DOCKER_TLS_VERIFY=1\nexport DOCKER_HOST=tcp://192.168.59.103:2376\n```\n\n**执行以下命令使设置的环境变量生效**\n\n```\nsource ~/.bash_profile\n```\n\n##3.\t测试Docker\n\n**开启boot2docker虚拟机**\n\n```\nboot2docker start\n```\n\n**运行hello-world程序**\n\n```\ndocker run hello-world\n```\n\n***\n\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n\n[http://kiwenlau.com/2015/11/07/151107-install-docker-macbook/](http://kiwenlau.com/2015/11/07/151107-install-docker-macbook/)\n***","slug":"151107-install-docker-macbook","published":1,"updated":"2015-11-07T08:36:53.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cihhlj30k000g0x8z9txob102"},{"title":"在Ubuntu上安装Python版Docker仓库","date":"2015-11-05T03:00:00.000Z","_content":"\n安装Docker仓库的Ubuntu的局域网IP为: 192.168.59.10\n\n##1.\t安装Docker仓库\n\n```\nsudo apt-get update\nsudo apt-get -y install build-essential python-dev libevent-dev python-pip\nliblzma-dev swig libssl-dev\nsudo pip install docker-registry\n```\n\n安装时会有很多warning以及少量error，暂时忽略。\n\n##2.\t测试Docker仓库是否安装成功\n\n```\ndocker-registry --help\n```\n\n输出：\n\n```\nusage: docker-registry [-h]\n\nrun the docker-registry with gunicorn, honoring the following\nenvironment variables:\nREGISTRY_HOST: TCP host or ip to bind to; default is 0.0.0.0\nREGISTRY_PORT: TCP port to bind to; default is 5000\nGUNICORN_WORKERS: number of worker processes gunicorn should start\nGUNICORN_GRACEFUL_TIMEOUT: timeout in seconds for graceful worker restart\nGUNICORN_SILENT_TIMEOUT: timeout in seconds for restarting silent workers\nGUNICORN_USER: unix user to downgrade priviledges to\nGUNICORN_GROUP: unix group to downgrade priviledges to\nGUNICORN_ACCESS_LOG_FILE: File to log access to\nGUNICORN_ERROR_LOG_FILE: File to log errors to\nGUNICORN_OPTS: extra options to pass to gunicorn\n\noptional arguments:\n  -h, --help  show this help message and exit\n```\n\n##3.\t配置Docker仓库\n\n```\ncd /usr/local/lib/python2.7/dist-packages/docker_registry/lib/../../config/\nsudo cp config_sample.yml config.yml\nsudo mkdir -p /var/docker-registry\n```\n\n修改config.yml文件，将/tmp目录改为/var/docker-registry目录\n\n```\nsudo vim config.yml\n```\n\n将：\n\n```\n# SQLite search backend\n    sqlalchemy_index_database: _env:SQLALCHEMY_INDEX_DATABASE:sqlite:////tmp/docker-registry.db\t\n```\n\t\t\t\n改为：\n\n```\n# SQLite search backend\nsqlalchemy_index_database: _env:SQLALCHEMY_INDEX_DATABASE:sqlite:////var/docker-registry/docker-registry.db\n```\n\n将：\n\n```\nlocal: &local\n    <<: *common\n    storage: local\n    storage_path: _env:STORAGE_PATH:/tmp/registry\n```\n\n改为\n\n```\nlocal: &local\n    <<: *common\n    storage: local\n    storage_path: _env:STORAGE_PATH:/var/docker-registry/registry\n```\n\n\n##4. 测试配置是否正确\n\n```\ngunicorn --access-logfile - --debug -k gevent -b 0.0.0.0:5000 -w 1 docker_registry.wsgi:application \n```\n\n输出为以下内容说明应该没错…(虽然有warning)\n\n```\n20/Jan/2015:20:46:27 +0000 WARNING: Cache storage disabled!\n20/Jan/2015:20:46:27 +0000 WARNING: LRU cache disabled!\n20/Jan/2015:20:46:27 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n```\n\n\n##5.\t继续配置\n\n```\nsudo mkdir -p /var/log/docker-registry\nsudo vim /etc/init/docker-registry.conf\n```\n\n在docker-registry.conf文件中写入以下内容：(此处0.0.0.0:5000很关键, 若设为localhost:5000则无法将镜像pull到192.168.59.10: 5000)\n\n```\ndescription \"Docker Registry\"\n\nstart on runlevel [2345]\nstop on runlevel [016]\n\nrespawn\nrespawn limit 10 5\n\nscript\nexec gunicorn --access-logfile /var/log/docker-registry/access.log --error-logfile /var/log/docker-registry/server.log -k gevent --max-requests 100 --graceful-timeout 3600 -t 3600 -b 0.0.0.0:5000 -w 8 docker_registry.wsgi:application\nend script\n```\n\n\n##6.\t启动Dcoker仓库\n\n```\nsudo service docker-registry start\n```\n\n测试Docker仓库\n\n```\ncurl 192.168.59.10:5000\n```\n \n正确输出为(此处若出错应该是docker-registry.conf文件中0.0.0.0:5000设为了localhost:5000)\n\n```\n\"\\\"docker-registry server\\\"\"\n```\n\n\n##7. 安装Docker\n\n首先在本机(Ubuntu 14.04)上进行测试，需要安装Docker\n\n```\nsudo apt-get install apt-transport-https\nsudo apt-key adv --keyserver hkp://keyserver.Ubuntu.com:80 --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9\nsudo bash -c \"echo deb https://get.docker.io/Ubuntu docker main > /etc/apt/sources.list.d/docker.list\"\nsudo apt-get update\nsudo apt-get install -y lxc-docker\n```\n\n查看docker版本信息\n\n```\nsudo docker version\n```\n\n输出:\n\n```\nClient version: 1.4.1\nClient API version: 1.16\nGo version (client): go1.3.3\nGit commit (client): 5bc2ff8\nOS/Arch (client): linux/amd64\nServer version: 1.4.1\nServer API version: 1.16\nGo version (server): go1.3.3\nGit commit (server): 5bc2ff8\n```\n\n\n##8.\t配置Docker\n\n这一步不做push和pull将会出错\n\n```\nsudo vim /etc/default/docker\n```\n\n增加以下一行：\n\n```\nDOCKER_OPTS=\"$DOCKER_OPTS --insecure-registry=192.168.59.10:5000\"\n```\n\n重启Docker\n\n```\nsudo service docker restart\n```\n\n##9. push镜像到Docker仓库中\n\n下载busybox镜像作为测试\n\n```\nsudo docker pull busybox\n```\n\n查看image\n\n```\nsudo docker images\n```\n\nbusybox镜像的ID为4986bf8c1536\n\n```\nREPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox             latest              4986bf8c1536        2 weeks ago         2.433 MB\n```\n\n改变busybox镜像的tag\n\n```\nsudo docker tag 4986bf8c1536 192.168.59.10:5000/busybox:test\n```\n\n将busybox镜像push到本机的Docker仓库中\n\n```\nsudo docker push 192.168.59.10:5000/busybox:test\n```\n\n输出：\n\n```\nThe push refers to a repository [192.168.59.10:5000/busybox] (len: 1)\nSending image list\nPushing repository 192.168.59.10:5000/busybox (1 tags)\n511136ea3c5a: Image successfully pushed \ndf7546f9f060: Image successfully pushed \nea13149945cb: Image successfully pushed \n4986bf8c1536: Image successfully pushed \nPushing tag for rev [4986bf8c1536] on {http://192.168.59.10:5000/v1/repositories/busybox/tags/test}\n```\n\n\n##10. 从Docker仓库中pull镜像\n\n在局域网的另一台机器上安装并配置好Docker后，即可pull刚才push过的image\n\n```\nsudo docker pull 192.168.59.10:5000/busybox:test\n```\n\n正确输出结果为：\n\n```\nPulling repository 192.168.59.10:5000/busybox\n4986bf8c1536: Download complete \n511136ea3c5a: Download complete \ndf7546f9f060: Download complete \nea13149945cb: Download complete \nStatus: Downloaded newer image for 192.168.59.10:5000/busybox:test\n```\n\n***\n\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n\n[http://kiwenlau.com/2015/11/05/151105-install-docker-registry-python-ubuntu/](http://kiwenlau.com/2015/11/05/151105-install-docker-registry-python-ubuntu/)\n***","source":"_posts/151105-install-docker-registry-python-ubuntu.md","raw":"title: 在Ubuntu上安装Python版Docker仓库\n\ndate: 2015-11-05 12:00:00\n\ntags: [Docker]\n\n---\n\n安装Docker仓库的Ubuntu的局域网IP为: 192.168.59.10\n\n##1.\t安装Docker仓库\n\n```\nsudo apt-get update\nsudo apt-get -y install build-essential python-dev libevent-dev python-pip\nliblzma-dev swig libssl-dev\nsudo pip install docker-registry\n```\n\n安装时会有很多warning以及少量error，暂时忽略。\n\n##2.\t测试Docker仓库是否安装成功\n\n```\ndocker-registry --help\n```\n\n输出：\n\n```\nusage: docker-registry [-h]\n\nrun the docker-registry with gunicorn, honoring the following\nenvironment variables:\nREGISTRY_HOST: TCP host or ip to bind to; default is 0.0.0.0\nREGISTRY_PORT: TCP port to bind to; default is 5000\nGUNICORN_WORKERS: number of worker processes gunicorn should start\nGUNICORN_GRACEFUL_TIMEOUT: timeout in seconds for graceful worker restart\nGUNICORN_SILENT_TIMEOUT: timeout in seconds for restarting silent workers\nGUNICORN_USER: unix user to downgrade priviledges to\nGUNICORN_GROUP: unix group to downgrade priviledges to\nGUNICORN_ACCESS_LOG_FILE: File to log access to\nGUNICORN_ERROR_LOG_FILE: File to log errors to\nGUNICORN_OPTS: extra options to pass to gunicorn\n\noptional arguments:\n  -h, --help  show this help message and exit\n```\n\n##3.\t配置Docker仓库\n\n```\ncd /usr/local/lib/python2.7/dist-packages/docker_registry/lib/../../config/\nsudo cp config_sample.yml config.yml\nsudo mkdir -p /var/docker-registry\n```\n\n修改config.yml文件，将/tmp目录改为/var/docker-registry目录\n\n```\nsudo vim config.yml\n```\n\n将：\n\n```\n# SQLite search backend\n    sqlalchemy_index_database: _env:SQLALCHEMY_INDEX_DATABASE:sqlite:////tmp/docker-registry.db\t\n```\n\t\t\t\n改为：\n\n```\n# SQLite search backend\nsqlalchemy_index_database: _env:SQLALCHEMY_INDEX_DATABASE:sqlite:////var/docker-registry/docker-registry.db\n```\n\n将：\n\n```\nlocal: &local\n    <<: *common\n    storage: local\n    storage_path: _env:STORAGE_PATH:/tmp/registry\n```\n\n改为\n\n```\nlocal: &local\n    <<: *common\n    storage: local\n    storage_path: _env:STORAGE_PATH:/var/docker-registry/registry\n```\n\n\n##4. 测试配置是否正确\n\n```\ngunicorn --access-logfile - --debug -k gevent -b 0.0.0.0:5000 -w 1 docker_registry.wsgi:application \n```\n\n输出为以下内容说明应该没错…(虽然有warning)\n\n```\n20/Jan/2015:20:46:27 +0000 WARNING: Cache storage disabled!\n20/Jan/2015:20:46:27 +0000 WARNING: LRU cache disabled!\n20/Jan/2015:20:46:27 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n```\n\n\n##5.\t继续配置\n\n```\nsudo mkdir -p /var/log/docker-registry\nsudo vim /etc/init/docker-registry.conf\n```\n\n在docker-registry.conf文件中写入以下内容：(此处0.0.0.0:5000很关键, 若设为localhost:5000则无法将镜像pull到192.168.59.10: 5000)\n\n```\ndescription \"Docker Registry\"\n\nstart on runlevel [2345]\nstop on runlevel [016]\n\nrespawn\nrespawn limit 10 5\n\nscript\nexec gunicorn --access-logfile /var/log/docker-registry/access.log --error-logfile /var/log/docker-registry/server.log -k gevent --max-requests 100 --graceful-timeout 3600 -t 3600 -b 0.0.0.0:5000 -w 8 docker_registry.wsgi:application\nend script\n```\n\n\n##6.\t启动Dcoker仓库\n\n```\nsudo service docker-registry start\n```\n\n测试Docker仓库\n\n```\ncurl 192.168.59.10:5000\n```\n \n正确输出为(此处若出错应该是docker-registry.conf文件中0.0.0.0:5000设为了localhost:5000)\n\n```\n\"\\\"docker-registry server\\\"\"\n```\n\n\n##7. 安装Docker\n\n首先在本机(Ubuntu 14.04)上进行测试，需要安装Docker\n\n```\nsudo apt-get install apt-transport-https\nsudo apt-key adv --keyserver hkp://keyserver.Ubuntu.com:80 --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9\nsudo bash -c \"echo deb https://get.docker.io/Ubuntu docker main > /etc/apt/sources.list.d/docker.list\"\nsudo apt-get update\nsudo apt-get install -y lxc-docker\n```\n\n查看docker版本信息\n\n```\nsudo docker version\n```\n\n输出:\n\n```\nClient version: 1.4.1\nClient API version: 1.16\nGo version (client): go1.3.3\nGit commit (client): 5bc2ff8\nOS/Arch (client): linux/amd64\nServer version: 1.4.1\nServer API version: 1.16\nGo version (server): go1.3.3\nGit commit (server): 5bc2ff8\n```\n\n\n##8.\t配置Docker\n\n这一步不做push和pull将会出错\n\n```\nsudo vim /etc/default/docker\n```\n\n增加以下一行：\n\n```\nDOCKER_OPTS=\"$DOCKER_OPTS --insecure-registry=192.168.59.10:5000\"\n```\n\n重启Docker\n\n```\nsudo service docker restart\n```\n\n##9. push镜像到Docker仓库中\n\n下载busybox镜像作为测试\n\n```\nsudo docker pull busybox\n```\n\n查看image\n\n```\nsudo docker images\n```\n\nbusybox镜像的ID为4986bf8c1536\n\n```\nREPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox             latest              4986bf8c1536        2 weeks ago         2.433 MB\n```\n\n改变busybox镜像的tag\n\n```\nsudo docker tag 4986bf8c1536 192.168.59.10:5000/busybox:test\n```\n\n将busybox镜像push到本机的Docker仓库中\n\n```\nsudo docker push 192.168.59.10:5000/busybox:test\n```\n\n输出：\n\n```\nThe push refers to a repository [192.168.59.10:5000/busybox] (len: 1)\nSending image list\nPushing repository 192.168.59.10:5000/busybox (1 tags)\n511136ea3c5a: Image successfully pushed \ndf7546f9f060: Image successfully pushed \nea13149945cb: Image successfully pushed \n4986bf8c1536: Image successfully pushed \nPushing tag for rev [4986bf8c1536] on {http://192.168.59.10:5000/v1/repositories/busybox/tags/test}\n```\n\n\n##10. 从Docker仓库中pull镜像\n\n在局域网的另一台机器上安装并配置好Docker后，即可pull刚才push过的image\n\n```\nsudo docker pull 192.168.59.10:5000/busybox:test\n```\n\n正确输出结果为：\n\n```\nPulling repository 192.168.59.10:5000/busybox\n4986bf8c1536: Download complete \n511136ea3c5a: Download complete \ndf7546f9f060: Download complete \nea13149945cb: Download complete \nStatus: Downloaded newer image for 192.168.59.10:5000/busybox:test\n```\n\n***\n\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n\n[http://kiwenlau.com/2015/11/05/151105-install-docker-registry-python-ubuntu/](http://kiwenlau.com/2015/11/05/151105-install-docker-registry-python-ubuntu/)\n***","slug":"151105-install-docker-registry-python-ubuntu","published":1,"updated":"2015-11-05T12:08:27.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cihhlj30r000j0x8zw010zpyz"},{"title":"Hadoop在Ubuntu中编译","date":"2015-11-05T05:00:00.000Z","_content":"\n官网提供的编译好的hadoop-2.3.0.tar.gz二进制包是在32位系统上编译的，在64系统上运行会有一些错误，比如：\n\n```\nWARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n```\n\n此时需要自行编译hadoop 2.30 源码。本人编译的hadoop 2.30的二进制包经实验证明可以正确安装并正确运行了Word Count程序。此处可以下载我编译的二进制包: [http://pan.baidu.com/s/1eQrgsWa](http://pan.baidu.com/s/1eQrgsWa)\n\n**更新软件列表**\n\n```\napt-get update\n```\n\n**安装编译Hadoop所需要的软件**\n\n```\napt-get install -y openjdk-7-jdk libprotobuf-dev protobuf-compiler maven cmake build-essential pkg-config libssl-dev zlib1g-dev llvm-gcc automake autoconf make\n```\n\n**下载hadoop 2.30的源文件包**\n\n```\nwget http://archive.apache.org/dist/hadoop/core/hadoop-2.3.0/hadoop-2.3.0-src.tar.gz \n```\n\n**解压hadoop 2.30 的源文件包**\n\n```\ntar -xzvf hadoop-2.3.0-src.tar.gz \n```\n\n**进入hadoop 2.30 文件夹**\n\n```\ncd hadoop-2.3.0-src\n```\n\n**编译hadoop 2.30 源文件**\n\n```\nmvn package -Pdist,native -DskipTests -Dtar\n```\n\n正确执行的结果如下:\n\n```\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Apache Hadoop Main ................................ SUCCESS [1:11.968s]\n[INFO] Apache Hadoop Project POM ......................... SUCCESS [30.393s]\n[INFO] Apache Hadoop Annotations ......................... SUCCESS [18.398s]\n[INFO] Apache Hadoop Assemblies .......................... SUCCESS [0.246s]\n[INFO] Apache Hadoop Project Dist POM .................... SUCCESS [20.372s]\n[INFO] Apache Hadoop Maven Plugins ....................... SUCCESS [23.721s]\n[INFO] Apache Hadoop MiniKDC ............................. SUCCESS [1:41.836s]\n[INFO] Apache Hadoop Auth ................................ SUCCESS [22.303s]\n[INFO] Apache Hadoop Auth Examples ....................... SUCCESS [7.052s]\n[INFO] Apache Hadoop Common .............................. SUCCESS [2:29.466s]\n[INFO] Apache Hadoop NFS ................................. SUCCESS [11.604s]\n[INFO] Apache Hadoop Common Project ...................... SUCCESS [0.073s]\n[INFO] Apache Hadoop HDFS ................................ SUCCESS [1:30.230s]\n[INFO] Apache Hadoop HttpFS .............................. SUCCESS [17.976s]\n[INFO] Apache Hadoop HDFS BookKeeper Journal ............. SUCCESS [19.927s]\n[INFO] Apache Hadoop HDFS-NFS ............................ SUCCESS [3.304s]\n[INFO] Apache Hadoop HDFS Project ........................ SUCCESS [0.032s]\n[INFO] hadoop-yarn ....................................... SUCCESS [0.033s]\n[INFO] hadoop-yarn-api ................................... SUCCESS [36.284s]\n[INFO] hadoop-yarn-common ................................ SUCCESS [33.912s]\n[INFO] hadoop-yarn-server ................................ SUCCESS [0.213s]\n[INFO] hadoop-yarn-server-common ......................... SUCCESS [8.193s]\n[INFO] hadoop-yarn-server-nodemanager .................... SUCCESS [41.181s]\n[INFO] hadoop-yarn-server-web-proxy ...................... SUCCESS [2.768s]\n[INFO] hadoop-yarn-server-resourcemanager ................ SUCCESS [13.923s]\n[INFO] hadoop-yarn-server-tests .......................... SUCCESS [0.904s]\n[INFO] hadoop-yarn-client ................................ SUCCESS [4.363s]\n[INFO] hadoop-yarn-applications .......................... SUCCESS [0.120s]\n[INFO] hadoop-yarn-applications-distributedshell ......... SUCCESS [2.262s]\n[INFO] hadoop-yarn-applications-unmanaged-am-launcher .... SUCCESS [1.615s]\n[INFO] hadoop-yarn-site .................................. SUCCESS [0.086s]\n[INFO] hadoop-yarn-project ............................... SUCCESS [2.703s]\n[INFO] hadoop-mapreduce-client ........................... SUCCESS [0.132s]\n[INFO] hadoop-mapreduce-client-core ...................... SUCCESS [18.951s]\n[INFO] hadoop-mapreduce-client-common .................... SUCCESS [14.320s]\n[INFO] hadoop-mapreduce-client-shuffle ................... SUCCESS [3.330s]\n[INFO] hadoop-mapreduce-client-app ....................... SUCCESS [9.664s]\n[INFO] hadoop-mapreduce-client-hs ........................ SUCCESS [7.678s]\n[INFO] hadoop-mapreduce-client-jobclient ................. SUCCESS [9.263s]\n[INFO] hadoop-mapreduce-client-hs-plugins ................ SUCCESS [1.549s]\n[INFO] Apache Hadoop MapReduce Examples .................. SUCCESS [5.748s]\n[INFO] hadoop-mapreduce .................................. SUCCESS [2.880s]\n[INFO] Apache Hadoop MapReduce Streaming ................. SUCCESS [7.080s]\n[INFO] Apache Hadoop Distributed Copy .................... SUCCESS [14.648s]\n[INFO] Apache Hadoop Archives ............................ SUCCESS [2.602s]\n[INFO] Apache Hadoop Rumen ............................... SUCCESS [5.706s]\n[INFO] Apache Hadoop Gridmix ............................. SUCCESS [3.649s]\n[INFO] Apache Hadoop Data Join ........................... SUCCESS [2.483s]\n[INFO] Apache Hadoop Extras .............................. SUCCESS [2.678s]\n[INFO] Apache Hadoop Pipes ............................... SUCCESS [6.359s]\n[INFO] Apache Hadoop OpenStack support ................... SUCCESS [5.088s]\n[INFO] Apache Hadoop Client .............................. SUCCESS [4.534s]\n[INFO] Apache Hadoop Mini-Cluster ........................ SUCCESS [0.433s]\n[INFO] Apache Hadoop Scheduler Load Simulator ............ SUCCESS [7.757s]\n[INFO] Apache Hadoop Tools Dist .......................... SUCCESS [4.099s]\n[INFO] Apache Hadoop Tools ............................... SUCCESS [0.428s]\n[INFO] Apache Hadoop Distribution ........................ SUCCESS [18.045s]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 14:59.240s\n[INFO] Finished at: Thu Jan 15 18:51:59 JST 2015\n[INFO] Final Memory: 168M/435M\n[INFO] ------------------------------------------------------------------------\n```\n\n**编译好的二进制文件包位于**\n\n```\nhadoop-2.3.0-src/hadoop-dist/target/hadoop-2.3.0.tar.gz\n```\n\nPS: 使用自行编译的hadoop 2.30二进制包安装hadoop 2.30时需要注意删除 .bashrc文件与hadoop-env.sh文件中下面两行（默认不会有这两行，但是尝试解决报错时可能改写了）\n\n```\nexport HADOOP_COMMON_LIB_NATIVE_DIR=\"~/hadoop/lib/\"\nexport HADOOP_OPTS=\"$HADOOP_OPTS -Djava.library.path=~/hadoop/lib/\"\n```\n\n***\n\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n\n[http://kiwenlau.com/2015/11/05/151105-compile-hadoop-ubuntu/](http://kiwenlau.com/2015/11/05/151105-compile-hadoop-ubuntu/)\n***","source":"_posts/151105-compile-hadoop-ubuntu.md","raw":"title: Hadoop在Ubuntu中编译\n\ndate: 2015-11-05 14:00:00\n\ntags: [Hadoop]\n\n---\n\n官网提供的编译好的hadoop-2.3.0.tar.gz二进制包是在32位系统上编译的，在64系统上运行会有一些错误，比如：\n\n```\nWARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n```\n\n此时需要自行编译hadoop 2.30 源码。本人编译的hadoop 2.30的二进制包经实验证明可以正确安装并正确运行了Word Count程序。此处可以下载我编译的二进制包: [http://pan.baidu.com/s/1eQrgsWa](http://pan.baidu.com/s/1eQrgsWa)\n\n**更新软件列表**\n\n```\napt-get update\n```\n\n**安装编译Hadoop所需要的软件**\n\n```\napt-get install -y openjdk-7-jdk libprotobuf-dev protobuf-compiler maven cmake build-essential pkg-config libssl-dev zlib1g-dev llvm-gcc automake autoconf make\n```\n\n**下载hadoop 2.30的源文件包**\n\n```\nwget http://archive.apache.org/dist/hadoop/core/hadoop-2.3.0/hadoop-2.3.0-src.tar.gz \n```\n\n**解压hadoop 2.30 的源文件包**\n\n```\ntar -xzvf hadoop-2.3.0-src.tar.gz \n```\n\n**进入hadoop 2.30 文件夹**\n\n```\ncd hadoop-2.3.0-src\n```\n\n**编译hadoop 2.30 源文件**\n\n```\nmvn package -Pdist,native -DskipTests -Dtar\n```\n\n正确执行的结果如下:\n\n```\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Apache Hadoop Main ................................ SUCCESS [1:11.968s]\n[INFO] Apache Hadoop Project POM ......................... SUCCESS [30.393s]\n[INFO] Apache Hadoop Annotations ......................... SUCCESS [18.398s]\n[INFO] Apache Hadoop Assemblies .......................... SUCCESS [0.246s]\n[INFO] Apache Hadoop Project Dist POM .................... SUCCESS [20.372s]\n[INFO] Apache Hadoop Maven Plugins ....................... SUCCESS [23.721s]\n[INFO] Apache Hadoop MiniKDC ............................. SUCCESS [1:41.836s]\n[INFO] Apache Hadoop Auth ................................ SUCCESS [22.303s]\n[INFO] Apache Hadoop Auth Examples ....................... SUCCESS [7.052s]\n[INFO] Apache Hadoop Common .............................. SUCCESS [2:29.466s]\n[INFO] Apache Hadoop NFS ................................. SUCCESS [11.604s]\n[INFO] Apache Hadoop Common Project ...................... SUCCESS [0.073s]\n[INFO] Apache Hadoop HDFS ................................ SUCCESS [1:30.230s]\n[INFO] Apache Hadoop HttpFS .............................. SUCCESS [17.976s]\n[INFO] Apache Hadoop HDFS BookKeeper Journal ............. SUCCESS [19.927s]\n[INFO] Apache Hadoop HDFS-NFS ............................ SUCCESS [3.304s]\n[INFO] Apache Hadoop HDFS Project ........................ SUCCESS [0.032s]\n[INFO] hadoop-yarn ....................................... SUCCESS [0.033s]\n[INFO] hadoop-yarn-api ................................... SUCCESS [36.284s]\n[INFO] hadoop-yarn-common ................................ SUCCESS [33.912s]\n[INFO] hadoop-yarn-server ................................ SUCCESS [0.213s]\n[INFO] hadoop-yarn-server-common ......................... SUCCESS [8.193s]\n[INFO] hadoop-yarn-server-nodemanager .................... SUCCESS [41.181s]\n[INFO] hadoop-yarn-server-web-proxy ...................... SUCCESS [2.768s]\n[INFO] hadoop-yarn-server-resourcemanager ................ SUCCESS [13.923s]\n[INFO] hadoop-yarn-server-tests .......................... SUCCESS [0.904s]\n[INFO] hadoop-yarn-client ................................ SUCCESS [4.363s]\n[INFO] hadoop-yarn-applications .......................... SUCCESS [0.120s]\n[INFO] hadoop-yarn-applications-distributedshell ......... SUCCESS [2.262s]\n[INFO] hadoop-yarn-applications-unmanaged-am-launcher .... SUCCESS [1.615s]\n[INFO] hadoop-yarn-site .................................. SUCCESS [0.086s]\n[INFO] hadoop-yarn-project ............................... SUCCESS [2.703s]\n[INFO] hadoop-mapreduce-client ........................... SUCCESS [0.132s]\n[INFO] hadoop-mapreduce-client-core ...................... SUCCESS [18.951s]\n[INFO] hadoop-mapreduce-client-common .................... SUCCESS [14.320s]\n[INFO] hadoop-mapreduce-client-shuffle ................... SUCCESS [3.330s]\n[INFO] hadoop-mapreduce-client-app ....................... SUCCESS [9.664s]\n[INFO] hadoop-mapreduce-client-hs ........................ SUCCESS [7.678s]\n[INFO] hadoop-mapreduce-client-jobclient ................. SUCCESS [9.263s]\n[INFO] hadoop-mapreduce-client-hs-plugins ................ SUCCESS [1.549s]\n[INFO] Apache Hadoop MapReduce Examples .................. SUCCESS [5.748s]\n[INFO] hadoop-mapreduce .................................. SUCCESS [2.880s]\n[INFO] Apache Hadoop MapReduce Streaming ................. SUCCESS [7.080s]\n[INFO] Apache Hadoop Distributed Copy .................... SUCCESS [14.648s]\n[INFO] Apache Hadoop Archives ............................ SUCCESS [2.602s]\n[INFO] Apache Hadoop Rumen ............................... SUCCESS [5.706s]\n[INFO] Apache Hadoop Gridmix ............................. SUCCESS [3.649s]\n[INFO] Apache Hadoop Data Join ........................... SUCCESS [2.483s]\n[INFO] Apache Hadoop Extras .............................. SUCCESS [2.678s]\n[INFO] Apache Hadoop Pipes ............................... SUCCESS [6.359s]\n[INFO] Apache Hadoop OpenStack support ................... SUCCESS [5.088s]\n[INFO] Apache Hadoop Client .............................. SUCCESS [4.534s]\n[INFO] Apache Hadoop Mini-Cluster ........................ SUCCESS [0.433s]\n[INFO] Apache Hadoop Scheduler Load Simulator ............ SUCCESS [7.757s]\n[INFO] Apache Hadoop Tools Dist .......................... SUCCESS [4.099s]\n[INFO] Apache Hadoop Tools ............................... SUCCESS [0.428s]\n[INFO] Apache Hadoop Distribution ........................ SUCCESS [18.045s]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 14:59.240s\n[INFO] Finished at: Thu Jan 15 18:51:59 JST 2015\n[INFO] Final Memory: 168M/435M\n[INFO] ------------------------------------------------------------------------\n```\n\n**编译好的二进制文件包位于**\n\n```\nhadoop-2.3.0-src/hadoop-dist/target/hadoop-2.3.0.tar.gz\n```\n\nPS: 使用自行编译的hadoop 2.30二进制包安装hadoop 2.30时需要注意删除 .bashrc文件与hadoop-env.sh文件中下面两行（默认不会有这两行，但是尝试解决报错时可能改写了）\n\n```\nexport HADOOP_COMMON_LIB_NATIVE_DIR=\"~/hadoop/lib/\"\nexport HADOOP_OPTS=\"$HADOOP_OPTS -Djava.library.path=~/hadoop/lib/\"\n```\n\n***\n\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n\n[http://kiwenlau.com/2015/11/05/151105-compile-hadoop-ubuntu/](http://kiwenlau.com/2015/11/05/151105-compile-hadoop-ubuntu/)\n***","slug":"151105-compile-hadoop-ubuntu","published":1,"updated":"2015-11-07T07:28:59.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cihhlj30u000l0x8zwag4vuli"},{"title":"配置MacBook与Virtualbox ubuntu的共享文件夹","date":"2015-11-04T03:00:00.000Z","_content":"\n##1. 开启Ubuntu虚拟机，安装Guest Additions\n\nDevices > Insert Guest Additions CD image...\n\n挂载\n\n``` \nsudo mount /dev/cdrom /media/cdrom\n```\n \n安装 \n\n```\nsudo /media/cdrom/VBoxLinuxAdditions.run\n```\n \n##2. 开启Ubuntu虚拟机，设置共享文件夹\n \n先在mac中创建共享文件夹ubuntu\n \nDevices>Shared Fold settings>+>选中文件夹ubuntu>\n\n选中Auto-mount和Make Permanent\n \n##3. 关闭Ubuntu虚拟机，在MacBook中修改共享文件夹权限\n\n若不修改权限则将无法创建软链接。\n\n```\nVBoxManage setextradata VM_NAME VBoxInternal2/SharedFoldersEnableSymlinksCreate/SHARE_NAME 1\n```\nVM_NAME是虚拟机的名称, SHARE_NAME是共享文件夹名字\n \n##4. 开启Ubuntu虚拟机，创建共享文件夹的软链接\n \n共享文件夹位于/media目录内以sf_ 为前缀\n\n```\nln -s /media/sf_ubuntu mac\n```\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：[http://kiwenlau.com/2015/11/04/151104-macbook-virtualbox-ubuntu-share-fold/](http://kiwenlau.com/2015/11/04/151104-macbook-virtualbox-ubuntu-share-fold/)\n***","source":"_posts/151104-macbook-virtualbox-ubuntu-share-fold.md","raw":"title: 配置MacBook与Virtualbox ubuntu的共享文件夹\n\ndate: 2015-11-04 12:00:00\n\ntags: [VirtualBox]\n\n---\n\n##1. 开启Ubuntu虚拟机，安装Guest Additions\n\nDevices > Insert Guest Additions CD image...\n\n挂载\n\n``` \nsudo mount /dev/cdrom /media/cdrom\n```\n \n安装 \n\n```\nsudo /media/cdrom/VBoxLinuxAdditions.run\n```\n \n##2. 开启Ubuntu虚拟机，设置共享文件夹\n \n先在mac中创建共享文件夹ubuntu\n \nDevices>Shared Fold settings>+>选中文件夹ubuntu>\n\n选中Auto-mount和Make Permanent\n \n##3. 关闭Ubuntu虚拟机，在MacBook中修改共享文件夹权限\n\n若不修改权限则将无法创建软链接。\n\n```\nVBoxManage setextradata VM_NAME VBoxInternal2/SharedFoldersEnableSymlinksCreate/SHARE_NAME 1\n```\nVM_NAME是虚拟机的名称, SHARE_NAME是共享文件夹名字\n \n##4. 开启Ubuntu虚拟机，创建共享文件夹的软链接\n \n共享文件夹位于/media目录内以sf_ 为前缀\n\n```\nln -s /media/sf_ubuntu mac\n```\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：[http://kiwenlau.com/2015/11/04/151104-macbook-virtualbox-ubuntu-share-fold/](http://kiwenlau.com/2015/11/04/151104-macbook-virtualbox-ubuntu-share-fold/)\n***","slug":"151104-macbook-virtualbox-ubuntu-share-fold","published":1,"updated":"2015-11-03T16:33:36.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cihhlj30x000o0x8zx2xdklw6"},{"title":"在Docker容器中运行Docker仓库","date":"2015-11-04T05:00:00.000Z","_content":"\n操作系统：ubuntu 14.04\n\n本机IP为：192.168.59.10\n \n##1. 安装Docker\n \n```\nsudo apt-get install apt-transport-https && sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9 && sudo bash -c \"echo deb https://get.docker.io/ubuntu docker main > /etc/apt/sources.list.d/docker.list\" && sudo apt-get update && sudo apt-get install -y lxc-docker && sudo docker version\n```\n \n##2. 在Docker容器中运行Docker仓库\n \n```\nsudo docker run -p 5000:5000 -d registry\n```\n \n##3. 测试Docker仓库\n \n可以在本机以及同一个局域网的其他机器上测试\n\n```\ncurl 192.168.59.10:5000 \n```\n \n输出信息：\n\n```\n\"\\\"docker-registry server\\\"\"\n```\n \n##4. 在本机push镜像到Dcker仓库中\n \n下载busybox镜像做测试\n\n```\nsudo docker pull busybox\n```\n\n查看image：\n\n```\nsudo docker images\n```\n\n输出信息(可知busybox镜像的ID为4986bf8c1536)\n\n```\nREPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox             latest              4986bf8c1536        2 weeks ago         2.433 MB\n```\n \n修改busybox镜像的tag\n\n```\nsudo docker tag 4986bf8c1536 192.168.59.10:5000/busybox:latest\n```\n  \npush镜像到Docker仓库中\n\n```\nsudo docker push 192.168.59.10:5000/busybox:latest \n```\n\n此时会报错：\n\n```\nFATA[0002] Error: Invalid registry endpoint https://192.168.59.10:5000/v1/: Get https://192.168.59.10:5000/v1/_ping: EOF. If this private registry supports only HTTP or HTTPS with an unknown CA certificate, please add `--insecure-registry 192.168.59.10:5000` to the daemon's arguments. In the case of HTTPS, if you have access to the registry's CA certificate, no need for the flag; simply place the CA certificate at /etc/docker/certs.d/192.168.59.10:5000/ca.crt \n```\n\n解决方法：根据第5步修改docker的配置\n \n##5. 修改Docker配置\n \n修改Docker配置文件\n\n```\nsudo vim /etc/default/docker\n```\n \n在文件中增加以下内容\n\n```\nDOCKER_OPTS=\"$DOCKER_OPTS --insecure-registry=192.168.59.10:5000\"\n```\n \n重启docker\n\n```\nsudo service docker restart\n```\n \n##6. 再次在本机push镜像到Docker仓库中\n\n运行Docker仓库\n\n```\nsudo docker run -p 5000:5000 -d registry\n```\n\npush镜像到Docker仓库中\n\n```\nsudo docker push 192.168.59.10:5000/busybox:latest \n```\n \n正确输出信息：\n\n```\nThe push refers to a repository [192.168.59.10:5000/busybox] (len: 1)\nSending image list\nPushing repository 192.168.59.10:5000/busybox (1 tags)\n511136ea3c5a: Image successfully pushed \ndf7546f9f060: Image successfully pushed \nea13149945cb: Image successfully pushed \n4986bf8c1536: Image successfully pushed \nPushing tag for rev [4986bf8c1536] on {http://192.168.59.10:5000/v1/repositories/busybox/tags/latest}\n```\n\n  \n##7. 从Docker仓库中pull镜像\n\n在同一个局域网内的另外一台机器进行实验, 按照第1步的方法安装Docker, 按照第5步的方法配置Docker。\n\n \n```\nsudo docker pull 192.168.59.10:5000/busybox:latest \n```\n\n \n正确输出结果：\n\n```\nPulling repository 192.168.59.10:5000/busybox\n4986bf8c1536: Download complete \n511136ea3c5a: Download complete \ndf7546f9f060: Download complete \nea13149945cb: Download complete \nStatus: Image is up to date for 192.168.59.10:5000/busybox:latest\n```\n\n***\n\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n\n[http://kiwenlau.com/2015/11/04/151104-docker-registry-in-docker-container/](http://kiwenlau.com/2015/11/04/151104-docker-registry-in-docker-container/)\n***","source":"_posts/151104-docker-registry-in-docker-container.md","raw":"title: 在Docker容器中运行Docker仓库\n\ndate: 2015-11-04 14:00:00\n\ntags: [Docker]\n\n---\n\n操作系统：ubuntu 14.04\n\n本机IP为：192.168.59.10\n \n##1. 安装Docker\n \n```\nsudo apt-get install apt-transport-https && sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9 && sudo bash -c \"echo deb https://get.docker.io/ubuntu docker main > /etc/apt/sources.list.d/docker.list\" && sudo apt-get update && sudo apt-get install -y lxc-docker && sudo docker version\n```\n \n##2. 在Docker容器中运行Docker仓库\n \n```\nsudo docker run -p 5000:5000 -d registry\n```\n \n##3. 测试Docker仓库\n \n可以在本机以及同一个局域网的其他机器上测试\n\n```\ncurl 192.168.59.10:5000 \n```\n \n输出信息：\n\n```\n\"\\\"docker-registry server\\\"\"\n```\n \n##4. 在本机push镜像到Dcker仓库中\n \n下载busybox镜像做测试\n\n```\nsudo docker pull busybox\n```\n\n查看image：\n\n```\nsudo docker images\n```\n\n输出信息(可知busybox镜像的ID为4986bf8c1536)\n\n```\nREPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox             latest              4986bf8c1536        2 weeks ago         2.433 MB\n```\n \n修改busybox镜像的tag\n\n```\nsudo docker tag 4986bf8c1536 192.168.59.10:5000/busybox:latest\n```\n  \npush镜像到Docker仓库中\n\n```\nsudo docker push 192.168.59.10:5000/busybox:latest \n```\n\n此时会报错：\n\n```\nFATA[0002] Error: Invalid registry endpoint https://192.168.59.10:5000/v1/: Get https://192.168.59.10:5000/v1/_ping: EOF. If this private registry supports only HTTP or HTTPS with an unknown CA certificate, please add `--insecure-registry 192.168.59.10:5000` to the daemon's arguments. In the case of HTTPS, if you have access to the registry's CA certificate, no need for the flag; simply place the CA certificate at /etc/docker/certs.d/192.168.59.10:5000/ca.crt \n```\n\n解决方法：根据第5步修改docker的配置\n \n##5. 修改Docker配置\n \n修改Docker配置文件\n\n```\nsudo vim /etc/default/docker\n```\n \n在文件中增加以下内容\n\n```\nDOCKER_OPTS=\"$DOCKER_OPTS --insecure-registry=192.168.59.10:5000\"\n```\n \n重启docker\n\n```\nsudo service docker restart\n```\n \n##6. 再次在本机push镜像到Docker仓库中\n\n运行Docker仓库\n\n```\nsudo docker run -p 5000:5000 -d registry\n```\n\npush镜像到Docker仓库中\n\n```\nsudo docker push 192.168.59.10:5000/busybox:latest \n```\n \n正确输出信息：\n\n```\nThe push refers to a repository [192.168.59.10:5000/busybox] (len: 1)\nSending image list\nPushing repository 192.168.59.10:5000/busybox (1 tags)\n511136ea3c5a: Image successfully pushed \ndf7546f9f060: Image successfully pushed \nea13149945cb: Image successfully pushed \n4986bf8c1536: Image successfully pushed \nPushing tag for rev [4986bf8c1536] on {http://192.168.59.10:5000/v1/repositories/busybox/tags/latest}\n```\n\n  \n##7. 从Docker仓库中pull镜像\n\n在同一个局域网内的另外一台机器进行实验, 按照第1步的方法安装Docker, 按照第5步的方法配置Docker。\n\n \n```\nsudo docker pull 192.168.59.10:5000/busybox:latest \n```\n\n \n正确输出结果：\n\n```\nPulling repository 192.168.59.10:5000/busybox\n4986bf8c1536: Download complete \n511136ea3c5a: Download complete \ndf7546f9f060: Download complete \nea13149945cb: Download complete \nStatus: Image is up to date for 192.168.59.10:5000/busybox:latest\n```\n\n***\n\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n\n[http://kiwenlau.com/2015/11/04/151104-docker-registry-in-docker-container/](http://kiwenlau.com/2015/11/04/151104-docker-registry-in-docker-container/)\n***","slug":"151104-docker-registry-in-docker-container","published":1,"updated":"2015-11-04T11:19:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cihhlj311000q0x8zud376s7o"},{"title":"基于Docker快速搭建单节点Mesos/Marathon集群","date":"2015-09-18T03:00:00.000Z","_content":"\n- GitHub地址：[kiwenlau/single-mesos-docker](https://github.com/kiwenlau/single-mesos-docker)\n- 博客地址：[基于Docker快速搭建单节点Mesos/Marathon集群](http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/)\n\n##一. 简介\n\n[Mesos](http://mesos.apache.org)是集群资源管理系统，[Marathon](http://mesosphere.github.io/marathon)是运行在Mesos之上的集群计算架构。将Mesos和Marathon打包到[Docker](https://www.docker.com/)镜像中，开发者便可以在本机上快速搭建Mesos/Marathon集群，进行学习和测试。\n\n**kiwenlau/single-mesos**镜像非常简单。Docker容器运行在Ubuntu主机之上，Mesos和Marathon运行在该容器之中。具体来讲，Docker容器中运行了一个Mesos Master和一个Mesos Slave，以及Marathon和[ZooKeeper](https://zookeeper.apache.org/)。集群架构如下图：\n\n![](/image/150918/architecuture.png)\n\n\n##二. 搭建Mesos/Marathon集群\n\n**1. 下载Docker镜像:**\n\n```sh\nsudo docker pull kiwenlau/single-mesos:3.0\n```\n\n**2. 运行Docker容器:**\n\n```sh\nsudo docker run -p 5050:5050 -p 8080:8080 --name mesos -it -w /root kiwenlau/single-mesos:3.0\n```\n\ndocker run命令运行成功后即进入容器内部，以下为输出：\n\n```bash\nStart ZooKeeper...\nStart Mesos master...\nStart Mesos slave...\nStart Marathon...\n```\n\n\n##三. 测试Mesos/Marathon集群\n\n**1. 通过curl命令调用Marathon的REST API, 创建一个hello程序：**\n\n```sh\ncurl -v -H \"Content-Type: application/json\" -X POST --data \"@hello.json\" http://127.0.0.1:8080/v2/apps\n```\n\n下面为hello.json。由cmd可知，该程序每隔1秒往output.txt文件中写入hello。\n\n```bash\n{\n  \"id\": \"hello\",\n  \"cmd\": \"while [ true ] ; do echo hello >> /root/output.txt; sleep 1; done\",\n  \"cpus\": 0.1,\n  \"mem\": 10.0,\n  \"instances\": 1\n}\n```\n\ncurl执行结果:\n\n```bash\n* Hostname was NOT found in DNS cache\n*   Trying 127.0.0.1...\n* Connected to 127.0.0.1 (127.0.0.1) port 8080 (#0)\n> POST /v2/apps HTTP/1.1\n> User-Agent: curl/7.35.0\n> Host: 127.0.0.1:8080\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 139\n> \n* upload completely sent off: 139 out of 139 bytes\n< HTTP/1.1 201 Created\n< X-Marathon-Leader: http://ec054cabb9af:8080\n< Cache-Control: no-cache, no-store, must-revalidate\n< Pragma: no-cache\n< Expires: 0\n< Location: http://127.0.0.1:8080/v2/apps/hello\n< Content-Type: application/json; qs=2\n< Transfer-Encoding: chunked\n* Server Jetty(8.y.z-SNAPSHOT) is not blacklisted\n< Server: Jetty(8.y.z-SNAPSHOT)\n< \n* Connection #0 to host 127.0.0.1 left intact\n{\"id\":\"/hello\",\"cmd\":\"while [ true ] ; do echo hello >> /root/output.txt; sleep 1; done\",\"args\":null,\"user\":null,\"env\":{},\"instances\":1,\"cpus\":0.1,\"mem\":10.0,\"disk\":0.0,\"executor\":\"\",\"constraints\":[],\"uris\":[],\"storeUrls\":[],\"ports\":[0],\"requirePorts\":false,\"backoffFactor\":1.15,\"container\":null,\"healthChecks\":[],\"dependencies\":[],\"upgradeStrategy\":{\"minimumHealthCapacity\":1.0,\"maximumOverCapacity\":1.0},\"labels\":{},\"acceptedResourceRoles\":null,\"version\":\"2015-09-16T11:22:27.967Z\",\"deployments\":[{\"id\":\"2cd2fdd4-e5f9-4088-895f-7976349b7a19\"}],\"tasks\":[],\"tasksStaged\":0,\"tasksRunning\":0,\"tasksHealthy\":0,\"tasksUnhealthy\":0,\"backoffSeconds\":1,\"maxLaunchDelaySeconds\":3600}\n```\n\n**2. 查看hello程序的运行结果：**\n\n```sh\ntail -f output.txt\n```\n当你看到终端不断输出\"hello\"时说明运行成功。\n\n**3. 使用浏览器查看Mesos和Marathon的网页管理界面**\n\n**注意**将IP替换运行Docker容器的主机IP地址\n\nMesos网页管理界面地址：[http://192.168.59.10:5050](http://192.168.59.10:5050)\n\nMesos网页管理界面如图，可知hello程序正在运行：\n\n![](/image/150918/Mesos.png)\n\nMarathon网页管理界面地址：[http://192.168.59.10:8080](http://192.168.59.10:8080)\n\nMarathon网页管理界面如图，可知hello程序正在运行：\n\n![](/image/150918/Marathon.png)\n\n**4. 通过Marathon网页管理界面创建测试程序**\n\n在Marathon的网页管理界面上点击\"New APP\"，在弹框中配置测试程序。ID为\"hello\", Command为\"echo hello >> /root/output.txt\", 然后点击\"Create\"即可。如下图：\n\n![](/image/150918/hello.png)\n\n\n##四. 存在的问题\n\n其实，参考[Setting up a Single Node Mesosphere Cluster](https://open.mesosphere.com/getting-started/developer/single-node-install/)，可以很快地在ubuntu主机上直接搭建一个单节点的Mesos/Marathon集群。但是，当我安装该教程的步骤将Mesos/Marathon集群打包到Docker镜像中时，遇到了一个比较奇怪的问题。\n\n在Docker容器中使用**\"sudo service mesos-master start\"**和**\"sudo service mesos-slave start\"**命令启动Mesos Master和Mesos Slave时，出现**\"mesos-master: unrecognized service\"**和**\"mesos-slave: unrecognized service\"**错误。但是，我在ubuntu主机上安装Mesos/Marathon集群后，使用同样的命令启动Mesos并没有问题。后来，我是通过直接执行mesos-master和mesos-slave命令启动Mesos，命令如下：\n\n```sh\n/usr/sbin/mesos-master --zk=zk://127.0.0.1:2181/mesos --quorum=1 --work_dir=/var/lib/mesos --log_dir=/log/mesos  \n```\n\n```sh\n/usr/sbin/mesos-slave --master=zk://127.0.0.1:2181/mesos --log_dir=/log/mesos\n```\n\n由这个问题可知，虽然在Docker容器几乎可以运行任意程序，似乎和Ubuntu主机没有区别。但是事实上，**Docker容器与ubuntu主机并非完全一致**，而且这些细节的不同点比较坑。这一点很值得探讨，可以让大家在使用Docker时少走些弯路。对于提到的问题，虽然是解决了，然而我仍然不清楚其中的原因:(\n\n\n##五. Docker镜像备份\n\n我将Docker镜像上传到了灵雀云（Alaudo）的Docker仓库，可以通过以下命令下载和运行：\n\n```sh\nsudo docker pull index.alauda.cn/kiwenlau/single-mesos:3.0\n```\n\n```sh\nsudo docker run -p 5050:5050 -p 8080:8080 --name mesos -it -w /root index.alauda.cn/kiwenlau/single-mesos:3.0\n```\n\n##六. 参考\n\n1. [Setting up a Single Node Mesosphere Cluster](https://open.mesosphere.com/getting-started/developer/single-node-install/)\n2. [Setting up a Cluster on Mesos and Marathon](https://open.mesosphere.com/getting-started/datacenter/install/#master-setup)\n3. [An Introduction to Mesosphere](https://www.digitalocean.com/community/tutorials/an-introduction-to-mesosphere)\n4. [How To Configure a Production-Ready Mesosphere Cluster on Ubuntu 14.04](https://www.digitalocean.com/community/tutorials/how-to-configure-a-production-ready-mesosphere-cluster-on-ubuntu-14-04)\n5. [Deploy a Mesos Cluster with 7 Commands Using Docker](https://medium.com/@gargar454/deploy-a-mesos-cluster-with-7-commands-using-docker-57951e020586)\n6. [sekka1/mesosphere-docker](https://github.com/sekka1/mesosphere-docker)\n7. [Marathon: Application Basics](http://mesosphere.github.io/marathon/docs/application-basics.html)\n8. [Marathon: REST API](http://mesosphere.github.io/marathon/docs/rest-api.html)\n\n***\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文URL地址：\n\n[http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/](http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/)\n***\n\n\n\n\n\n","source":"_posts/150918-single-mesos-docker.md","raw":"title: 基于Docker快速搭建单节点Mesos/Marathon集群\n\ndate: 2015-09-18 12:00:00\n\ntags: [Docker,Mesos,Marathon]\n\n---\n\n- GitHub地址：[kiwenlau/single-mesos-docker](https://github.com/kiwenlau/single-mesos-docker)\n- 博客地址：[基于Docker快速搭建单节点Mesos/Marathon集群](http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/)\n\n##一. 简介\n\n[Mesos](http://mesos.apache.org)是集群资源管理系统，[Marathon](http://mesosphere.github.io/marathon)是运行在Mesos之上的集群计算架构。将Mesos和Marathon打包到[Docker](https://www.docker.com/)镜像中，开发者便可以在本机上快速搭建Mesos/Marathon集群，进行学习和测试。\n\n**kiwenlau/single-mesos**镜像非常简单。Docker容器运行在Ubuntu主机之上，Mesos和Marathon运行在该容器之中。具体来讲，Docker容器中运行了一个Mesos Master和一个Mesos Slave，以及Marathon和[ZooKeeper](https://zookeeper.apache.org/)。集群架构如下图：\n\n![](/image/150918/architecuture.png)\n\n\n##二. 搭建Mesos/Marathon集群\n\n**1. 下载Docker镜像:**\n\n```sh\nsudo docker pull kiwenlau/single-mesos:3.0\n```\n\n**2. 运行Docker容器:**\n\n```sh\nsudo docker run -p 5050:5050 -p 8080:8080 --name mesos -it -w /root kiwenlau/single-mesos:3.0\n```\n\ndocker run命令运行成功后即进入容器内部，以下为输出：\n\n```bash\nStart ZooKeeper...\nStart Mesos master...\nStart Mesos slave...\nStart Marathon...\n```\n\n\n##三. 测试Mesos/Marathon集群\n\n**1. 通过curl命令调用Marathon的REST API, 创建一个hello程序：**\n\n```sh\ncurl -v -H \"Content-Type: application/json\" -X POST --data \"@hello.json\" http://127.0.0.1:8080/v2/apps\n```\n\n下面为hello.json。由cmd可知，该程序每隔1秒往output.txt文件中写入hello。\n\n```bash\n{\n  \"id\": \"hello\",\n  \"cmd\": \"while [ true ] ; do echo hello >> /root/output.txt; sleep 1; done\",\n  \"cpus\": 0.1,\n  \"mem\": 10.0,\n  \"instances\": 1\n}\n```\n\ncurl执行结果:\n\n```bash\n* Hostname was NOT found in DNS cache\n*   Trying 127.0.0.1...\n* Connected to 127.0.0.1 (127.0.0.1) port 8080 (#0)\n> POST /v2/apps HTTP/1.1\n> User-Agent: curl/7.35.0\n> Host: 127.0.0.1:8080\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 139\n> \n* upload completely sent off: 139 out of 139 bytes\n< HTTP/1.1 201 Created\n< X-Marathon-Leader: http://ec054cabb9af:8080\n< Cache-Control: no-cache, no-store, must-revalidate\n< Pragma: no-cache\n< Expires: 0\n< Location: http://127.0.0.1:8080/v2/apps/hello\n< Content-Type: application/json; qs=2\n< Transfer-Encoding: chunked\n* Server Jetty(8.y.z-SNAPSHOT) is not blacklisted\n< Server: Jetty(8.y.z-SNAPSHOT)\n< \n* Connection #0 to host 127.0.0.1 left intact\n{\"id\":\"/hello\",\"cmd\":\"while [ true ] ; do echo hello >> /root/output.txt; sleep 1; done\",\"args\":null,\"user\":null,\"env\":{},\"instances\":1,\"cpus\":0.1,\"mem\":10.0,\"disk\":0.0,\"executor\":\"\",\"constraints\":[],\"uris\":[],\"storeUrls\":[],\"ports\":[0],\"requirePorts\":false,\"backoffFactor\":1.15,\"container\":null,\"healthChecks\":[],\"dependencies\":[],\"upgradeStrategy\":{\"minimumHealthCapacity\":1.0,\"maximumOverCapacity\":1.0},\"labels\":{},\"acceptedResourceRoles\":null,\"version\":\"2015-09-16T11:22:27.967Z\",\"deployments\":[{\"id\":\"2cd2fdd4-e5f9-4088-895f-7976349b7a19\"}],\"tasks\":[],\"tasksStaged\":0,\"tasksRunning\":0,\"tasksHealthy\":0,\"tasksUnhealthy\":0,\"backoffSeconds\":1,\"maxLaunchDelaySeconds\":3600}\n```\n\n**2. 查看hello程序的运行结果：**\n\n```sh\ntail -f output.txt\n```\n当你看到终端不断输出\"hello\"时说明运行成功。\n\n**3. 使用浏览器查看Mesos和Marathon的网页管理界面**\n\n**注意**将IP替换运行Docker容器的主机IP地址\n\nMesos网页管理界面地址：[http://192.168.59.10:5050](http://192.168.59.10:5050)\n\nMesos网页管理界面如图，可知hello程序正在运行：\n\n![](/image/150918/Mesos.png)\n\nMarathon网页管理界面地址：[http://192.168.59.10:8080](http://192.168.59.10:8080)\n\nMarathon网页管理界面如图，可知hello程序正在运行：\n\n![](/image/150918/Marathon.png)\n\n**4. 通过Marathon网页管理界面创建测试程序**\n\n在Marathon的网页管理界面上点击\"New APP\"，在弹框中配置测试程序。ID为\"hello\", Command为\"echo hello >> /root/output.txt\", 然后点击\"Create\"即可。如下图：\n\n![](/image/150918/hello.png)\n\n\n##四. 存在的问题\n\n其实，参考[Setting up a Single Node Mesosphere Cluster](https://open.mesosphere.com/getting-started/developer/single-node-install/)，可以很快地在ubuntu主机上直接搭建一个单节点的Mesos/Marathon集群。但是，当我安装该教程的步骤将Mesos/Marathon集群打包到Docker镜像中时，遇到了一个比较奇怪的问题。\n\n在Docker容器中使用**\"sudo service mesos-master start\"**和**\"sudo service mesos-slave start\"**命令启动Mesos Master和Mesos Slave时，出现**\"mesos-master: unrecognized service\"**和**\"mesos-slave: unrecognized service\"**错误。但是，我在ubuntu主机上安装Mesos/Marathon集群后，使用同样的命令启动Mesos并没有问题。后来，我是通过直接执行mesos-master和mesos-slave命令启动Mesos，命令如下：\n\n```sh\n/usr/sbin/mesos-master --zk=zk://127.0.0.1:2181/mesos --quorum=1 --work_dir=/var/lib/mesos --log_dir=/log/mesos  \n```\n\n```sh\n/usr/sbin/mesos-slave --master=zk://127.0.0.1:2181/mesos --log_dir=/log/mesos\n```\n\n由这个问题可知，虽然在Docker容器几乎可以运行任意程序，似乎和Ubuntu主机没有区别。但是事实上，**Docker容器与ubuntu主机并非完全一致**，而且这些细节的不同点比较坑。这一点很值得探讨，可以让大家在使用Docker时少走些弯路。对于提到的问题，虽然是解决了，然而我仍然不清楚其中的原因:(\n\n\n##五. Docker镜像备份\n\n我将Docker镜像上传到了灵雀云（Alaudo）的Docker仓库，可以通过以下命令下载和运行：\n\n```sh\nsudo docker pull index.alauda.cn/kiwenlau/single-mesos:3.0\n```\n\n```sh\nsudo docker run -p 5050:5050 -p 8080:8080 --name mesos -it -w /root index.alauda.cn/kiwenlau/single-mesos:3.0\n```\n\n##六. 参考\n\n1. [Setting up a Single Node Mesosphere Cluster](https://open.mesosphere.com/getting-started/developer/single-node-install/)\n2. [Setting up a Cluster on Mesos and Marathon](https://open.mesosphere.com/getting-started/datacenter/install/#master-setup)\n3. [An Introduction to Mesosphere](https://www.digitalocean.com/community/tutorials/an-introduction-to-mesosphere)\n4. [How To Configure a Production-Ready Mesosphere Cluster on Ubuntu 14.04](https://www.digitalocean.com/community/tutorials/how-to-configure-a-production-ready-mesosphere-cluster-on-ubuntu-14-04)\n5. [Deploy a Mesos Cluster with 7 Commands Using Docker](https://medium.com/@gargar454/deploy-a-mesos-cluster-with-7-commands-using-docker-57951e020586)\n6. [sekka1/mesosphere-docker](https://github.com/sekka1/mesosphere-docker)\n7. [Marathon: Application Basics](http://mesosphere.github.io/marathon/docs/application-basics.html)\n8. [Marathon: REST API](http://mesosphere.github.io/marathon/docs/rest-api.html)\n\n***\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文URL地址：\n\n[http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/](http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/)\n***\n\n\n\n\n\n","slug":"150918-single-mesos-docker","published":1,"updated":"2015-09-25T07:48:46.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cihhlj314000s0x8zukldr1db"},{"title":"grep命令匹配多个单词","date":"2015-06-26T00:41:52.000Z","_content":"\n使用grep命令可以通过匹配单词迅速定位对应的行，但是有时候需要同时匹配多个单词，而且会有不同的匹配要求。本文将通过匹配两个单词作为示例，介绍grep匹配多个单词的方法。\n\n\n![](/image/150626/grep_word.png)\n\n\n**输入文本(country.txt)**\n```bash\nAustria England\nAustria Canada\nChina England\nChina Canada\n```\n\n**1. 匹配同时含两个单词的行**\n```sh\ncat country.txt | grep Austria | grep England\n```\n\n*输出*\n```plain\nAustria England\n```\n\n\n**2. 匹配两个单词都不存在的行**\n```sh\ncat country.txt | grep -v Austria | grep -v England\n```\n\n*输出*\n```plain\nChina Canada\n```\n\n**3. 匹配含有任意一个单词的行**\n```sh\ncat country.txt | grep -E 'Austria|England'\n```\n\n*输出*\n```bash\nAustria England\nAustria Canada\nChina England\n```\n\n**3. 匹配含有其中一个单词但是不含另一个单词的行**\n```sh\ncat country.txt | grep Austria | grep -v England\n```\n\n*输出*\n```plain\nAustria Canada\n```\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/06/26/150626-grep-multiple-word/](http://kiwenlau.com/2015/06/26/150626-grep-multiple-word/)\n***\n\n\n\n\n\n\n\n\n","source":"_posts/150626-grep-multiple-word.md","raw":"title: grep命令匹配多个单词\n\ndate: 2015-06-26 09:41:52\n\ntags: [Linux]\n\n---\n\n使用grep命令可以通过匹配单词迅速定位对应的行，但是有时候需要同时匹配多个单词，而且会有不同的匹配要求。本文将通过匹配两个单词作为示例，介绍grep匹配多个单词的方法。\n\n\n![](/image/150626/grep_word.png)\n\n\n**输入文本(country.txt)**\n```bash\nAustria England\nAustria Canada\nChina England\nChina Canada\n```\n\n**1. 匹配同时含两个单词的行**\n```sh\ncat country.txt | grep Austria | grep England\n```\n\n*输出*\n```plain\nAustria England\n```\n\n\n**2. 匹配两个单词都不存在的行**\n```sh\ncat country.txt | grep -v Austria | grep -v England\n```\n\n*输出*\n```plain\nChina Canada\n```\n\n**3. 匹配含有任意一个单词的行**\n```sh\ncat country.txt | grep -E 'Austria|England'\n```\n\n*输出*\n```bash\nAustria England\nAustria Canada\nChina England\n```\n\n**3. 匹配含有其中一个单词但是不含另一个单词的行**\n```sh\ncat country.txt | grep Austria | grep -v England\n```\n\n*输出*\n```plain\nAustria Canada\n```\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/06/26/150626-grep-multiple-word/](http://kiwenlau.com/2015/06/26/150626-grep-multiple-word/)\n***\n\n\n\n\n\n\n\n\n","slug":"150626-grep-multiple-word","published":1,"updated":"2015-09-25T07:55:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cihhlj319000y0x8zrk84quur"},{"title":"tr与sed命令将换行符替换为空","date":"2015-06-23T09:56:27.000Z","_content":"处理文本时需要将换行符替换为空格，若使用sed命令会比较麻烦，而使用tr命令非常方便。\n\n![](/image/150623/tr-sed-newline.png)\n\n**输入文本(country.txt)**\n```bash\nChina\nAmerica\nFrance\nGerman\n```\n\n**sed命令**(参考[sed命令替换换行符](http://my.oschina.net/shelllife/blog/118337))\n```sh\ncat country.txt | sed ':label;N;s/\\n/ /;b label'\n```\n\n**tr命令**\n```sh\ncat country.txt | tr \"\\n\" \" \"\n```\n\n**两个命令输出一致，但是sed命令的输出结尾有换行符，而tr命令的输出结尾没有换行符**\n```plain\nChina America France German\n```\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：[http://kiwenlau.com/2015/06/23/150623-tr-sed-newline/](http://kiwenlau.com/2015/06/23/150623-tr-sed-newline/)\n***\n","source":"_posts/150623-tr-sed-newline.md","raw":"title: tr与sed命令将换行符替换为空\n\ndate: 2015-06-23 18:56:27\n\ntags: [Linux]\n\n---\n处理文本时需要将换行符替换为空格，若使用sed命令会比较麻烦，而使用tr命令非常方便。\n\n![](/image/150623/tr-sed-newline.png)\n\n**输入文本(country.txt)**\n```bash\nChina\nAmerica\nFrance\nGerman\n```\n\n**sed命令**(参考[sed命令替换换行符](http://my.oschina.net/shelllife/blog/118337))\n```sh\ncat country.txt | sed ':label;N;s/\\n/ /;b label'\n```\n\n**tr命令**\n```sh\ncat country.txt | tr \"\\n\" \" \"\n```\n\n**两个命令输出一致，但是sed命令的输出结尾有换行符，而tr命令的输出结尾没有换行符**\n```plain\nChina America France German\n```\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：[http://kiwenlau.com/2015/06/23/150623-tr-sed-newline/](http://kiwenlau.com/2015/06/23/150623-tr-sed-newline/)\n***\n","slug":"150623-tr-sed-newline","published":1,"updated":"2015-09-25T07:55:10.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cihhlj31b00100x8z8donptck"},{"title":"Python截取字符串的子串","date":"2015-06-21T07:31:40.000Z","_content":"\nPython处理字符串非常方便。这篇博客将通过一个简单的示例程序介绍如何使用Python截取字符串的子串。\n\n![](/image/150621/python-substring.png)\n\n**示例程序(example.py):**\n```python\n# coding=utf-8\n\nstr = \"2015-06-21\"\n\nyear = str[0:4]\nmonth = str[5:7]\nday= str[8:10]\n\nprint \"日期: \" + str\nprint \"年: \" + year\nprint \"月: \" + month\nprint \"日: \" + day\n```\n**执行程序：**\n```sh\npython example.py\n```\n**程序输出:**\n```bash\n日期: 2015-06-21\n年: 2015\n月: 06\n日: 21\n```\n**分析：**\n示例程序非常简单，它会截取日期字符串中的年月日。使用子串开始字符与结束字符的偏移量就可以很方便地进行截取。**但是，有两点需要注意**\n\n- 字符串的偏移从0开始。例如，截取\"2015\"时使用str[0:4]而不是str[1:5]。\n- 截取子串时，结束字符是子串最后一个字符的后面一个字符。例如，截取\"2015\"时使用str[0:4]而不是str[0:3]，而“5”的实际偏移量是3。这一点比较容易出现错误。\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/06/21/150621-python-get-substring/](http://kiwenlau.com/2015/06/21/150621-python-get-substring/)\n***\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/150621-python-get-substring.md","raw":"title: Python截取字符串的子串\n\ndate: 2015-06-21 16:31:40\n\ntags: [Python]\n\n---\n\nPython处理字符串非常方便。这篇博客将通过一个简单的示例程序介绍如何使用Python截取字符串的子串。\n\n![](/image/150621/python-substring.png)\n\n**示例程序(example.py):**\n```python\n# coding=utf-8\n\nstr = \"2015-06-21\"\n\nyear = str[0:4]\nmonth = str[5:7]\nday= str[8:10]\n\nprint \"日期: \" + str\nprint \"年: \" + year\nprint \"月: \" + month\nprint \"日: \" + day\n```\n**执行程序：**\n```sh\npython example.py\n```\n**程序输出:**\n```bash\n日期: 2015-06-21\n年: 2015\n月: 06\n日: 21\n```\n**分析：**\n示例程序非常简单，它会截取日期字符串中的年月日。使用子串开始字符与结束字符的偏移量就可以很方便地进行截取。**但是，有两点需要注意**\n\n- 字符串的偏移从0开始。例如，截取\"2015\"时使用str[0:4]而不是str[1:5]。\n- 截取子串时，结束字符是子串最后一个字符的后面一个字符。例如，截取\"2015\"时使用str[0:4]而不是str[0:3]，而“5”的实际偏移量是3。这一点比较容易出现错误。\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/06/21/150621-python-get-substring/](http://kiwenlau.com/2015/06/21/150621-python-get-substring/)\n***\n\n\n\n\n\n\n\n\n\n\n\n","slug":"150621-python-get-substring","published":1,"updated":"2015-09-25T07:54:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cihhlj31e00120x8zsf3f292k"},{"title":"ssh-copy-id出错 \"No such file\"","date":"2015-06-20T09:19:40.000Z","_content":"\n[ssh-copy-id](http://manpages.ubuntu.com/manpages/lucid/man1/ssh-copy-id.1.html)命令可以将本地的公钥添加到远程机器的authorized_keys中，从而实现SSH无密码登录。\n\n\n![](/image/150620/ssh-copy-id.png \"ssh-copy-id命令\")\n\n\n在使用ssh-copy-id命令时，出现了如下错误：\n```sh\n$ ssh-copy-id -i id_rsa.pub kiwenlau@136.187.59.2\n/usr/bin/ssh-copy-id: ERROR: failed to open ID file './id_rsa': No such file\n```\n\n命令的输出信息提示没有私钥\"id_rsa\"文件。可知，ssh-copy-id命令执行时将检查公钥id_rsa.pub所在目录是否存在私钥id_rsa，若没有私钥则会导致命令执行失败。\n\n这个问题的解决办法很简单，使用touch命令在公钥id_rsa.pub所在目录下创建一个空文件“id_rsa”即可。\n```sh\n$ touch id_rsa\n```\n\n然后ssh-copy-id命令就可以执行成功了。\n\n```sh\n$ ssh-copy-id -i id_rsa.pub kiwenlau@192.168.59.2\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys\n\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   \"ssh 'kiwenlau@192.168.59.2'\"\nand check to make sure that only the key(s) you wanted were added.\n```\n\n这时，使用ssh登录远程主机将不需要输入密码。\n```sh\n$ ssh kiwenlau@192.168.59.2\nWelcome to Ubuntu 14.04.2 LTS (GNU/Linux 3.13.0-32-generic x86_64)\n\n * Documentation:  https://help.ubuntu.com/\n\n  System information as of Sat Jun 20 15:39:25 JST 2015\n\n  System load:  0.0                Processes:              93\n  Usage of /:   13.0% of 17.34GB   Users logged in:        1\n  Memory usage: 22%                IP address for eth0:    192.168.59.2\n  Swap usage:   0%                 IP address for docker0: 172.17.42.1\n\n  Graph this data and manage this system at:\n    https://landscape.canonical.com/\n\n22 packages can be updated.\n2 updates are security updates.\n\nLast login: Sat Jun 20 15:39:25 2015 from 192.168.59.3\n```\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：[http://kiwenlau.com/2015/06/20/150620-ssh-copy-id-error/](http://kiwenlau.com/2015/06/20/150620-ssh-copy-id-error/)\n***\n\n\n\n\n\n\n\n","source":"_posts/150620-ssh-copy-id-error.md","raw":"title: ssh-copy-id出错 \"No such file\"\n\ndate: 2015-06-20 18:19:40\n\ntags: [Linux]\n\n---\n\n[ssh-copy-id](http://manpages.ubuntu.com/manpages/lucid/man1/ssh-copy-id.1.html)命令可以将本地的公钥添加到远程机器的authorized_keys中，从而实现SSH无密码登录。\n\n\n![](/image/150620/ssh-copy-id.png \"ssh-copy-id命令\")\n\n\n在使用ssh-copy-id命令时，出现了如下错误：\n```sh\n$ ssh-copy-id -i id_rsa.pub kiwenlau@136.187.59.2\n/usr/bin/ssh-copy-id: ERROR: failed to open ID file './id_rsa': No such file\n```\n\n命令的输出信息提示没有私钥\"id_rsa\"文件。可知，ssh-copy-id命令执行时将检查公钥id_rsa.pub所在目录是否存在私钥id_rsa，若没有私钥则会导致命令执行失败。\n\n这个问题的解决办法很简单，使用touch命令在公钥id_rsa.pub所在目录下创建一个空文件“id_rsa”即可。\n```sh\n$ touch id_rsa\n```\n\n然后ssh-copy-id命令就可以执行成功了。\n\n```sh\n$ ssh-copy-id -i id_rsa.pub kiwenlau@192.168.59.2\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys\n\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   \"ssh 'kiwenlau@192.168.59.2'\"\nand check to make sure that only the key(s) you wanted were added.\n```\n\n这时，使用ssh登录远程主机将不需要输入密码。\n```sh\n$ ssh kiwenlau@192.168.59.2\nWelcome to Ubuntu 14.04.2 LTS (GNU/Linux 3.13.0-32-generic x86_64)\n\n * Documentation:  https://help.ubuntu.com/\n\n  System information as of Sat Jun 20 15:39:25 JST 2015\n\n  System load:  0.0                Processes:              93\n  Usage of /:   13.0% of 17.34GB   Users logged in:        1\n  Memory usage: 22%                IP address for eth0:    192.168.59.2\n  Swap usage:   0%                 IP address for docker0: 172.17.42.1\n\n  Graph this data and manage this system at:\n    https://landscape.canonical.com/\n\n22 packages can be updated.\n2 updates are security updates.\n\nLast login: Sat Jun 20 15:39:25 2015 from 192.168.59.3\n```\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：[http://kiwenlau.com/2015/06/20/150620-ssh-copy-id-error/](http://kiwenlau.com/2015/06/20/150620-ssh-copy-id-error/)\n***\n\n\n\n\n\n\n\n","slug":"150620-ssh-copy-id-error","published":1,"updated":"2015-09-18T05:39:38.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cihhlj31h00150x8zh0m3j9ma"},{"title":"基于Docker快速搭建多节点Hadopp集群","date":"2015-06-08T03:44:40.000Z","_content":"\nGitHub: [kiwenlau/hadoop-cluster-docker](https://github.com/kiwenlau)\n\n可以直接进入第三部分，快速在本机搭建一个3个节点的Hadoop集群\n\n\n##一. 项目简介\n\n直接用机器搭建Hadoop集群是一个相当痛苦的过程，尤其对初学者来说。他们还没开始跑wordcount，可能就被这个问题折腾的体无完肤了。\n\n我的目标是将Hadoop集群运行在Docker容器中，使Hadoop开发者能够快速便捷地在本机搭建多节点的Hadoop集群。其实这个想法已经有了不少实现，但是都不是很理想，他们或者镜像太大，或者使用太慢，或者使用了第三方工具使得使用起来过于复杂...下表为一些已知的Hadoop on Docker项目以及其存在的问题。\n\n\n|                  项目                             | 镜像大小    |        问题                        |\n| : ------------- ---------------| : ------ | :-------------------- |\n|sequenceiq/hadoop-docker:latest  |1.491GB    | 镜像太大，只支持单个节点|\n|sequenceiq/hadoop-docker:2.7.0  |1.76 GB     |           同上                           |\n|sequenceiq/hadoop-docker:2.6.0  |1.624GB    |         同上                            |\n|sequenceiq/ambari:latest               |1.782GB     |  镜像太大，使用太慢|\n|sequenceiq/ambari:2.0.0               |4.804GB     |          同上                           |\n|sequenceiq/ambari:latest:1.70      |4.761GB    |         同上                         |\n|alvinhenrick/hadoop-mutinode     |4.331GB    |镜像太大，构建太慢，增加节点麻烦，有bug\n\n我的项目参考了alvinhenrick/hadoop-mutinode项目，不过我做了大量的优化和重构。alvinhenrick/hadoop-mutinode项目的Github主页以及作者所写的博客地址：[GitHub](https://github.com/alvinhenrick/hadoop-mutinode)，[博客](http://alvinhenrick.com/2014/07/16/hadoop-yarn-multinode-cluster-with-docker/)\n\n下面两个表是alvinhenrick/hadoop-mutinode项目与我的kiwenlau/hadoop-cluster-docker项目的参数对比\n\n|镜像名称\t                                 |构建时间\t       | 镜像层数   | 镜像大小 |\n| :-------------------------| :---------- | :------- | :------- |\n|alvinhenrick/serf                     |258.213s       | 21\t        | 239.4MB |\n|alvinhenrick/hadoop-base\t| 2236.055s    | 58\t        | 4.328GB |\n|alvinhenrick/hadoop-dn\t        | 51.959s        | 74\t        | 4.331GB |\n|alvinhenrick/hadoop-nn-dn    | 49.548s       |  84           | 4.331GB |\n\n|镜像名称\t                                 |构建时间\t       | 镜像层数   | 镜像大小 |\n| :-------------------------| :---------- | :------- | :------- |\n| kiwenlau/serf-dnsmasq          | 509.46s        |  8\t        | 206.6 MB |\n|kiwenlau/hadoop-base\t         | 400.29s\t        |  7\t        | 775.4 MB |\n|kiwenlau/hadoop-master         | 5.41s            |  9\t        | 775.4 MB |\n|kiwenlau/hadoop-slave\t         | 2.41s\t        |  8 \t        | 775.4 MB |\n\n可知，我主要优化了这样几点\n- 更小的镜像大小\n- 更快的构造时间\n- 更少的镜像层数\n\n####更快更方便地改变Hadoop集群节点数目\n\n另外，alvinhenrick/hadoop-mutinode项目增加节点时需要手动修改Hadoop配置文件然后重新构建hadoop-nn-dn镜像,然后修改容器启动脚本，才能实现增加节点的功能。而我通过shell脚本实现自动话，不到1分钟可以重新构建hadoop-master镜像，然后立即运行！！！本项目默认启动3个节点的Hadoop集群，支持任意节点数的hadoop集群。\n\n另外，启动hadoop, 运行wordcount以及重新构建镜像都采用了shell脚本实现自动化。这样使得整个项目的使用以及开发都变得非常方便快捷:)\n\n####开发测试环境\n\n- 操作系统：ubuntu 14.04 和 ubuntu 12.04\n- 内核版本: 3.13.0-32-generic\n- Docker版本：1.5.0 和1.6.2\n\n####硬盘不够，内存不够，尤其是内核版本过低会导致运行失败:(\n\n##二. 镜像简介\n\n###本项目一共开发了4个镜像\n\n- serf-dnsmasq\n- hadoop-base\n- hadoop-master\n- hadoop-slave\n\n###serf-dnsmasq镜像\n\n- 基于ubuntu:15.04 (选它是因为它最小，不是因为它最新...)\n- 安装serf: serf是一个分布式的机器节点管理工具。它可以动态地发现所有hadoop集群节点。\n- 安装dnsmasq: dnsmasq作为轻量级的dns服务器。它可以为hadoop集群提供域名解析服务。\n\n容器启动时，master节点的IP会传给所有slave节点。serf会在container启动后立即启动。slave节点上的serf agent会马上发现master节点（master IP它们都知道嘛），master节点就马上发现了所有slave节点。然后它们之间通过互相交换信息，所有节点就能知道其他所有节点的存在了！(Everyone will know Everyone). serf发现新的节点时，就会重新配置dnsmasq,然后重启dnsmasq. 所以dnsmasq就能够解析集群的所有节点的域名啦。这个过程随着节点的增加会耗时更久，因此，若配置的Hadoop节点比较多，则在启动容器后需要测试serf是否发现了所有节点，dns是否能够解析所有节点域名。稍等片刻才能启动Hadoop。这个解决方案是由SequenceIQ公司提出的，该公司专注于将Hadoop运行在Docker中。请参考这个PPT：[Docker-based Hadoop Provisioning](http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning)\n\n###hadoop-base镜像\n\n- 基于serf-dnsmasq镜像\n- 安装JDK(openjdk)\n- 安装openssh-server, 配置无密码ssh\n- 安装vim：介样就可以愉快地在容器中敲代码了:)\n- 安装Hadoop 2.3.0: 安装编译过的hadoop （2.5.2， 2.6.0， 2.7.0 都比2.3.0大，所以我懒得升级了）\n\n编译Hadoop的步骤请参考我的博客：[[Hadoop 2.30 在Ubuntu 14.04 中编译](http://www.cnblogs.com/kiwenlau/p/4227204.html)](http://www.cnblogs.com/kiwenlau/p/4227204.html)\n\n如果需要重新开发我的hadoop-base, 需要下载编译过的hadoop-2.3.0安装包，放到hadoop-cluster-docker/hadoop-base/files目录内。我编译的64位hadoop-2.3.0下载地址：[hadoop-2.3.0](http://pan.baidu.com/s/1sjFRaFz)\n\n另外，我还编译了64位的hadoop 2.5.2, 2.6.0, 2.7.0, 其下载地址如下：\n\n- [hadoop-2.3.0](http://pan.baidu.com/s/1sjFRaFz)\n- [hadoop-2.5.2](http://pan.baidu.com/s/1jGw24aa)\n- [hadoop-2.6.0](http://pan.baidu.com/s/1eQgvF2M)\n- [hadoop-2.7.0]( http://pan.baidu.com/s/1c0HD0Nu)\n\n###hadoop-master镜像\n\n- 基于hadoop-base镜像\n- 配置hadoop的master节点\n- 格式化namenode\n\n这一步需要配置slaves文件，而slaves文件需要列出所有节点的域名或者IP。因此，Hadoop节点数目不同时，slaves文件自然也不一样。因此，更改Hadoop集群节点数目时，需要修改slaves文件然后重新构建hadoop-master镜像。我编写了一个resize-cluster.sh脚本自动化这一过程。仅需给定节点数目作为脚本参数就可以轻松实现Hadoop集群节点数目的更改。由于hadoop-master镜像仅仅做一些配置工作，也无需下载任何文件，整个过程非常快，1分钟就足够了。\n\n###hadoop-slave镜像\n\n- 基于hadoop-base镜像\n- 配置hadoop的slave节点\n\n###镜像大小分析\n\n下表为sudo docker images的运行结果\n\n|REPOSITORY       |   TAG    |  IMAGE ID       | CREATED     |   VIRTUAL SIZE |\n| ------------- | ------- | ---------- | ---------- | ------- |\n|index.alauda.cn/kiwenlau/hadoop-slave   | 0.1.0|    d63869855c03 |   17 hours ago  |  777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-master|    0.1.0   | 7c9d32ede450  |  17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-base  |    0.1.0   | 5571bd5de58e    |17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/serf-dnsmasq  |  0.1.0    |09ed89c24ee8   | 17 hours ago  |  206.7 MB |\n|ubuntu     |                               15.04   | bd94ae587483  |  3 weeks ago    |131.3 MB |\n\n\n易知以下几个结论：\n- serf-dnsmasq镜像在ubuntu:15.04镜像的基础上增加了75.4MB\n- hadoop-base镜像在serf-dnsmasq镜像的基础上增加了570.7MB\n- hadoop-master和hadoop-slave镜像在hadoop-base镜像的基础上大小几乎没有增加\n\n下表为docker history index.alauda.cn/kiwenlau/hadoop-base:0.1.0命令的部分运行结果\n\n|IMAGE      |    CREATED        |    CREATED BY         |                             SIZE\n |  -----  | ---------------  | ---------------  |  -----------  | \n|2039b9b81146 |   44 hours ago   |       /bin/sh -c #(nop) ADD   multi:a93c971a49514e787  |  158.5 MB | \n | cdb620312f30    |  44 hours ago   |       /bin/sh -c apt-get install -y openjdk-7-jdk    |  324.6 MB  | \n | da7d10c790c1   |   44 hours ago      |    /bin/sh -c apt-get install -y openssh-server   |   87.58 MB  | \n |  c65cb568defc    |  44 hours ago     |     /bin/sh -c curl -Lso serf.zip https://dl.bint  |  14.46 MB  | \n | 3e22b3d72e33     | 44 hours ago       |   /bin/sh -c apt-get update && apt-get install     |  60.89 MB   | \n |  b68f8c8d2140    |  3 weeks ago     |     /bin/sh -c #(nop) ADD file:d90f7467c470bfa9a3  |   131.3 MB  | \n\n可知\n- 基础镜像ubuntu:15.04为131.3MB\n- 安装openjdk需要324.6MB\n- 安装hadoop需要158.5MB\n- ubuntu,openjdk与hadoop均为镜像所必须，三者一共占了:614.4MB\n\n### 因此，我所开发的hadoop镜像以及接近最小，优化空间已经很小了\n\n下图显示了项目的Docker镜像结构：\n\n![](/image/150608/image architecture.jpg \"Image Architecture\")\n\n##三. 3节点Hadoop集群搭建步骤\n\n###1. 拉取镜像\n\n```sh\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-master:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-slave:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-base:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/serf-dnsmasq:0.1.0\n```\n\n- 3~5分钟OK~\n\n*查看下载的镜像*\n\n```sh\nsudo docker images\n```\n\n*运行结果*\n\n|REPOSITORY    |    TAG  |    IMAGE ID      |  CREATED  |      VIRTUAL SIZE |\n |  ----------  |  -----------  |  ---------  |   --------  |  \n| index.alauda.cn/kiwenlau/hadoop-slave   | 0.1.0    |d63869855c03  |  17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-master   |   0.1.0 |     7c9d32ede450   |   17 hours ago  |    777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-base |       0.1.0     | 5571bd5de58e   |   17 hours ago  |    777.4 MB | \n |  index.alauda.cn/kiwenlau/serf-dnsmasq   |   0.1.0   |   09ed89c24ee8     | 17 hours ago     |  206.7 MB | \n\n- hadoop-base镜像是基于serf-dnsmasq镜像的，hadoop-slave镜像和hadoop-master镜像都是基于hadoop-base镜像\n- 所以其实4个镜像一共也就777.4MB:)\n\n###2. 修改镜像tag\n\n```sh\nsudo docker tag d63869855c03 kiwenlau/hadoop-slave:0.1.0\nsudo docker tag 7c9d32ede450 kiwenlau/hadoop-master:0.1.0\nsudo docker tag 5571bd5de58e kiwenlau/hadoop-base:0.1.0\nsudo docker tag 09ed89c24ee8 kiwenlau/serf-dnsmasq:0.1.0\n```\n\n*查看修改tag后镜像*\n\n```sh\nsudo docker images\n```\n\n*运行结果*\n\n| REPOSITORY   |   TAG   |    IMAGE ID    |    CREATED    |      VIRTUAL SIZE  | \n |  ----------  |  -----  |  ---------  |  ----------  |  -----------  | \n| index.alauda.cn/kiwenlau/hadoop-slave  |    0.1.0   |   d63869855c03    |  17 hours ago   |   777.4 MB\n | kiwenlau/hadoop-slave    |                  0.1.0   |   d63869855c03   |   17 hours ago  |    777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-master  |  0.1.0 |     7c9d32ede450   |   17 hours ago |     777.4 MB | \n | kiwenlau/hadoop-master  |                  0.1.0   |   7c9d32ede450   |   17 hours ago |     777.4 MB | \n | kiwenlau/hadoop-base   |                   0.1.0  |    5571bd5de58e  |    17 hours ago   |   777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-base  |    0.1.0   |   5571bd5de58e   |   17 hours ago |     777.4 MB | \n | kiwenlau/serf-dnsmasq          |            0.1.0    |  09ed89c24ee8  |    17 hours ago    |  206.7 MB | \n | index.alauda.cn/kiwenlau/serf-dnsmasq  |    0.1.0   |   09ed89c24ee8     | 17 hours ago   |   206.7 MB | \n\n- 之所以要修改镜像，是因为我默认是将镜像上传到Dockerhub, 因此Dokerfile以及shell脚本中得镜像名称都是没有alauada前缀的，sorry for this....不过改tag还是很快滴\n- 若直接下载我在DockerHub中的镜像，自然就不需要修改tag...不过Alauda镜像下载速度很快的哈~\n\n###3.下载源代码\n\n```sh\ngit clone https://github.com/kiwenlau/hadoop-cluster-docker\n```\n\n- 为了防止Github被XX, 我把代码导入到了开源中国的git仓库\n\n```sh\ngit clone http://git.oschina.net/kiwenlau/hadoop-cluster-docker\n```\n\n###4. 运行容器\n\n```sh\ncd hadoop-cluster-docker\n./start-container.sh\n\n```\n\n*运行结果*\n\n```bash\nstart master container...\nstart slave1 container...\nstart slave2 container...\nroot@master:~#\n```\n\n- 一共开启了3个容器，1个master, 2个slave\n- 开启容器后就进入了master容器root用户的家目录（/root）\n\n*查看master的root用户家目录的文件*\n\n```sh\nls\n```\n\n*运行结果*\n\n```plain\nhdfs  run-wordcount.sh\tserf_log  start-hadoop.sh  start-ssh-serf.sh\n```\n\n- start-hadoop.sh是开启hadoop的shell脚本\n- run-wordcount.sh是运行wordcount的shell脚本，可以测试镜像是否正常工作\n\n###5.测试容器是否正常启动(此时已进入master容器)\n\n*查看hadoop集群成员*\n\n```sh\nserf members\n```\n\n*运行结果*\n\n```bash\nmaster.kiwenlau.com  172.17.0.65:7946  alive\nslave1.kiwenlau.com  172.17.0.66:7946  alive\nslave2.kiwenlau.com  172.17.0.67:7946  alive\n```\n\n- 若结果缺少节点，可以稍等片刻，再执行“serf members”命令。因为serf agent需要时间发现所有节点。\n\n*测试ssh*\n\n```sh\nssh slave2.kiwenlau.com\n```\n\n*运行结果*\n\n```bash\nWarning: Permanently added 'slave2.kiwenlau.com,172.17.0.67' (ECDSA) to the list of known hosts.\nWelcome to Ubuntu 15.04 (GNU/Linux 3.13.0-53-generic x86_64)\n* Documentation:  https://help.ubuntu.com/\nThe programs included with the Ubuntu system are free software;\nthe exact distribution terms for each program are described in the\nindividual files in /usr/share/doc/*/copyright.\nUbuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\napplicable law.\nroot@slave2:~#\n```\n\n*退出slave2*\n\n```sh\nexit\n```\n\n*运行结果*\n\n```bash\nlogout\nConnection to slave2.kiwenlau.com closed.\n```\n\n- 若ssh失败，请稍等片刻再测试，因为dnsmasq的dns服务器启动需要时间。\n- 测试成功后，就可以开启Hadoop集群了！其实你也可以不进行测试，开启容器后耐心等待一分钟即可！\n\n###6. 开启hadoop\n\n```sh\n./start-hadoop.sh\n```\n\n- 上一步ssh到slave2之后，请记得回到master啊!!！\n- 运行结果太多，忽略....\n- hadoop的启动速度取决于机器性能....\n\n###7. 运行wordcount\n\n```sh\n./run-wordcount.sh\n```\n\n*运行结果*\n```bash\ninput file1.txt:\nHello Hadoop\ninput file2.txt:\nHello Docker\nwordcount output:\nDocker\t1\nHadoop\t1\nHello\t2\n```\n\n- wordcount的执行速度取决于机器性能....\n\n##四. N节点Hadoop集群搭建步骤\n\n###1. 准备工作\n\n- 参考第二部分1~3：下载镜像，修改tag，下载源代码\n- 注意，你可以不下载serf-dnsmasq, 但是请最好下载hadoop-base，因为hadoop-master是基于hadoop-base构建的\n\n###2. 重新构建hadoop-master镜像\n\n```sh\n./resize-cluster.sh 5\n```\n\n- 不要担心，1分钟就能搞定\n- 你可以为resize-cluster.sh脚本设不同的正整数作为参数数1, 2, 3, 4, 5, 6...\n\n###3. 启动容器\n\n```sh\n./start-container.sh 5\n```\n\n- 你可以为resize-cluster.sh脚本设不同的正整数作为参数数1, 2, 3, 4, 5, 6...\n- 这个参数呢，最好还是得和上一步的参数一致:)\n- 这个参数如果比上一步的参数大，你多启动的节点，Hadoop不认识它们..\n- 这个参数如果比上一步的参数小，Hadoop觉得少启动的节点挂掉了..\n\n###4. 测试工作\n\n- 参考第三部分5~7：测试容器，开启Hadoop，运行wordcount\n- 请注意，若节点增加，请务必先测试容器，然后再开启Hadoop, 因为serf可能还没有发现所有节点，而dnsmasq的DNS服务器表示还没有配置好服务\n- 测试等待时间取决于机器性能....\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/](http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/)\n***","source":"_posts/150608-hadoop-cluster-docker.md","raw":"title: 基于Docker快速搭建多节点Hadopp集群\n\ndate: 2015-06-08 12:44:40\n\ntags: [Hadoop, Docker]\n\n---\n\nGitHub: [kiwenlau/hadoop-cluster-docker](https://github.com/kiwenlau)\n\n可以直接进入第三部分，快速在本机搭建一个3个节点的Hadoop集群\n\n\n##一. 项目简介\n\n直接用机器搭建Hadoop集群是一个相当痛苦的过程，尤其对初学者来说。他们还没开始跑wordcount，可能就被这个问题折腾的体无完肤了。\n\n我的目标是将Hadoop集群运行在Docker容器中，使Hadoop开发者能够快速便捷地在本机搭建多节点的Hadoop集群。其实这个想法已经有了不少实现，但是都不是很理想，他们或者镜像太大，或者使用太慢，或者使用了第三方工具使得使用起来过于复杂...下表为一些已知的Hadoop on Docker项目以及其存在的问题。\n\n\n|                  项目                             | 镜像大小    |        问题                        |\n| : ------------- ---------------| : ------ | :-------------------- |\n|sequenceiq/hadoop-docker:latest  |1.491GB    | 镜像太大，只支持单个节点|\n|sequenceiq/hadoop-docker:2.7.0  |1.76 GB     |           同上                           |\n|sequenceiq/hadoop-docker:2.6.0  |1.624GB    |         同上                            |\n|sequenceiq/ambari:latest               |1.782GB     |  镜像太大，使用太慢|\n|sequenceiq/ambari:2.0.0               |4.804GB     |          同上                           |\n|sequenceiq/ambari:latest:1.70      |4.761GB    |         同上                         |\n|alvinhenrick/hadoop-mutinode     |4.331GB    |镜像太大，构建太慢，增加节点麻烦，有bug\n\n我的项目参考了alvinhenrick/hadoop-mutinode项目，不过我做了大量的优化和重构。alvinhenrick/hadoop-mutinode项目的Github主页以及作者所写的博客地址：[GitHub](https://github.com/alvinhenrick/hadoop-mutinode)，[博客](http://alvinhenrick.com/2014/07/16/hadoop-yarn-multinode-cluster-with-docker/)\n\n下面两个表是alvinhenrick/hadoop-mutinode项目与我的kiwenlau/hadoop-cluster-docker项目的参数对比\n\n|镜像名称\t                                 |构建时间\t       | 镜像层数   | 镜像大小 |\n| :-------------------------| :---------- | :------- | :------- |\n|alvinhenrick/serf                     |258.213s       | 21\t        | 239.4MB |\n|alvinhenrick/hadoop-base\t| 2236.055s    | 58\t        | 4.328GB |\n|alvinhenrick/hadoop-dn\t        | 51.959s        | 74\t        | 4.331GB |\n|alvinhenrick/hadoop-nn-dn    | 49.548s       |  84           | 4.331GB |\n\n|镜像名称\t                                 |构建时间\t       | 镜像层数   | 镜像大小 |\n| :-------------------------| :---------- | :------- | :------- |\n| kiwenlau/serf-dnsmasq          | 509.46s        |  8\t        | 206.6 MB |\n|kiwenlau/hadoop-base\t         | 400.29s\t        |  7\t        | 775.4 MB |\n|kiwenlau/hadoop-master         | 5.41s            |  9\t        | 775.4 MB |\n|kiwenlau/hadoop-slave\t         | 2.41s\t        |  8 \t        | 775.4 MB |\n\n可知，我主要优化了这样几点\n- 更小的镜像大小\n- 更快的构造时间\n- 更少的镜像层数\n\n####更快更方便地改变Hadoop集群节点数目\n\n另外，alvinhenrick/hadoop-mutinode项目增加节点时需要手动修改Hadoop配置文件然后重新构建hadoop-nn-dn镜像,然后修改容器启动脚本，才能实现增加节点的功能。而我通过shell脚本实现自动话，不到1分钟可以重新构建hadoop-master镜像，然后立即运行！！！本项目默认启动3个节点的Hadoop集群，支持任意节点数的hadoop集群。\n\n另外，启动hadoop, 运行wordcount以及重新构建镜像都采用了shell脚本实现自动化。这样使得整个项目的使用以及开发都变得非常方便快捷:)\n\n####开发测试环境\n\n- 操作系统：ubuntu 14.04 和 ubuntu 12.04\n- 内核版本: 3.13.0-32-generic\n- Docker版本：1.5.0 和1.6.2\n\n####硬盘不够，内存不够，尤其是内核版本过低会导致运行失败:(\n\n##二. 镜像简介\n\n###本项目一共开发了4个镜像\n\n- serf-dnsmasq\n- hadoop-base\n- hadoop-master\n- hadoop-slave\n\n###serf-dnsmasq镜像\n\n- 基于ubuntu:15.04 (选它是因为它最小，不是因为它最新...)\n- 安装serf: serf是一个分布式的机器节点管理工具。它可以动态地发现所有hadoop集群节点。\n- 安装dnsmasq: dnsmasq作为轻量级的dns服务器。它可以为hadoop集群提供域名解析服务。\n\n容器启动时，master节点的IP会传给所有slave节点。serf会在container启动后立即启动。slave节点上的serf agent会马上发现master节点（master IP它们都知道嘛），master节点就马上发现了所有slave节点。然后它们之间通过互相交换信息，所有节点就能知道其他所有节点的存在了！(Everyone will know Everyone). serf发现新的节点时，就会重新配置dnsmasq,然后重启dnsmasq. 所以dnsmasq就能够解析集群的所有节点的域名啦。这个过程随着节点的增加会耗时更久，因此，若配置的Hadoop节点比较多，则在启动容器后需要测试serf是否发现了所有节点，dns是否能够解析所有节点域名。稍等片刻才能启动Hadoop。这个解决方案是由SequenceIQ公司提出的，该公司专注于将Hadoop运行在Docker中。请参考这个PPT：[Docker-based Hadoop Provisioning](http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning)\n\n###hadoop-base镜像\n\n- 基于serf-dnsmasq镜像\n- 安装JDK(openjdk)\n- 安装openssh-server, 配置无密码ssh\n- 安装vim：介样就可以愉快地在容器中敲代码了:)\n- 安装Hadoop 2.3.0: 安装编译过的hadoop （2.5.2， 2.6.0， 2.7.0 都比2.3.0大，所以我懒得升级了）\n\n编译Hadoop的步骤请参考我的博客：[[Hadoop 2.30 在Ubuntu 14.04 中编译](http://www.cnblogs.com/kiwenlau/p/4227204.html)](http://www.cnblogs.com/kiwenlau/p/4227204.html)\n\n如果需要重新开发我的hadoop-base, 需要下载编译过的hadoop-2.3.0安装包，放到hadoop-cluster-docker/hadoop-base/files目录内。我编译的64位hadoop-2.3.0下载地址：[hadoop-2.3.0](http://pan.baidu.com/s/1sjFRaFz)\n\n另外，我还编译了64位的hadoop 2.5.2, 2.6.0, 2.7.0, 其下载地址如下：\n\n- [hadoop-2.3.0](http://pan.baidu.com/s/1sjFRaFz)\n- [hadoop-2.5.2](http://pan.baidu.com/s/1jGw24aa)\n- [hadoop-2.6.0](http://pan.baidu.com/s/1eQgvF2M)\n- [hadoop-2.7.0]( http://pan.baidu.com/s/1c0HD0Nu)\n\n###hadoop-master镜像\n\n- 基于hadoop-base镜像\n- 配置hadoop的master节点\n- 格式化namenode\n\n这一步需要配置slaves文件，而slaves文件需要列出所有节点的域名或者IP。因此，Hadoop节点数目不同时，slaves文件自然也不一样。因此，更改Hadoop集群节点数目时，需要修改slaves文件然后重新构建hadoop-master镜像。我编写了一个resize-cluster.sh脚本自动化这一过程。仅需给定节点数目作为脚本参数就可以轻松实现Hadoop集群节点数目的更改。由于hadoop-master镜像仅仅做一些配置工作，也无需下载任何文件，整个过程非常快，1分钟就足够了。\n\n###hadoop-slave镜像\n\n- 基于hadoop-base镜像\n- 配置hadoop的slave节点\n\n###镜像大小分析\n\n下表为sudo docker images的运行结果\n\n|REPOSITORY       |   TAG    |  IMAGE ID       | CREATED     |   VIRTUAL SIZE |\n| ------------- | ------- | ---------- | ---------- | ------- |\n|index.alauda.cn/kiwenlau/hadoop-slave   | 0.1.0|    d63869855c03 |   17 hours ago  |  777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-master|    0.1.0   | 7c9d32ede450  |  17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-base  |    0.1.0   | 5571bd5de58e    |17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/serf-dnsmasq  |  0.1.0    |09ed89c24ee8   | 17 hours ago  |  206.7 MB |\n|ubuntu     |                               15.04   | bd94ae587483  |  3 weeks ago    |131.3 MB |\n\n\n易知以下几个结论：\n- serf-dnsmasq镜像在ubuntu:15.04镜像的基础上增加了75.4MB\n- hadoop-base镜像在serf-dnsmasq镜像的基础上增加了570.7MB\n- hadoop-master和hadoop-slave镜像在hadoop-base镜像的基础上大小几乎没有增加\n\n下表为docker history index.alauda.cn/kiwenlau/hadoop-base:0.1.0命令的部分运行结果\n\n|IMAGE      |    CREATED        |    CREATED BY         |                             SIZE\n |  -----  | ---------------  | ---------------  |  -----------  | \n|2039b9b81146 |   44 hours ago   |       /bin/sh -c #(nop) ADD   multi:a93c971a49514e787  |  158.5 MB | \n | cdb620312f30    |  44 hours ago   |       /bin/sh -c apt-get install -y openjdk-7-jdk    |  324.6 MB  | \n | da7d10c790c1   |   44 hours ago      |    /bin/sh -c apt-get install -y openssh-server   |   87.58 MB  | \n |  c65cb568defc    |  44 hours ago     |     /bin/sh -c curl -Lso serf.zip https://dl.bint  |  14.46 MB  | \n | 3e22b3d72e33     | 44 hours ago       |   /bin/sh -c apt-get update && apt-get install     |  60.89 MB   | \n |  b68f8c8d2140    |  3 weeks ago     |     /bin/sh -c #(nop) ADD file:d90f7467c470bfa9a3  |   131.3 MB  | \n\n可知\n- 基础镜像ubuntu:15.04为131.3MB\n- 安装openjdk需要324.6MB\n- 安装hadoop需要158.5MB\n- ubuntu,openjdk与hadoop均为镜像所必须，三者一共占了:614.4MB\n\n### 因此，我所开发的hadoop镜像以及接近最小，优化空间已经很小了\n\n下图显示了项目的Docker镜像结构：\n\n![](/image/150608/image architecture.jpg \"Image Architecture\")\n\n##三. 3节点Hadoop集群搭建步骤\n\n###1. 拉取镜像\n\n```sh\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-master:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-slave:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-base:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/serf-dnsmasq:0.1.0\n```\n\n- 3~5分钟OK~\n\n*查看下载的镜像*\n\n```sh\nsudo docker images\n```\n\n*运行结果*\n\n|REPOSITORY    |    TAG  |    IMAGE ID      |  CREATED  |      VIRTUAL SIZE |\n |  ----------  |  -----------  |  ---------  |   --------  |  \n| index.alauda.cn/kiwenlau/hadoop-slave   | 0.1.0    |d63869855c03  |  17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-master   |   0.1.0 |     7c9d32ede450   |   17 hours ago  |    777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-base |       0.1.0     | 5571bd5de58e   |   17 hours ago  |    777.4 MB | \n |  index.alauda.cn/kiwenlau/serf-dnsmasq   |   0.1.0   |   09ed89c24ee8     | 17 hours ago     |  206.7 MB | \n\n- hadoop-base镜像是基于serf-dnsmasq镜像的，hadoop-slave镜像和hadoop-master镜像都是基于hadoop-base镜像\n- 所以其实4个镜像一共也就777.4MB:)\n\n###2. 修改镜像tag\n\n```sh\nsudo docker tag d63869855c03 kiwenlau/hadoop-slave:0.1.0\nsudo docker tag 7c9d32ede450 kiwenlau/hadoop-master:0.1.0\nsudo docker tag 5571bd5de58e kiwenlau/hadoop-base:0.1.0\nsudo docker tag 09ed89c24ee8 kiwenlau/serf-dnsmasq:0.1.0\n```\n\n*查看修改tag后镜像*\n\n```sh\nsudo docker images\n```\n\n*运行结果*\n\n| REPOSITORY   |   TAG   |    IMAGE ID    |    CREATED    |      VIRTUAL SIZE  | \n |  ----------  |  -----  |  ---------  |  ----------  |  -----------  | \n| index.alauda.cn/kiwenlau/hadoop-slave  |    0.1.0   |   d63869855c03    |  17 hours ago   |   777.4 MB\n | kiwenlau/hadoop-slave    |                  0.1.0   |   d63869855c03   |   17 hours ago  |    777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-master  |  0.1.0 |     7c9d32ede450   |   17 hours ago |     777.4 MB | \n | kiwenlau/hadoop-master  |                  0.1.0   |   7c9d32ede450   |   17 hours ago |     777.4 MB | \n | kiwenlau/hadoop-base   |                   0.1.0  |    5571bd5de58e  |    17 hours ago   |   777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-base  |    0.1.0   |   5571bd5de58e   |   17 hours ago |     777.4 MB | \n | kiwenlau/serf-dnsmasq          |            0.1.0    |  09ed89c24ee8  |    17 hours ago    |  206.7 MB | \n | index.alauda.cn/kiwenlau/serf-dnsmasq  |    0.1.0   |   09ed89c24ee8     | 17 hours ago   |   206.7 MB | \n\n- 之所以要修改镜像，是因为我默认是将镜像上传到Dockerhub, 因此Dokerfile以及shell脚本中得镜像名称都是没有alauada前缀的，sorry for this....不过改tag还是很快滴\n- 若直接下载我在DockerHub中的镜像，自然就不需要修改tag...不过Alauda镜像下载速度很快的哈~\n\n###3.下载源代码\n\n```sh\ngit clone https://github.com/kiwenlau/hadoop-cluster-docker\n```\n\n- 为了防止Github被XX, 我把代码导入到了开源中国的git仓库\n\n```sh\ngit clone http://git.oschina.net/kiwenlau/hadoop-cluster-docker\n```\n\n###4. 运行容器\n\n```sh\ncd hadoop-cluster-docker\n./start-container.sh\n\n```\n\n*运行结果*\n\n```bash\nstart master container...\nstart slave1 container...\nstart slave2 container...\nroot@master:~#\n```\n\n- 一共开启了3个容器，1个master, 2个slave\n- 开启容器后就进入了master容器root用户的家目录（/root）\n\n*查看master的root用户家目录的文件*\n\n```sh\nls\n```\n\n*运行结果*\n\n```plain\nhdfs  run-wordcount.sh\tserf_log  start-hadoop.sh  start-ssh-serf.sh\n```\n\n- start-hadoop.sh是开启hadoop的shell脚本\n- run-wordcount.sh是运行wordcount的shell脚本，可以测试镜像是否正常工作\n\n###5.测试容器是否正常启动(此时已进入master容器)\n\n*查看hadoop集群成员*\n\n```sh\nserf members\n```\n\n*运行结果*\n\n```bash\nmaster.kiwenlau.com  172.17.0.65:7946  alive\nslave1.kiwenlau.com  172.17.0.66:7946  alive\nslave2.kiwenlau.com  172.17.0.67:7946  alive\n```\n\n- 若结果缺少节点，可以稍等片刻，再执行“serf members”命令。因为serf agent需要时间发现所有节点。\n\n*测试ssh*\n\n```sh\nssh slave2.kiwenlau.com\n```\n\n*运行结果*\n\n```bash\nWarning: Permanently added 'slave2.kiwenlau.com,172.17.0.67' (ECDSA) to the list of known hosts.\nWelcome to Ubuntu 15.04 (GNU/Linux 3.13.0-53-generic x86_64)\n* Documentation:  https://help.ubuntu.com/\nThe programs included with the Ubuntu system are free software;\nthe exact distribution terms for each program are described in the\nindividual files in /usr/share/doc/*/copyright.\nUbuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\napplicable law.\nroot@slave2:~#\n```\n\n*退出slave2*\n\n```sh\nexit\n```\n\n*运行结果*\n\n```bash\nlogout\nConnection to slave2.kiwenlau.com closed.\n```\n\n- 若ssh失败，请稍等片刻再测试，因为dnsmasq的dns服务器启动需要时间。\n- 测试成功后，就可以开启Hadoop集群了！其实你也可以不进行测试，开启容器后耐心等待一分钟即可！\n\n###6. 开启hadoop\n\n```sh\n./start-hadoop.sh\n```\n\n- 上一步ssh到slave2之后，请记得回到master啊!!！\n- 运行结果太多，忽略....\n- hadoop的启动速度取决于机器性能....\n\n###7. 运行wordcount\n\n```sh\n./run-wordcount.sh\n```\n\n*运行结果*\n```bash\ninput file1.txt:\nHello Hadoop\ninput file2.txt:\nHello Docker\nwordcount output:\nDocker\t1\nHadoop\t1\nHello\t2\n```\n\n- wordcount的执行速度取决于机器性能....\n\n##四. N节点Hadoop集群搭建步骤\n\n###1. 准备工作\n\n- 参考第二部分1~3：下载镜像，修改tag，下载源代码\n- 注意，你可以不下载serf-dnsmasq, 但是请最好下载hadoop-base，因为hadoop-master是基于hadoop-base构建的\n\n###2. 重新构建hadoop-master镜像\n\n```sh\n./resize-cluster.sh 5\n```\n\n- 不要担心，1分钟就能搞定\n- 你可以为resize-cluster.sh脚本设不同的正整数作为参数数1, 2, 3, 4, 5, 6...\n\n###3. 启动容器\n\n```sh\n./start-container.sh 5\n```\n\n- 你可以为resize-cluster.sh脚本设不同的正整数作为参数数1, 2, 3, 4, 5, 6...\n- 这个参数呢，最好还是得和上一步的参数一致:)\n- 这个参数如果比上一步的参数大，你多启动的节点，Hadoop不认识它们..\n- 这个参数如果比上一步的参数小，Hadoop觉得少启动的节点挂掉了..\n\n###4. 测试工作\n\n- 参考第三部分5~7：测试容器，开启Hadoop，运行wordcount\n- 请注意，若节点增加，请务必先测试容器，然后再开启Hadoop, 因为serf可能还没有发现所有节点，而dnsmasq的DNS服务器表示还没有配置好服务\n- 测试等待时间取决于机器性能....\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/](http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/)\n***","slug":"150608-hadoop-cluster-docker","published":1,"updated":"2015-09-25T07:51:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cihhlj31j00170x8zk8c18bc1"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cihhlj2zr00010x8z68kdaa5d","tag_id":"cihhlj2zy00030x8zgij4nw2c","_id":"cihhlj2zy00050x8zm160bymr"},{"post_id":"cihhlj30800060x8zu75kfmdn","tag_id":"cihhlj2zw00020x8zsli6dc8u","_id":"cihhlj30b00070x8zfcw0h7tj"},{"post_id":"cihhlj30d00080x8zam9lsqr3","tag_id":"cihhlj2zw00020x8zsli6dc8u","_id":"cihhlj30e00090x8zjg8wdib8"},{"post_id":"cihhlj30f000a0x8zphlej4o2","tag_id":"cihhlj30g000b0x8zyj6peqw4","_id":"cihhlj30h000c0x8zkm3x9ai2"},{"post_id":"cihhlj30i000d0x8zgqss6i3z","tag_id":"cihhlj30j000e0x8zuh0o8uqm","_id":"cihhlj30j000f0x8z1fwepa9s"},{"post_id":"cihhlj30k000g0x8z9txob102","tag_id":"cihhlj30n000h0x8zgqmsonbq","_id":"cihhlj30o000i0x8zyev3e1do"},{"post_id":"cihhlj30r000j0x8zw010zpyz","tag_id":"cihhlj30n000h0x8zgqmsonbq","_id":"cihhlj30s000k0x8zlsi77vox"},{"post_id":"cihhlj30u000l0x8zwag4vuli","tag_id":"cihhlj30v000m0x8zqhp8t0b6","_id":"cihhlj30v000n0x8zc2e1vary"},{"post_id":"cihhlj30x000o0x8zx2xdklw6","tag_id":"cihhlj30g000b0x8zyj6peqw4","_id":"cihhlj30z000p0x8zm0zhptld"},{"post_id":"cihhlj311000q0x8zud376s7o","tag_id":"cihhlj30n000h0x8zgqmsonbq","_id":"cihhlj312000r0x8zfih9bes3"},{"post_id":"cihhlj314000s0x8zukldr1db","tag_id":"cihhlj30n000h0x8zgqmsonbq","_id":"cihhlj316000v0x8zggxw380v"},{"post_id":"cihhlj314000s0x8zukldr1db","tag_id":"cihhlj315000t0x8z84ezwczd","_id":"cihhlj317000w0x8zzjazf2rg"},{"post_id":"cihhlj314000s0x8zukldr1db","tag_id":"cihhlj316000u0x8zkqk9epp8","_id":"cihhlj317000x0x8zvhln4wur"},{"post_id":"cihhlj319000y0x8zrk84quur","tag_id":"cihhlj2zw00020x8zsli6dc8u","_id":"cihhlj31a000z0x8z6uszxgw1"},{"post_id":"cihhlj31b00100x8z8donptck","tag_id":"cihhlj2zw00020x8zsli6dc8u","_id":"cihhlj31d00110x8ze3uee9u8"},{"post_id":"cihhlj31e00120x8zsf3f292k","tag_id":"cihhlj31f00130x8z4ac1yiln","_id":"cihhlj31g00140x8zxh8mgrqk"},{"post_id":"cihhlj31h00150x8zh0m3j9ma","tag_id":"cihhlj2zw00020x8zsli6dc8u","_id":"cihhlj31i00160x8zll37j650"},{"post_id":"cihhlj31j00170x8zk8c18bc1","tag_id":"cihhlj30v000m0x8zqhp8t0b6","_id":"cihhlj31l00180x8zfn584q3f"},{"post_id":"cihhlj31j00170x8zk8c18bc1","tag_id":"cihhlj30n000h0x8zgqmsonbq","_id":"cihhlj31l00190x8za9i9uyxj"},{"post_id":"cihhlj2zr00010x8z68kdaa5d","tag_id":"cihhlj30n000h0x8zgqmsonbq","_id":"cihhlpv6n00001e8z6t7k5244"}],"Tag":[{"name":"Linux","_id":"cihhlj2zw00020x8zsli6dc8u"},{"name":"Kubernetes","_id":"cihhlj2zy00030x8zgij4nw2c"},{"name":"VirtualBox","_id":"cihhlj30g000b0x8zyj6peqw4"},{"name":"MPI","_id":"cihhlj30j000e0x8zuh0o8uqm"},{"name":"Docker","_id":"cihhlj30n000h0x8zgqmsonbq"},{"name":"Hadoop","_id":"cihhlj30v000m0x8zqhp8t0b6"},{"name":"Mesos","_id":"cihhlj315000t0x8z84ezwczd"},{"name":"Marathon","_id":"cihhlj316000u0x8zkqk9epp8"},{"name":"Python","_id":"cihhlj31f00130x8z4ac1yiln"}]}}