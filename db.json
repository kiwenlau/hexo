{"meta":{"version":1,"warehouse":"1.0.3"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0},{"_id":"source/image/160109/kubernetes-shell-supervisor.png","path":"image/160109/kubernetes-shell-supervisor.png","modified":0},{"_id":"source/image/151128/single-kubernetes-docker.png","path":"image/151128/single-kubernetes-docker.png","modified":0},{"_id":"source/image/150918/kiwenlau_single_mesos.graffle","path":"image/150918/kiwenlau_single_mesos.graffle","modified":0},{"_id":"source/image/150918/hello.png","path":"image/150918/hello.png","modified":0},{"_id":"source/image/150918/architecuture.png","path":"image/150918/architecuture.png","modified":0},{"_id":"source/image/150918/Mesos.png","path":"image/150918/Mesos.png","modified":0},{"_id":"source/image/150918/Marathon.png","path":"image/150918/Marathon.png","modified":0},{"_id":"source/image/150608/image architecture.jpg","path":"image/150608/image architecture.jpg","modified":0},{"_id":"themes/light/source/kiwenlau.jpg","path":"kiwenlau.jpg","modified":0},{"_id":"themes/light/source/kiwenlau.ico","path":"kiwenlau.ico","modified":0},{"_id":"themes/light/source/js/jquery.imagesloaded.min.js","path":"js/jquery.imagesloaded.min.js","modified":0},{"_id":"themes/light/source/js/gallery.js","path":"js/gallery.js","modified":0},{"_id":"themes/light/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0},{"_id":"themes/light/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0},{"_id":"themes/light/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0},{"_id":"themes/light/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0},{"_id":"themes/light/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0},{"_id":"themes/light/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0},{"_id":"themes/light/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0},{"_id":"themes/light/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0},{"_id":"themes/light/source/css/style.styl","path":"css/style.styl","modified":0},{"_id":"themes/light/source/css/font/fontawesome-webfont.woff","path":"css/font/fontawesome-webfont.woff","modified":0},{"_id":"themes/light/source/css/font/fontawesome-webfont.ttf","path":"css/font/fontawesome-webfont.ttf","modified":0},{"_id":"themes/light/source/css/font/fontawesome-webfont.svg","path":"css/font/fontawesome-webfont.svg","modified":0},{"_id":"themes/light/source/css/font/fontawesome-webfont.eot","path":"css/font/fontawesome-webfont.eot","modified":0}],"Cache":[{"_id":"source/CNAME","shasum":"abcdc15ac03b8fdb087e2ae5c120ef20dd63dcd1","modified":1433732308000},{"_id":"source/_posts/151128-single-kubernetes-docker.md","shasum":"a4c3dd901b90bf36c9b00be2ac516b1f7057aea0","modified":1449515240000},{"_id":"source/_posts/150608-hadoop-cluster-docker.md","shasum":"1d16575391a05d1bf3adf881769b11b809ba3e22","modified":1449515231000},{"_id":"source/google6c2fe14874348911.html","shasum":"13b1c75c0ffea7dd64b61e99c75e328e81812700","modified":1433934386000},{"_id":"source/_posts/150918-single-mesos-docker.md","shasum":"9fb78b306d5a2edebf9116d65b80e8b71f159632","modified":1449515058000},{"_id":"source/_posts/160109-multiple-processes--docker-container.md","shasum":"a5d6be4b2db7ed2abc479d7c0946dfa645e1fb40","modified":1452129128000},{"_id":"source/image/150608/image architecture.jpg","shasum":"450f2dffe1127f102221a9b3c934eb6054876d85","modified":1433822708000},{"_id":"source/image/150918/architecuture.png","shasum":"43b51d2b17bb54c7c11530d40f1068a558ebde10","modified":1442408879000},{"_id":"source/image/160109/kubernetes-shell-supervisor.png","shasum":"1e8f251362d7875128ce8cfee1af5bccf2d7a693","modified":1452062686000},{"_id":"source/image/150918/kiwenlau_single_mesos.graffle","shasum":"3fcf8e0540f83077eac1114e5b431e64886f19ca","modified":1442407988000},{"_id":"source/image/151128/single-kubernetes-docker.png","shasum":"75ee08265d42589d56dd36ac764d40521af2fe8c","modified":1448502361000},{"_id":"source/image/150918/Marathon.png","shasum":"58abd8a1ba30eedef47649093a37291e29f21e00","modified":1442402866000},{"_id":"source/image/150918/hello.png","shasum":"732ba80324b66a3ff47fafbde89f4c61dfb04a08","modified":1442471270000},{"_id":"source/image/150918/Mesos.png","shasum":"c21fb25a6da8801ecb9b6b25888cb88da16790b4","modified":1442402816000},{"_id":"themes/light/source/css/_base/utils.styl","shasum":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1433597960000},{"_id":"themes/light/LICENSE","shasum":"c6f301bc722f0af3a55267a36c1c147aeddc6e46","modified":1433597960000},{"_id":"themes/light/README.md","shasum":"aa189c7ff03c60d8fceb009f5fca1a61d8a0ecdf","modified":1433597960000},{"_id":"themes/light/_config.yml","shasum":"c3729dc713e5b3dc3a1d5bf0ac1be76bf997a8bf","modified":1433929122000},{"_id":"themes/light/languages/de.yml","shasum":"e076c7f2eb29ebcfb04d94861bf3063c4b08078c","modified":1433597960000},{"_id":"themes/light/languages/default.yml","shasum":"fd7397be7789b43c1c163ab4faf106318811c2a8","modified":1433597960000},{"_id":"themes/light/languages/es.yml","shasum":"de273af604b27812cfd4195e7b7f28ceff2734b3","modified":1433597960000},{"_id":"themes/light/languages/zh-TW.yml","shasum":"6141b4c7a094c74bd9df7c08908d92b561c1a0c0","modified":1433597960000},{"_id":"themes/light/languages/ru.yml","shasum":"35aadf8fdd28aaff8a1c8f50e80201dcf8ce0604","modified":1433597960000},{"_id":"themes/light/languages/zh-CN.yml","shasum":"ca0118e9081b54cc0fca8596660bd6acf4c0308f","modified":1433597960000},{"_id":"themes/light/layout/_partial/after_footer.ejs","shasum":"e3fc00ab06a8e41051602b65ef8a4f968a4bf2bb","modified":1433941216000},{"_id":"themes/light/layout/_partial/comment.ejs","shasum":"c5409a6920f96816506b4e0d5dfae13449021654","modified":1433609464000},{"_id":"themes/light/layout/_partial/article.ejs","shasum":"bcae2ea030e69242d938c33094fc60207bf25e22","modified":1433609506000},{"_id":"themes/light/layout/_partial/facebook_comment.ejs","shasum":"3fdc1d0ce9177825e7417635fbc545a35d528d04","modified":1433597960000},{"_id":"themes/light/layout/_partial/google_analytics.ejs","shasum":"7cf0d1f93051bda510d49dab7f684b9d7c6ba58f","modified":1433597960000},{"_id":"themes/light/layout/_partial/archive.ejs","shasum":"7e4f7c2909b1b90241424ea2ff8e7b4761d8360f","modified":1433597960000},{"_id":"themes/light/layout/_partial/footer.ejs","shasum":"61224149335ae515b0c23e27070c11b05d78749d","modified":1434024998000},{"_id":"themes/light/layout/_partial/post/category.ejs","shasum":"be740939c5c2d4ffdbed9557b4e63a590058b476","modified":1433597960000},{"_id":"themes/light/layout/_partial/pagination.ejs","shasum":"1206b630a07444e8744365f14ddb26095c925ae1","modified":1433597960000},{"_id":"themes/light/layout/_partial/header.ejs","shasum":"d9a99aca97d8b41ed907fbf5b25df05da3ffa4f6","modified":1433597960000},{"_id":"themes/light/layout/_partial/post/tag.ejs","shasum":"095418df66a27a28cbab16d7cb0d16001b0e23f1","modified":1433597960000},{"_id":"themes/light/layout/_partial/post/share.ejs","shasum":"24c04b319f1b19e887c42db961b90a7e0ab26fdc","modified":1433597960000},{"_id":"themes/light/layout/_partial/head.ejs","shasum":"d892420cbd253bf8707da7affb8f489cebaae6fd","modified":1433941148000},{"_id":"themes/light/layout/_partial/post/title.ejs","shasum":"d7fbc575d35ae68f9045a382c651450e4131f335","modified":1433597960000},{"_id":"themes/light/layout/_partial/post/gallery.ejs","shasum":"fafc2501d7e65983b0f5c2b58151ca12e57c0574","modified":1433597960000},{"_id":"themes/light/layout/_partial/sidebar.ejs","shasum":"caf351797a18d03d8ee945ceb9f83785c50c09f9","modified":1433597960000},{"_id":"themes/light/layout/_widget/category.ejs","shasum":"8a2b90dc29661371f060f710668929c3588e15e4","modified":1433597960000},{"_id":"themes/light/layout/_widget/photo.ejs","shasum":"a985f891ccad6a54352c38de34e90e14005c3e04","modified":1442554487000},{"_id":"themes/light/layout/_widget/search.ejs","shasum":"55c707f3aa7453c305c41898ad22556edd213830","modified":1433597960000},{"_id":"themes/light/layout/_widget/recent_posts.ejs","shasum":"8f2f3963bd568c681d7585bb8099fb1b3e1d4c81","modified":1442555459000},{"_id":"themes/light/layout/_widget/link.ejs","shasum":"84c4ff9ad2c098b21e09cba41e73c67a65d839f5","modified":1449515710000},{"_id":"themes/light/layout/_widget/tagcloud.ejs","shasum":"673b598903e1b2b9d9ea31dc79bab65ef3984348","modified":1433929542000},{"_id":"themes/light/layout/_widget/tag.ejs","shasum":"1914db78bea49c333067d79fe7ad9567d2b08d00","modified":1433597960000},{"_id":"themes/light/layout/archive.ejs","shasum":"a18842e3d719fe3ca9b977a6995f8facc75c8673","modified":1433597960000},{"_id":"themes/light/layout/category.ejs","shasum":"9b740fc33f6f028df60f0bc4312bf3ebd03aa8ea","modified":1433597960000},{"_id":"themes/light/layout/layout.ejs","shasum":"72da76881ebf00e71d7cc196f377e37a17ec7a6f","modified":1433597960000},{"_id":"themes/light/layout/index.ejs","shasum":"e569d8fe0741a24efb89e44781f9e616da17e036","modified":1433597960000},{"_id":"themes/light/layout/page.ejs","shasum":"70cbc9854655773cc6ba84eecaaf330fed430465","modified":1433597960000},{"_id":"themes/light/layout/tag.ejs","shasum":"45150a2365768b6b67880193c9264ad2bb4814db","modified":1433597960000},{"_id":"themes/light/layout/post.ejs","shasum":"70cbc9854655773cc6ba84eecaaf330fed430465","modified":1433597960000},{"_id":"themes/light/source/css/_partial/archive.styl","shasum":"072e9b8c5ee9acf95ac7cce9c34706d41e412229","modified":1433597960000},{"_id":"themes/light/source/css/_base/variable.styl","shasum":"c4e07cb7f4ec980553cd19d9d2cd8a1a44d4cd82","modified":1434027874000},{"_id":"themes/light/source/css/_base/layout.styl","shasum":"1b58c21aa48a8f9f7f811af681ac182dd058e23d","modified":1433597960000},{"_id":"themes/light/source/css/_partial/article.styl","shasum":"09e00a528f2524093660cf96647282aa2b98abbf","modified":1434030084000},{"_id":"themes/light/source/css/_partial/footer.styl","shasum":"1757872dbdbd09295a625f13e356aa798a8bb308","modified":1433597960000},{"_id":"themes/light/source/css/_partial/index.styl","shasum":"7a8c0ec6ab99a9f8e00c9687aca29d31752424a2","modified":1433597960000},{"_id":"themes/light/source/css/_partial/header.styl","shasum":"0f932c9514d13fea70fe109242e17ee633a2b28a","modified":1434021576000},{"_id":"themes/light/source/css/_partial/syntax.styl","shasum":"031903fe83aa7d9960bd084f4bb77a6671abcbfa","modified":1443169038000},{"_id":"themes/light/source/css/_partial/comment.styl","shasum":"a74254a6e713a522134cca4c644fde681c502823","modified":1434028066000},{"_id":"themes/light/source/css/font/fontawesome-webfont.eot","shasum":"d775f599ff3f23be082e6a9604b4898718923a37","modified":1433597960000},{"_id":"themes/light/source/css/_partial/sidebar.styl","shasum":"1bdd787d9dc40829dcab26e0e4543c0d2325b5b8","modified":1434022784000},{"_id":"themes/light/source/css/style.styl","shasum":"c03b2520e4a85b981e29516cadc0a365e6500e3d","modified":1433597960000},{"_id":"themes/light/source/css/font/fontawesome-webfont.woff","shasum":"0612cddf2f835cceffccc88fd194f97367d0b024","modified":1433597960000},{"_id":"themes/light/source/fancybox/blank.gif","shasum":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1433597960000},{"_id":"themes/light/source/fancybox/fancybox_overlay.png","shasum":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1433597960000},{"_id":"themes/light/source/fancybox/fancybox_sprite@2x.png","shasum":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1433597960000},{"_id":"themes/light/source/fancybox/fancybox_loading.gif","shasum":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1433597960000},{"_id":"themes/light/source/fancybox/jquery.fancybox.css","shasum":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1433597960000},{"_id":"themes/light/source/fancybox/fancybox_sprite.png","shasum":"17df19f97628e77be09c352bf27425faea248251","modified":1433597960000},{"_id":"themes/light/source/fancybox/fancybox_loading@2x.gif","shasum":"273b123496a42ba45c3416adb027cd99745058b0","modified":1433597960000},{"_id":"themes/light/source/js/jquery.imagesloaded.min.js","shasum":"4109837b1f6477bacc6b095a863b1b95b1b3693f","modified":1433597960000},{"_id":"themes/light/source/fancybox/jquery.fancybox.pack.js","shasum":"53360764b429c212f424399384417ccc233bb3be","modified":1433597960000},{"_id":"themes/light/source/js/gallery.js","shasum":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed","modified":1433597960000},{"_id":"themes/light/source/css/font/fontawesome-webfont.ttf","shasum":"a9468f6a1fe965fbcaf5a1bd6c11705e2fc5f84c","modified":1433597960000},{"_id":"themes/light/source/css/font/fontawesome-webfont.svg","shasum":"d162419c91b8bab3a4fd327c933a0fcf3799c251","modified":1433597960000},{"_id":"themes/light/source/kiwenlau.ico","shasum":"987d9c1658c3779037d96f49d8779b9bab18440b","modified":1433759880000},{"_id":"themes/light/source/kiwenlau.jpg","shasum":"e163949f9617ee5a0b92ef131f6e2bb57a2c0e36","modified":1433690104000},{"_id":"public/CNAME","modified":1452128813930,"shasum":"abcdc15ac03b8fdb087e2ae5c120ef20dd63dcd1"},{"_id":"public/image/160109/kubernetes-shell-supervisor.png","modified":1452128813934,"shasum":"1e8f251362d7875128ce8cfee1af5bccf2d7a693"},{"_id":"public/image/151128/single-kubernetes-docker.png","modified":1452128813936,"shasum":"75ee08265d42589d56dd36ac764d40521af2fe8c"},{"_id":"public/image/150918/kiwenlau_single_mesos.graffle","modified":1452128813937,"shasum":"3fcf8e0540f83077eac1114e5b431e64886f19ca"},{"_id":"public/image/150918/hello.png","modified":1452128813939,"shasum":"732ba80324b66a3ff47fafbde89f4c61dfb04a08"},{"_id":"public/image/150918/architecuture.png","modified":1452128813943,"shasum":"43b51d2b17bb54c7c11530d40f1068a558ebde10"},{"_id":"public/image/150918/Mesos.png","modified":1452128813946,"shasum":"c21fb25a6da8801ecb9b6b25888cb88da16790b4"},{"_id":"public/image/150918/Marathon.png","modified":1452128813949,"shasum":"58abd8a1ba30eedef47649093a37291e29f21e00"},{"_id":"public/image/150608/image architecture.jpg","modified":1452128813952,"shasum":"450f2dffe1127f102221a9b3c934eb6054876d85"},{"_id":"public/kiwenlau.jpg","modified":1452128813956,"shasum":"e163949f9617ee5a0b92ef131f6e2bb57a2c0e36"},{"_id":"public/kiwenlau.ico","modified":1452128813960,"shasum":"987d9c1658c3779037d96f49d8779b9bab18440b"},{"_id":"public/js/jquery.imagesloaded.min.js","modified":1452128813964,"shasum":"4109837b1f6477bacc6b095a863b1b95b1b3693f"},{"_id":"public/js/gallery.js","modified":1452128813970,"shasum":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed"},{"_id":"public/fancybox/jquery.fancybox.pack.js","modified":1452128813971,"shasum":"53360764b429c212f424399384417ccc233bb3be"},{"_id":"public/fancybox/jquery.fancybox.css","modified":1452128813977,"shasum":"5f163444617b6cf267342f06ac166a237bb62df9"},{"_id":"public/fancybox/fancybox_sprite@2x.png","modified":1452128813979,"shasum":"30c58913f327e28f466a00f4c1ac8001b560aed8"},{"_id":"public/fancybox/fancybox_sprite.png","modified":1452128813980,"shasum":"17df19f97628e77be09c352bf27425faea248251"},{"_id":"public/fancybox/fancybox_overlay.png","modified":1452128813982,"shasum":"b3a4ee645ba494f52840ef8412015ba0f465dbe0"},{"_id":"public/fancybox/fancybox_loading@2x.gif","modified":1452128813984,"shasum":"273b123496a42ba45c3416adb027cd99745058b0"},{"_id":"public/fancybox/fancybox_loading.gif","modified":1452128813986,"shasum":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c"},{"_id":"public/fancybox/blank.gif","modified":1452128813987,"shasum":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a"},{"_id":"public/css/style.css","modified":1452128814416,"shasum":"df9188017cc02f2f3eab54caf4ca3f185f756efd"},{"_id":"public/css/font/fontawesome-webfont.woff","modified":1452128814506,"shasum":"0612cddf2f835cceffccc88fd194f97367d0b024"},{"_id":"public/css/font/fontawesome-webfont.ttf","modified":1452128814508,"shasum":"a9468f6a1fe965fbcaf5a1bd6c11705e2fc5f84c"},{"_id":"public/css/font/fontawesome-webfont.svg","modified":1452128814511,"shasum":"d162419c91b8bab3a4fd327c933a0fcf3799c251"},{"_id":"public/css/font/fontawesome-webfont.eot","modified":1452128814513,"shasum":"d775f599ff3f23be082e6a9604b4898718923a37"},{"_id":"public/google6c2fe14874348911.html","modified":1452128814515,"shasum":"12e5058baee67bcef931133e10cf113179e7d1eb"},{"_id":"public/2015/11/28/160109-multiple-processes--docker-container/index.html","modified":1452129147252,"shasum":"e68c652125dd460d60b4d4e85daff1d99804303a"},{"_id":"public/2015/11/28/151128-single-kubernetes-docker/index.html","modified":1452128814571,"shasum":"f2c9734ec0038f0285a90b663cf3fc609efa123b"},{"_id":"public/2015/09/18/150918-single-mesos-docker/index.html","modified":1452128814582,"shasum":"f4522b2eaa39fca52f59400418a221b937c2c6ca"},{"_id":"public/2015/06/08/150608-hadoop-cluster-docker/index.html","modified":1452128814609,"shasum":"0e18e795e75001da80f16da5cde769606c7cfa42"},{"_id":"public/archives/index.html","modified":1452128814619,"shasum":"c8fe6642d793f1808e022a104381f2c842249a69"},{"_id":"public/archives/2015/index.html","modified":1452128814629,"shasum":"e8ed452960925305f282aec3d9e38b46d14c2f77"},{"_id":"public/archives/2015/06/index.html","modified":1452128814641,"shasum":"c164ea3df0a09b8cdfc96c87a5ec761757e9af02"},{"_id":"public/archives/2015/09/index.html","modified":1452128814653,"shasum":"51b435c5aaae4d3bebc6b5e53d7e34b7e5f6d9a9"},{"_id":"public/archives/2015/11/index.html","modified":1452128814663,"shasum":"08734c49586edb2749567800a0cf39c97e704379"},{"_id":"public/index.html","modified":1452129147329,"shasum":"41e60e61281b3e40ae043ec19e812b9e6792f063"},{"_id":"public/tags/Docker/index.html","modified":1452128814689,"shasum":"ea9193186e4f8d2077446a5f359669ec74ff024c"},{"_id":"public/tags/Kubernetes/index.html","modified":1452128814704,"shasum":"4a02bb9b4589fd1fcbb99d8d58f61d5d87aeb5ab"},{"_id":"public/tags/Mesos/index.html","modified":1452128814711,"shasum":"033b63cb53229cbe1c45cd06dd538250f3509ea5"},{"_id":"public/tags/Marathon/index.html","modified":1452128814726,"shasum":"6651dbefca496e3119c1d7e0ab8b4f58c098806a"},{"_id":"public/tags/Hadoop/index.html","modified":1452128814743,"shasum":"b9a112c68dfd0182c918ce532a07b767f9539b24"},{"_id":"public/sitemap.xml","modified":1452129147352,"shasum":"560d2f093830c8831eb84f8a9d39e4a8857a0b00"},{"_id":"public/atom.xml","modified":1452129147351,"shasum":"9659cdf5a0c6886c19443b7535e93b25516383ef"}],"Category":[],"Data":[],"Page":[{"layout":"false","_content":"google-site-verification: google6c2fe14874348911.html","source":"google6c2fe14874348911.html","raw":"layout: false\n---\ngoogle-site-verification: google6c2fe14874348911.html","date":"2015-12-09T08:20:30.000Z","updated":"2015-06-10T11:06:26.000Z","path":"google6c2fe14874348911.html","title":"","comments":1,"_id":"cij3k03km0000848sxh1em7hf"}],"Post":[{"title":"如何运行多进程Docker容器？","date":"2015-11-28T00:00:00.000Z","_content":"\n**GitHub地址:**\n\n- [kiwenlau/kubernetes-shell](https://github.com/kiwenlau/kubernetes-shell)\n- [kiwenlau/kubernetes-supervisor](https://github.com/kiwenlau/kubernetes-supervisor)\n\n![](/image/160109/kubernetes-shell-supervisor.png)\n\n\n##参考\n1. [Using Supervisor with Docker](https://docs.docker.com/engine/articles/using_supervisord/)\n2. [How To Install and Manage Supervisor on Ubuntu and Debian VPS](https://www.digitalocean.com/community/tutorials/how-to-install-and-manage-supervisor-on-ubuntu-and-debian-vps)\n3. [Supervisor: A Process Control System](http://supervisord.org/)\n4. [基于Docker搭建单机版Kuberntes](http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/)\n5. [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2016/01/09/160109-multiple-processes--docker-container/](http://kiwenlau.com/2016/01/09/160109-multiple-processes--docker-container/)\n***\n\n\n\n\n\n\n\n\n","source":"_posts/160109-multiple-processes--docker-container.md","raw":"title: 如何运行多进程Docker容器？\n\ndate: 2015-11-28 09:00:00\n\ntags: [Docker, Kubernetes]\n\n---\n\n**GitHub地址:**\n\n- [kiwenlau/kubernetes-shell](https://github.com/kiwenlau/kubernetes-shell)\n- [kiwenlau/kubernetes-supervisor](https://github.com/kiwenlau/kubernetes-supervisor)\n\n![](/image/160109/kubernetes-shell-supervisor.png)\n\n\n##参考\n1. [Using Supervisor with Docker](https://docs.docker.com/engine/articles/using_supervisord/)\n2. [How To Install and Manage Supervisor on Ubuntu and Debian VPS](https://www.digitalocean.com/community/tutorials/how-to-install-and-manage-supervisor-on-ubuntu-and-debian-vps)\n3. [Supervisor: A Process Control System](http://supervisord.org/)\n4. [基于Docker搭建单机版Kuberntes](http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/)\n5. [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2016/01/09/160109-multiple-processes--docker-container/](http://kiwenlau.com/2016/01/09/160109-multiple-processes--docker-container/)\n***\n\n\n\n\n\n\n\n\n","slug":"160109-multiple-processes--docker-container","published":1,"updated":"2016-01-07T01:12:08.000Z","_id":"cij3k03kq0001848sjsr6jhiy","comments":1,"layout":"post","photos":[],"link":""},{"title":"基于Docker搭建单机版Kuberntes","date":"2015-11-28T00:00:00.000Z","_content":"\n**GitHub地址:** [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n\n##1. Kubernetes简介\n\n2006年，Google工程师Rohit Seth发起了Cgroups内核项目。Cgroups是容器实现CPU，内存等资源隔离的基础，由此可见Google其实很早就开始涉足容器技术。而事实上，Google内部使用容器技术已经长达十年，目前谷歌所有业务包括搜索，Gmail，MapReduce等均运行在容器之中。Google内部使用的集群管理系统--Borg，堪称其容器技术的瑞士军刀。\n\n2014年，Google发起了开源容器集群管理系统--Kubernetes，其设计之初就吸取了Borg的经验和教训，并原生支持了Docker。因此，Kubernetees与较早的集群管理系统Mesos和YARN相比，对容器技术尤其是Docker的支持更加原生，同时提供了更强大的机制实现资源调度，负载均衡，高可用等底层功能，使开发者可以专注于开发应用。\n\n与其他集群系统一致，Kubernetes也采用了Master/Slave结构。下表显示了Kubernetes的各个组件及其功能。\n\n| 角色     | 组件               | 功能                                           |\n| ------- |:-----------------: | :--------------------------------------------:|\n| Master  | apiserver          | 提供RESTful接口                                |\n| Master  | scheduler          | 负责调度，将pod分配到Slave节点                   |\n| Master  | controller-manager | 负责Master的其他功能                           |\n| Master  | etde               | 储存配置信息，节点信息，pod信息等                 |\n| Slave   | kubelet            | 负责管理Pod、容器和容器镜像                       |\n| Slave   | proxy              | 将访问Service的请求转发给对应的Pod，做一些负载均衡  |\n| 客户端   | kubectl            | 命令行工具，向apiserver发起创建Pod等请求          |\n\n\n##2. kiwenlau/kubernetes镜像简介\n\n下图显示了我在Ubuntu主机上运行单机版Kubernetes的架构。可知，我一共运行了7个容器，分别运行Kubernetes的各个组件。事实上，Kuberenetes未来的开发目标正是将Kubernetes的各个组件运行到容器之中，这样可以方便Kubernetes的部署和升级。现在我将Kubernetes的各个组件全部运行在容器中必然存在很多问题且很多问题是未知的，因此这个项目仅做学习测试而不宜部署到生产环境中。Kubernetes各个组件容器之间的通信通过docker link实现，其中apiserver与ectd的4001端口进行通信，scheduler，controller-manager，kubelet，proxy以及kubectl与apiserver的8080端口进行通信。\n\n![](/image/151128/single-kubernetes-docker.png)\n\n集群的大致运行流程是这样的: 用户通过kubectl命令向apiserver发起创建Pod的请求; scheduler将创建Pod的任务分配给kubelet；kubelet中包含了一个docker命令行工具，该工具会向Docker deamon发起创建容器的请求; Docker deamon负责下载镜像然后创建容器。\n\n我将Docker deamon运行在Ubuntu主机上，因此Docker daemon所创建的应用容器与Kubernetes各个组件的容器均运行在Ubuntu主机上。docker socket采用volume的形式挂载到kubelet容器内，因此kubelet中的docker命令行工具可以直接与主机上的Docker daemon进行通信。\n\n我是直接将kubernetes发布的各个组件的二进制可执行文件安装在/usr/local/bin目录下，因此，修改Dockerfile中的Kubernetes下载链接的版本号，就可以快速构建其他版本的Kubernetes镜像。另外，仅需修改网络配置，就可以很方便地在多个节点上部署Kubernetes。\n\nkiwenlau/kubernetes:1.0.7镜像版本信息:\n\n- ubuntu: 14.04\n- Kubernetes: 1.0.7\n- ectd: 2.2.1\n\nUbuntu主机版本信息:\n\n- ubuntu: 14.04.3 LTS\n- kernel: 3.16.0-30-generic\n- docker: 1.9.1\n\n\n\n##3. 运行步骤\n\n**1. 安装Docker**\n\nubuntu 14.04上安装Docker: \n\n```\ncurl -fLsS https://get.docker.com/ | sh\n```\n\n其他系统请参考: [https://docs.docker.com/](https://docs.docker.com/)\n\n**2. 下载Docker镜像**\n\n我将kiwenlau/kubernetes:1.07以及其他用到的Docker镜像都放在[灵雀云](http://www.alauda.cn/)\n\n```\nsudo docker pull index.alauda.cn/kiwenlau/kubernetes:1.0.7\nsudo docker pull index.alauda.cn/kiwenlau/etcd:v2.2.1\nsudo docker pull index.alauda.cn/kiwenlau/nginx:1.9.7\nsudo docker pull index.alauda.cn/kiwenlau/pause:0.8.0\n```\n\n**3. 启动Kubernetes**\n\n```sh\ngit clone https://github.com/kiwenlau/single-kubernetes-docker\ncd single-kubernetes-docker/\nsudo chmod +x start-kubernetes-alauda.sh stop-kubernetes.sh\nsudo ./start-kubernetes-alauda.sh\n```\n\n运行结束后进入kubectl容器。容器主机名为kubeclt。可以通过\"exit\"命令退出容器返回到主机，然后可以通过\"sudo docker exec -it kubectl bash\"命令再次进入kubectl容器。\n\n\n**4. 测试Kubernetes**\n\n运行测试脚本，该脚本会启动一个nginx pod。\n\n```\nchmod +x test-kubernetes-alauda.sh\n./test-kubernetes-alauda.sh \n```\n\n输出\n\n```\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n<style>\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n</style>\n</head>\n<body>\n<h1>Welcome to nginx!</h1>\n<p>If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.</p>\n\n<p>For online documentation and support please refer to\n<a href=\"http://nginx.org/\">nginx.org</a>.<br/>\nCommercial support is available at\n<a href=\"http://nginx.com/\">nginx.com</a>.</p>\n\n<p><em>Thank you for using nginx.</em></p>\n</body>\n</html>\n```\n\n\n##3. 参考\n1. [meteorhacks/hyperkube](https://github.com/meteorhacks/hyperkube)\n2. [meteorhacks/kube-init](https://github.com/meteorhacks/kube-init)\n3. [Kubernetes: The Future of Cloud Hosting](https://meteorhacks.com/learn-kubernetes-the-future-of-the-cloud)\n4. [Kubernetes 架构浅析](http://weibo.com/p/1001603912843031387951?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io)\n5. [An Introduction to Kubernetes](https://www.digitalocean.com/community/tutorials/an-introduction-to-kubernetes)\n\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/](http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/)\n***\n\n\n\n\n\n\n\n\n","source":"_posts/151128-single-kubernetes-docker.md","raw":"title: 基于Docker搭建单机版Kuberntes\n\ndate: 2015-11-28 09:00:00\n\ntags: [Docker, Kubernetes]\n\n---\n\n**GitHub地址:** [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n\n##1. Kubernetes简介\n\n2006年，Google工程师Rohit Seth发起了Cgroups内核项目。Cgroups是容器实现CPU，内存等资源隔离的基础，由此可见Google其实很早就开始涉足容器技术。而事实上，Google内部使用容器技术已经长达十年，目前谷歌所有业务包括搜索，Gmail，MapReduce等均运行在容器之中。Google内部使用的集群管理系统--Borg，堪称其容器技术的瑞士军刀。\n\n2014年，Google发起了开源容器集群管理系统--Kubernetes，其设计之初就吸取了Borg的经验和教训，并原生支持了Docker。因此，Kubernetees与较早的集群管理系统Mesos和YARN相比，对容器技术尤其是Docker的支持更加原生，同时提供了更强大的机制实现资源调度，负载均衡，高可用等底层功能，使开发者可以专注于开发应用。\n\n与其他集群系统一致，Kubernetes也采用了Master/Slave结构。下表显示了Kubernetes的各个组件及其功能。\n\n| 角色     | 组件               | 功能                                           |\n| ------- |:-----------------: | :--------------------------------------------:|\n| Master  | apiserver          | 提供RESTful接口                                |\n| Master  | scheduler          | 负责调度，将pod分配到Slave节点                   |\n| Master  | controller-manager | 负责Master的其他功能                           |\n| Master  | etde               | 储存配置信息，节点信息，pod信息等                 |\n| Slave   | kubelet            | 负责管理Pod、容器和容器镜像                       |\n| Slave   | proxy              | 将访问Service的请求转发给对应的Pod，做一些负载均衡  |\n| 客户端   | kubectl            | 命令行工具，向apiserver发起创建Pod等请求          |\n\n\n##2. kiwenlau/kubernetes镜像简介\n\n下图显示了我在Ubuntu主机上运行单机版Kubernetes的架构。可知，我一共运行了7个容器，分别运行Kubernetes的各个组件。事实上，Kuberenetes未来的开发目标正是将Kubernetes的各个组件运行到容器之中，这样可以方便Kubernetes的部署和升级。现在我将Kubernetes的各个组件全部运行在容器中必然存在很多问题且很多问题是未知的，因此这个项目仅做学习测试而不宜部署到生产环境中。Kubernetes各个组件容器之间的通信通过docker link实现，其中apiserver与ectd的4001端口进行通信，scheduler，controller-manager，kubelet，proxy以及kubectl与apiserver的8080端口进行通信。\n\n![](/image/151128/single-kubernetes-docker.png)\n\n集群的大致运行流程是这样的: 用户通过kubectl命令向apiserver发起创建Pod的请求; scheduler将创建Pod的任务分配给kubelet；kubelet中包含了一个docker命令行工具，该工具会向Docker deamon发起创建容器的请求; Docker deamon负责下载镜像然后创建容器。\n\n我将Docker deamon运行在Ubuntu主机上，因此Docker daemon所创建的应用容器与Kubernetes各个组件的容器均运行在Ubuntu主机上。docker socket采用volume的形式挂载到kubelet容器内，因此kubelet中的docker命令行工具可以直接与主机上的Docker daemon进行通信。\n\n我是直接将kubernetes发布的各个组件的二进制可执行文件安装在/usr/local/bin目录下，因此，修改Dockerfile中的Kubernetes下载链接的版本号，就可以快速构建其他版本的Kubernetes镜像。另外，仅需修改网络配置，就可以很方便地在多个节点上部署Kubernetes。\n\nkiwenlau/kubernetes:1.0.7镜像版本信息:\n\n- ubuntu: 14.04\n- Kubernetes: 1.0.7\n- ectd: 2.2.1\n\nUbuntu主机版本信息:\n\n- ubuntu: 14.04.3 LTS\n- kernel: 3.16.0-30-generic\n- docker: 1.9.1\n\n\n\n##3. 运行步骤\n\n**1. 安装Docker**\n\nubuntu 14.04上安装Docker: \n\n```\ncurl -fLsS https://get.docker.com/ | sh\n```\n\n其他系统请参考: [https://docs.docker.com/](https://docs.docker.com/)\n\n**2. 下载Docker镜像**\n\n我将kiwenlau/kubernetes:1.07以及其他用到的Docker镜像都放在[灵雀云](http://www.alauda.cn/)\n\n```\nsudo docker pull index.alauda.cn/kiwenlau/kubernetes:1.0.7\nsudo docker pull index.alauda.cn/kiwenlau/etcd:v2.2.1\nsudo docker pull index.alauda.cn/kiwenlau/nginx:1.9.7\nsudo docker pull index.alauda.cn/kiwenlau/pause:0.8.0\n```\n\n**3. 启动Kubernetes**\n\n```sh\ngit clone https://github.com/kiwenlau/single-kubernetes-docker\ncd single-kubernetes-docker/\nsudo chmod +x start-kubernetes-alauda.sh stop-kubernetes.sh\nsudo ./start-kubernetes-alauda.sh\n```\n\n运行结束后进入kubectl容器。容器主机名为kubeclt。可以通过\"exit\"命令退出容器返回到主机，然后可以通过\"sudo docker exec -it kubectl bash\"命令再次进入kubectl容器。\n\n\n**4. 测试Kubernetes**\n\n运行测试脚本，该脚本会启动一个nginx pod。\n\n```\nchmod +x test-kubernetes-alauda.sh\n./test-kubernetes-alauda.sh \n```\n\n输出\n\n```\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n<style>\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n</style>\n</head>\n<body>\n<h1>Welcome to nginx!</h1>\n<p>If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.</p>\n\n<p>For online documentation and support please refer to\n<a href=\"http://nginx.org/\">nginx.org</a>.<br/>\nCommercial support is available at\n<a href=\"http://nginx.com/\">nginx.com</a>.</p>\n\n<p><em>Thank you for using nginx.</em></p>\n</body>\n</html>\n```\n\n\n##3. 参考\n1. [meteorhacks/hyperkube](https://github.com/meteorhacks/hyperkube)\n2. [meteorhacks/kube-init](https://github.com/meteorhacks/kube-init)\n3. [Kubernetes: The Future of Cloud Hosting](https://meteorhacks.com/learn-kubernetes-the-future-of-the-cloud)\n4. [Kubernetes 架构浅析](http://weibo.com/p/1001603912843031387951?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io)\n5. [An Introduction to Kubernetes](https://www.digitalocean.com/community/tutorials/an-introduction-to-kubernetes)\n\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/](http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/)\n***\n\n\n\n\n\n\n\n\n","slug":"151128-single-kubernetes-docker","published":1,"updated":"2015-12-07T19:07:20.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cij3k03l00006848sijsqn889"},{"title":"基于Docker搭建单机版Mesos/Marathon","date":"2015-09-18T03:00:00.000Z","_content":"\n- GitHub地址：[kiwenlau/single-mesos-docker](https://github.com/kiwenlau/single-mesos-docker)\n- 博客地址：[基于Docker快速搭建单节点Mesos/Marathon集群](http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/)\n\n##一. 简介\n\n[Mesos](http://mesos.apache.org)是集群资源管理系统，[Marathon](http://mesosphere.github.io/marathon)是运行在Mesos之上的集群计算架构。将Mesos和Marathon打包到[Docker](https://www.docker.com/)镜像中，开发者便可以在本机上快速搭建Mesos/Marathon集群，进行学习和测试。\n\n**kiwenlau/single-mesos**镜像非常简单。Docker容器运行在Ubuntu主机之上，Mesos和Marathon运行在该容器之中。具体来讲，Docker容器中运行了一个Mesos Master和一个Mesos Slave，以及Marathon和[ZooKeeper](https://zookeeper.apache.org/)。集群架构如下图：\n\n![](/image/150918/architecuture.png)\n\n\n##二. 搭建Mesos/Marathon集群\n\n**1. 下载Docker镜像:**\n\n```sh\nsudo docker pull kiwenlau/single-mesos:3.0\n```\n\n**2. 运行Docker容器:**\n\n```sh\nsudo docker run -p 5050:5050 -p 8080:8080 --name mesos -it -w /root kiwenlau/single-mesos:3.0\n```\n\ndocker run命令运行成功后即进入容器内部，以下为输出：\n\n```bash\nStart ZooKeeper...\nStart Mesos master...\nStart Mesos slave...\nStart Marathon...\n```\n\n\n##三. 测试Mesos/Marathon集群\n\n**1. 通过curl命令调用Marathon的REST API, 创建一个hello程序：**\n\n```sh\ncurl -v -H \"Content-Type: application/json\" -X POST --data \"@hello.json\" http://127.0.0.1:8080/v2/apps\n```\n\n下面为hello.json。由cmd可知，该程序每隔1秒往output.txt文件中写入hello。\n\n```bash\n{\n  \"id\": \"hello\",\n  \"cmd\": \"while [ true ] ; do echo hello >> /root/output.txt; sleep 1; done\",\n  \"cpus\": 0.1,\n  \"mem\": 10.0,\n  \"instances\": 1\n}\n```\n\ncurl执行结果:\n\n```bash\n* Hostname was NOT found in DNS cache\n*   Trying 127.0.0.1...\n* Connected to 127.0.0.1 (127.0.0.1) port 8080 (#0)\n> POST /v2/apps HTTP/1.1\n> User-Agent: curl/7.35.0\n> Host: 127.0.0.1:8080\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 139\n> \n* upload completely sent off: 139 out of 139 bytes\n< HTTP/1.1 201 Created\n< X-Marathon-Leader: http://ec054cabb9af:8080\n< Cache-Control: no-cache, no-store, must-revalidate\n< Pragma: no-cache\n< Expires: 0\n< Location: http://127.0.0.1:8080/v2/apps/hello\n< Content-Type: application/json; qs=2\n< Transfer-Encoding: chunked\n* Server Jetty(8.y.z-SNAPSHOT) is not blacklisted\n< Server: Jetty(8.y.z-SNAPSHOT)\n< \n* Connection #0 to host 127.0.0.1 left intact\n{\"id\":\"/hello\",\"cmd\":\"while [ true ] ; do echo hello >> /root/output.txt; sleep 1; done\",\"args\":null,\"user\":null,\"env\":{},\"instances\":1,\"cpus\":0.1,\"mem\":10.0,\"disk\":0.0,\"executor\":\"\",\"constraints\":[],\"uris\":[],\"storeUrls\":[],\"ports\":[0],\"requirePorts\":false,\"backoffFactor\":1.15,\"container\":null,\"healthChecks\":[],\"dependencies\":[],\"upgradeStrategy\":{\"minimumHealthCapacity\":1.0,\"maximumOverCapacity\":1.0},\"labels\":{},\"acceptedResourceRoles\":null,\"version\":\"2015-09-16T11:22:27.967Z\",\"deployments\":[{\"id\":\"2cd2fdd4-e5f9-4088-895f-7976349b7a19\"}],\"tasks\":[],\"tasksStaged\":0,\"tasksRunning\":0,\"tasksHealthy\":0,\"tasksUnhealthy\":0,\"backoffSeconds\":1,\"maxLaunchDelaySeconds\":3600}\n```\n\n**2. 查看hello程序的运行结果：**\n\n```sh\ntail -f output.txt\n```\n当你看到终端不断输出\"hello\"时说明运行成功。\n\n**3. 使用浏览器查看Mesos和Marathon的网页管理界面**\n\n**注意**将IP替换运行Docker容器的主机IP地址\n\nMesos网页管理界面地址：[http://192.168.59.10:5050](http://192.168.59.10:5050)\n\nMesos网页管理界面如图，可知hello程序正在运行：\n\n![](/image/150918/Mesos.png)\n\nMarathon网页管理界面地址：[http://192.168.59.10:8080](http://192.168.59.10:8080)\n\nMarathon网页管理界面如图，可知hello程序正在运行：\n\n![](/image/150918/Marathon.png)\n\n**4. 通过Marathon网页管理界面创建测试程序**\n\n在Marathon的网页管理界面上点击\"New APP\"，在弹框中配置测试程序。ID为\"hello\", Command为\"echo hello >> /root/output.txt\", 然后点击\"Create\"即可。如下图：\n\n![](/image/150918/hello.png)\n\n\n##四. 存在的问题\n\n其实，参考[Setting up a Single Node Mesosphere Cluster](https://open.mesosphere.com/getting-started/developer/single-node-install/)，可以很快地在ubuntu主机上直接搭建一个单节点的Mesos/Marathon集群。但是，当我安装该教程的步骤将Mesos/Marathon集群打包到Docker镜像中时，遇到了一个比较奇怪的问题。\n\n在Docker容器中使用**\"sudo service mesos-master start\"**和**\"sudo service mesos-slave start\"**命令启动Mesos Master和Mesos Slave时，出现**\"mesos-master: unrecognized service\"**和**\"mesos-slave: unrecognized service\"**错误。但是，我在ubuntu主机上安装Mesos/Marathon集群后，使用同样的命令启动Mesos并没有问题。后来，我是通过直接执行mesos-master和mesos-slave命令启动Mesos，命令如下：\n\n```sh\n/usr/sbin/mesos-master --zk=zk://127.0.0.1:2181/mesos --quorum=1 --work_dir=/var/lib/mesos --log_dir=/log/mesos  \n```\n\n```sh\n/usr/sbin/mesos-slave --master=zk://127.0.0.1:2181/mesos --log_dir=/log/mesos\n```\n\n由这个问题可知，虽然在Docker容器几乎可以运行任意程序，似乎和Ubuntu主机没有区别。但是事实上，**Docker容器与ubuntu主机并非完全一致**，而且这些细节的不同点比较坑。这一点很值得探讨，可以让大家在使用Docker时少走些弯路。对于提到的问题，虽然是解决了，然而我仍然不清楚其中的原因:(\n\n\n##五. Docker镜像备份\n\n我将Docker镜像上传到了灵雀云（Alaudo）的Docker仓库，可以通过以下命令下载和运行：\n\n```sh\nsudo docker pull index.alauda.cn/kiwenlau/single-mesos:3.0\n```\n\n```sh\nsudo docker run -p 5050:5050 -p 8080:8080 --name mesos -it -w /root index.alauda.cn/kiwenlau/single-mesos:3.0\n```\n\n##六. 参考\n\n1. [Setting up a Single Node Mesosphere Cluster](https://open.mesosphere.com/getting-started/developer/single-node-install/)\n2. [Setting up a Cluster on Mesos and Marathon](https://open.mesosphere.com/getting-started/datacenter/install/#master-setup)\n3. [An Introduction to Mesosphere](https://www.digitalocean.com/community/tutorials/an-introduction-to-mesosphere)\n4. [How To Configure a Production-Ready Mesosphere Cluster on Ubuntu 14.04](https://www.digitalocean.com/community/tutorials/how-to-configure-a-production-ready-mesosphere-cluster-on-ubuntu-14-04)\n5. [Deploy a Mesos Cluster with 7 Commands Using Docker](https://medium.com/@gargar454/deploy-a-mesos-cluster-with-7-commands-using-docker-57951e020586)\n6. [sekka1/mesosphere-docker](https://github.com/sekka1/mesosphere-docker)\n7. [Marathon: Application Basics](http://mesosphere.github.io/marathon/docs/application-basics.html)\n8. [Marathon: REST API](http://mesosphere.github.io/marathon/docs/rest-api.html)\n\n***\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文URL地址：\n\n[http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/](http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/)\n***\n\n\n\n\n\n","source":"_posts/150918-single-mesos-docker.md","raw":"title: 基于Docker搭建单机版Mesos/Marathon\n\ndate: 2015-09-18 12:00:00\n\ntags: [Docker,Mesos,Marathon]\n\n---\n\n- GitHub地址：[kiwenlau/single-mesos-docker](https://github.com/kiwenlau/single-mesos-docker)\n- 博客地址：[基于Docker快速搭建单节点Mesos/Marathon集群](http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/)\n\n##一. 简介\n\n[Mesos](http://mesos.apache.org)是集群资源管理系统，[Marathon](http://mesosphere.github.io/marathon)是运行在Mesos之上的集群计算架构。将Mesos和Marathon打包到[Docker](https://www.docker.com/)镜像中，开发者便可以在本机上快速搭建Mesos/Marathon集群，进行学习和测试。\n\n**kiwenlau/single-mesos**镜像非常简单。Docker容器运行在Ubuntu主机之上，Mesos和Marathon运行在该容器之中。具体来讲，Docker容器中运行了一个Mesos Master和一个Mesos Slave，以及Marathon和[ZooKeeper](https://zookeeper.apache.org/)。集群架构如下图：\n\n![](/image/150918/architecuture.png)\n\n\n##二. 搭建Mesos/Marathon集群\n\n**1. 下载Docker镜像:**\n\n```sh\nsudo docker pull kiwenlau/single-mesos:3.0\n```\n\n**2. 运行Docker容器:**\n\n```sh\nsudo docker run -p 5050:5050 -p 8080:8080 --name mesos -it -w /root kiwenlau/single-mesos:3.0\n```\n\ndocker run命令运行成功后即进入容器内部，以下为输出：\n\n```bash\nStart ZooKeeper...\nStart Mesos master...\nStart Mesos slave...\nStart Marathon...\n```\n\n\n##三. 测试Mesos/Marathon集群\n\n**1. 通过curl命令调用Marathon的REST API, 创建一个hello程序：**\n\n```sh\ncurl -v -H \"Content-Type: application/json\" -X POST --data \"@hello.json\" http://127.0.0.1:8080/v2/apps\n```\n\n下面为hello.json。由cmd可知，该程序每隔1秒往output.txt文件中写入hello。\n\n```bash\n{\n  \"id\": \"hello\",\n  \"cmd\": \"while [ true ] ; do echo hello >> /root/output.txt; sleep 1; done\",\n  \"cpus\": 0.1,\n  \"mem\": 10.0,\n  \"instances\": 1\n}\n```\n\ncurl执行结果:\n\n```bash\n* Hostname was NOT found in DNS cache\n*   Trying 127.0.0.1...\n* Connected to 127.0.0.1 (127.0.0.1) port 8080 (#0)\n> POST /v2/apps HTTP/1.1\n> User-Agent: curl/7.35.0\n> Host: 127.0.0.1:8080\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 139\n> \n* upload completely sent off: 139 out of 139 bytes\n< HTTP/1.1 201 Created\n< X-Marathon-Leader: http://ec054cabb9af:8080\n< Cache-Control: no-cache, no-store, must-revalidate\n< Pragma: no-cache\n< Expires: 0\n< Location: http://127.0.0.1:8080/v2/apps/hello\n< Content-Type: application/json; qs=2\n< Transfer-Encoding: chunked\n* Server Jetty(8.y.z-SNAPSHOT) is not blacklisted\n< Server: Jetty(8.y.z-SNAPSHOT)\n< \n* Connection #0 to host 127.0.0.1 left intact\n{\"id\":\"/hello\",\"cmd\":\"while [ true ] ; do echo hello >> /root/output.txt; sleep 1; done\",\"args\":null,\"user\":null,\"env\":{},\"instances\":1,\"cpus\":0.1,\"mem\":10.0,\"disk\":0.0,\"executor\":\"\",\"constraints\":[],\"uris\":[],\"storeUrls\":[],\"ports\":[0],\"requirePorts\":false,\"backoffFactor\":1.15,\"container\":null,\"healthChecks\":[],\"dependencies\":[],\"upgradeStrategy\":{\"minimumHealthCapacity\":1.0,\"maximumOverCapacity\":1.0},\"labels\":{},\"acceptedResourceRoles\":null,\"version\":\"2015-09-16T11:22:27.967Z\",\"deployments\":[{\"id\":\"2cd2fdd4-e5f9-4088-895f-7976349b7a19\"}],\"tasks\":[],\"tasksStaged\":0,\"tasksRunning\":0,\"tasksHealthy\":0,\"tasksUnhealthy\":0,\"backoffSeconds\":1,\"maxLaunchDelaySeconds\":3600}\n```\n\n**2. 查看hello程序的运行结果：**\n\n```sh\ntail -f output.txt\n```\n当你看到终端不断输出\"hello\"时说明运行成功。\n\n**3. 使用浏览器查看Mesos和Marathon的网页管理界面**\n\n**注意**将IP替换运行Docker容器的主机IP地址\n\nMesos网页管理界面地址：[http://192.168.59.10:5050](http://192.168.59.10:5050)\n\nMesos网页管理界面如图，可知hello程序正在运行：\n\n![](/image/150918/Mesos.png)\n\nMarathon网页管理界面地址：[http://192.168.59.10:8080](http://192.168.59.10:8080)\n\nMarathon网页管理界面如图，可知hello程序正在运行：\n\n![](/image/150918/Marathon.png)\n\n**4. 通过Marathon网页管理界面创建测试程序**\n\n在Marathon的网页管理界面上点击\"New APP\"，在弹框中配置测试程序。ID为\"hello\", Command为\"echo hello >> /root/output.txt\", 然后点击\"Create\"即可。如下图：\n\n![](/image/150918/hello.png)\n\n\n##四. 存在的问题\n\n其实，参考[Setting up a Single Node Mesosphere Cluster](https://open.mesosphere.com/getting-started/developer/single-node-install/)，可以很快地在ubuntu主机上直接搭建一个单节点的Mesos/Marathon集群。但是，当我安装该教程的步骤将Mesos/Marathon集群打包到Docker镜像中时，遇到了一个比较奇怪的问题。\n\n在Docker容器中使用**\"sudo service mesos-master start\"**和**\"sudo service mesos-slave start\"**命令启动Mesos Master和Mesos Slave时，出现**\"mesos-master: unrecognized service\"**和**\"mesos-slave: unrecognized service\"**错误。但是，我在ubuntu主机上安装Mesos/Marathon集群后，使用同样的命令启动Mesos并没有问题。后来，我是通过直接执行mesos-master和mesos-slave命令启动Mesos，命令如下：\n\n```sh\n/usr/sbin/mesos-master --zk=zk://127.0.0.1:2181/mesos --quorum=1 --work_dir=/var/lib/mesos --log_dir=/log/mesos  \n```\n\n```sh\n/usr/sbin/mesos-slave --master=zk://127.0.0.1:2181/mesos --log_dir=/log/mesos\n```\n\n由这个问题可知，虽然在Docker容器几乎可以运行任意程序，似乎和Ubuntu主机没有区别。但是事实上，**Docker容器与ubuntu主机并非完全一致**，而且这些细节的不同点比较坑。这一点很值得探讨，可以让大家在使用Docker时少走些弯路。对于提到的问题，虽然是解决了，然而我仍然不清楚其中的原因:(\n\n\n##五. Docker镜像备份\n\n我将Docker镜像上传到了灵雀云（Alaudo）的Docker仓库，可以通过以下命令下载和运行：\n\n```sh\nsudo docker pull index.alauda.cn/kiwenlau/single-mesos:3.0\n```\n\n```sh\nsudo docker run -p 5050:5050 -p 8080:8080 --name mesos -it -w /root index.alauda.cn/kiwenlau/single-mesos:3.0\n```\n\n##六. 参考\n\n1. [Setting up a Single Node Mesosphere Cluster](https://open.mesosphere.com/getting-started/developer/single-node-install/)\n2. [Setting up a Cluster on Mesos and Marathon](https://open.mesosphere.com/getting-started/datacenter/install/#master-setup)\n3. [An Introduction to Mesosphere](https://www.digitalocean.com/community/tutorials/an-introduction-to-mesosphere)\n4. [How To Configure a Production-Ready Mesosphere Cluster on Ubuntu 14.04](https://www.digitalocean.com/community/tutorials/how-to-configure-a-production-ready-mesosphere-cluster-on-ubuntu-14-04)\n5. [Deploy a Mesos Cluster with 7 Commands Using Docker](https://medium.com/@gargar454/deploy-a-mesos-cluster-with-7-commands-using-docker-57951e020586)\n6. [sekka1/mesosphere-docker](https://github.com/sekka1/mesosphere-docker)\n7. [Marathon: Application Basics](http://mesosphere.github.io/marathon/docs/application-basics.html)\n8. [Marathon: REST API](http://mesosphere.github.io/marathon/docs/rest-api.html)\n\n***\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文URL地址：\n\n[http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/](http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/)\n***\n\n\n\n\n\n","slug":"150918-single-mesos-docker","published":1,"updated":"2015-12-07T19:04:18.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cij3k03l30009848so7xx5yhs"},{"title":"基于Docker搭建多节点Hadopp集群","date":"2015-06-08T03:44:40.000Z","_content":"\nGitHub: [kiwenlau/hadoop-cluster-docker](https://github.com/kiwenlau)\n\n可以直接进入第三部分，快速在本机搭建一个3个节点的Hadoop集群\n\n\n##一. 项目简介\n\n直接用机器搭建Hadoop集群是一个相当痛苦的过程，尤其对初学者来说。他们还没开始跑wordcount，可能就被这个问题折腾的体无完肤了。\n\n我的目标是将Hadoop集群运行在Docker容器中，使Hadoop开发者能够快速便捷地在本机搭建多节点的Hadoop集群。其实这个想法已经有了不少实现，但是都不是很理想，他们或者镜像太大，或者使用太慢，或者使用了第三方工具使得使用起来过于复杂...下表为一些已知的Hadoop on Docker项目以及其存在的问题。\n\n\n|                  项目                             | 镜像大小    |        问题                        |\n| : ------------- ---------------| : ------ | :-------------------- |\n|sequenceiq/hadoop-docker:latest  |1.491GB    | 镜像太大，只支持单个节点|\n|sequenceiq/hadoop-docker:2.7.0  |1.76 GB     |           同上                           |\n|sequenceiq/hadoop-docker:2.6.0  |1.624GB    |         同上                            |\n|sequenceiq/ambari:latest               |1.782GB     |  镜像太大，使用太慢|\n|sequenceiq/ambari:2.0.0               |4.804GB     |          同上                           |\n|sequenceiq/ambari:latest:1.70      |4.761GB    |         同上                         |\n|alvinhenrick/hadoop-mutinode     |4.331GB    |镜像太大，构建太慢，增加节点麻烦，有bug\n\n我的项目参考了alvinhenrick/hadoop-mutinode项目，不过我做了大量的优化和重构。alvinhenrick/hadoop-mutinode项目的Github主页以及作者所写的博客地址：[GitHub](https://github.com/alvinhenrick/hadoop-mutinode)，[博客](http://alvinhenrick.com/2014/07/16/hadoop-yarn-multinode-cluster-with-docker/)\n\n下面两个表是alvinhenrick/hadoop-mutinode项目与我的kiwenlau/hadoop-cluster-docker项目的参数对比\n\n|镜像名称\t                                 |构建时间\t       | 镜像层数   | 镜像大小 |\n| :-------------------------| :---------- | :------- | :------- |\n|alvinhenrick/serf                     |258.213s       | 21\t        | 239.4MB |\n|alvinhenrick/hadoop-base\t| 2236.055s    | 58\t        | 4.328GB |\n|alvinhenrick/hadoop-dn\t        | 51.959s        | 74\t        | 4.331GB |\n|alvinhenrick/hadoop-nn-dn    | 49.548s       |  84           | 4.331GB |\n\n|镜像名称\t                                 |构建时间\t       | 镜像层数   | 镜像大小 |\n| :-------------------------| :---------- | :------- | :------- |\n| kiwenlau/serf-dnsmasq          | 509.46s        |  8\t        | 206.6 MB |\n|kiwenlau/hadoop-base\t         | 400.29s\t        |  7\t        | 775.4 MB |\n|kiwenlau/hadoop-master         | 5.41s            |  9\t        | 775.4 MB |\n|kiwenlau/hadoop-slave\t         | 2.41s\t        |  8 \t        | 775.4 MB |\n\n可知，我主要优化了这样几点\n- 更小的镜像大小\n- 更快的构造时间\n- 更少的镜像层数\n\n####更快更方便地改变Hadoop集群节点数目\n\n另外，alvinhenrick/hadoop-mutinode项目增加节点时需要手动修改Hadoop配置文件然后重新构建hadoop-nn-dn镜像,然后修改容器启动脚本，才能实现增加节点的功能。而我通过shell脚本实现自动话，不到1分钟可以重新构建hadoop-master镜像，然后立即运行！！！本项目默认启动3个节点的Hadoop集群，支持任意节点数的hadoop集群。\n\n另外，启动hadoop, 运行wordcount以及重新构建镜像都采用了shell脚本实现自动化。这样使得整个项目的使用以及开发都变得非常方便快捷:)\n\n####开发测试环境\n\n- 操作系统：ubuntu 14.04 和 ubuntu 12.04\n- 内核版本: 3.13.0-32-generic\n- Docker版本：1.5.0 和1.6.2\n\n####硬盘不够，内存不够，尤其是内核版本过低会导致运行失败:(\n\n##二. 镜像简介\n\n###本项目一共开发了4个镜像\n\n- serf-dnsmasq\n- hadoop-base\n- hadoop-master\n- hadoop-slave\n\n###serf-dnsmasq镜像\n\n- 基于ubuntu:15.04 (选它是因为它最小，不是因为它最新...)\n- 安装serf: serf是一个分布式的机器节点管理工具。它可以动态地发现所有hadoop集群节点。\n- 安装dnsmasq: dnsmasq作为轻量级的dns服务器。它可以为hadoop集群提供域名解析服务。\n\n容器启动时，master节点的IP会传给所有slave节点。serf会在container启动后立即启动。slave节点上的serf agent会马上发现master节点（master IP它们都知道嘛），master节点就马上发现了所有slave节点。然后它们之间通过互相交换信息，所有节点就能知道其他所有节点的存在了！(Everyone will know Everyone). serf发现新的节点时，就会重新配置dnsmasq,然后重启dnsmasq. 所以dnsmasq就能够解析集群的所有节点的域名啦。这个过程随着节点的增加会耗时更久，因此，若配置的Hadoop节点比较多，则在启动容器后需要测试serf是否发现了所有节点，dns是否能够解析所有节点域名。稍等片刻才能启动Hadoop。这个解决方案是由SequenceIQ公司提出的，该公司专注于将Hadoop运行在Docker中。请参考这个PPT：[Docker-based Hadoop Provisioning](http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning)\n\n###hadoop-base镜像\n\n- 基于serf-dnsmasq镜像\n- 安装JDK(openjdk)\n- 安装openssh-server, 配置无密码ssh\n- 安装vim：介样就可以愉快地在容器中敲代码了:)\n- 安装Hadoop 2.3.0: 安装编译过的hadoop （2.5.2， 2.6.0， 2.7.0 都比2.3.0大，所以我懒得升级了）\n\n编译Hadoop的步骤请参考我的博客：[[Hadoop 2.30 在Ubuntu 14.04 中编译](http://www.cnblogs.com/kiwenlau/p/4227204.html)](http://www.cnblogs.com/kiwenlau/p/4227204.html)\n\n如果需要重新开发我的hadoop-base, 需要下载编译过的hadoop-2.3.0安装包，放到hadoop-cluster-docker/hadoop-base/files目录内。我编译的64位hadoop-2.3.0下载地址：[hadoop-2.3.0](http://pan.baidu.com/s/1sjFRaFz)\n\n另外，我还编译了64位的hadoop 2.5.2, 2.6.0, 2.7.0, 其下载地址如下：\n\n- [hadoop-2.3.0](http://pan.baidu.com/s/1sjFRaFz)\n- [hadoop-2.5.2](http://pan.baidu.com/s/1jGw24aa)\n- [hadoop-2.6.0](http://pan.baidu.com/s/1eQgvF2M)\n- [hadoop-2.7.0]( http://pan.baidu.com/s/1c0HD0Nu)\n\n###hadoop-master镜像\n\n- 基于hadoop-base镜像\n- 配置hadoop的master节点\n- 格式化namenode\n\n这一步需要配置slaves文件，而slaves文件需要列出所有节点的域名或者IP。因此，Hadoop节点数目不同时，slaves文件自然也不一样。因此，更改Hadoop集群节点数目时，需要修改slaves文件然后重新构建hadoop-master镜像。我编写了一个resize-cluster.sh脚本自动化这一过程。仅需给定节点数目作为脚本参数就可以轻松实现Hadoop集群节点数目的更改。由于hadoop-master镜像仅仅做一些配置工作，也无需下载任何文件，整个过程非常快，1分钟就足够了。\n\n###hadoop-slave镜像\n\n- 基于hadoop-base镜像\n- 配置hadoop的slave节点\n\n###镜像大小分析\n\n下表为sudo docker images的运行结果\n\n|REPOSITORY       |   TAG    |  IMAGE ID       | CREATED     |   VIRTUAL SIZE |\n| ------------- | ------- | ---------- | ---------- | ------- |\n|index.alauda.cn/kiwenlau/hadoop-slave   | 0.1.0|    d63869855c03 |   17 hours ago  |  777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-master|    0.1.0   | 7c9d32ede450  |  17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-base  |    0.1.0   | 5571bd5de58e    |17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/serf-dnsmasq  |  0.1.0    |09ed89c24ee8   | 17 hours ago  |  206.7 MB |\n|ubuntu     |                               15.04   | bd94ae587483  |  3 weeks ago    |131.3 MB |\n\n\n易知以下几个结论：\n- serf-dnsmasq镜像在ubuntu:15.04镜像的基础上增加了75.4MB\n- hadoop-base镜像在serf-dnsmasq镜像的基础上增加了570.7MB\n- hadoop-master和hadoop-slave镜像在hadoop-base镜像的基础上大小几乎没有增加\n\n下表为docker history index.alauda.cn/kiwenlau/hadoop-base:0.1.0命令的部分运行结果\n\n|IMAGE      |    CREATED        |    CREATED BY         |                             SIZE\n |  -----  | ---------------  | ---------------  |  -----------  | \n|2039b9b81146 |   44 hours ago   |       /bin/sh -c #(nop) ADD   multi:a93c971a49514e787  |  158.5 MB | \n | cdb620312f30    |  44 hours ago   |       /bin/sh -c apt-get install -y openjdk-7-jdk    |  324.6 MB  | \n | da7d10c790c1   |   44 hours ago      |    /bin/sh -c apt-get install -y openssh-server   |   87.58 MB  | \n |  c65cb568defc    |  44 hours ago     |     /bin/sh -c curl -Lso serf.zip https://dl.bint  |  14.46 MB  | \n | 3e22b3d72e33     | 44 hours ago       |   /bin/sh -c apt-get update && apt-get install     |  60.89 MB   | \n |  b68f8c8d2140    |  3 weeks ago     |     /bin/sh -c #(nop) ADD file:d90f7467c470bfa9a3  |   131.3 MB  | \n\n可知\n- 基础镜像ubuntu:15.04为131.3MB\n- 安装openjdk需要324.6MB\n- 安装hadoop需要158.5MB\n- ubuntu,openjdk与hadoop均为镜像所必须，三者一共占了:614.4MB\n\n### 因此，我所开发的hadoop镜像以及接近最小，优化空间已经很小了\n\n下图显示了项目的Docker镜像结构：\n\n![](/image/150608/image architecture.jpg \"Image Architecture\")\n\n##三. 3节点Hadoop集群搭建步骤\n\n###1. 拉取镜像\n\n```sh\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-master:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-slave:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-base:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/serf-dnsmasq:0.1.0\n```\n\n- 3~5分钟OK~\n\n*查看下载的镜像*\n\n```sh\nsudo docker images\n```\n\n*运行结果*\n\n|REPOSITORY    |    TAG  |    IMAGE ID      |  CREATED  |      VIRTUAL SIZE |\n |  ----------  |  -----------  |  ---------  |   --------  |  \n| index.alauda.cn/kiwenlau/hadoop-slave   | 0.1.0    |d63869855c03  |  17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-master   |   0.1.0 |     7c9d32ede450   |   17 hours ago  |    777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-base |       0.1.0     | 5571bd5de58e   |   17 hours ago  |    777.4 MB | \n |  index.alauda.cn/kiwenlau/serf-dnsmasq   |   0.1.0   |   09ed89c24ee8     | 17 hours ago     |  206.7 MB | \n\n- hadoop-base镜像是基于serf-dnsmasq镜像的，hadoop-slave镜像和hadoop-master镜像都是基于hadoop-base镜像\n- 所以其实4个镜像一共也就777.4MB:)\n\n###2. 修改镜像tag\n\n```sh\nsudo docker tag d63869855c03 kiwenlau/hadoop-slave:0.1.0\nsudo docker tag 7c9d32ede450 kiwenlau/hadoop-master:0.1.0\nsudo docker tag 5571bd5de58e kiwenlau/hadoop-base:0.1.0\nsudo docker tag 09ed89c24ee8 kiwenlau/serf-dnsmasq:0.1.0\n```\n\n*查看修改tag后镜像*\n\n```sh\nsudo docker images\n```\n\n*运行结果*\n\n| REPOSITORY   |   TAG   |    IMAGE ID    |    CREATED    |      VIRTUAL SIZE  | \n |  ----------  |  -----  |  ---------  |  ----------  |  -----------  | \n| index.alauda.cn/kiwenlau/hadoop-slave  |    0.1.0   |   d63869855c03    |  17 hours ago   |   777.4 MB\n | kiwenlau/hadoop-slave    |                  0.1.0   |   d63869855c03   |   17 hours ago  |    777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-master  |  0.1.0 |     7c9d32ede450   |   17 hours ago |     777.4 MB | \n | kiwenlau/hadoop-master  |                  0.1.0   |   7c9d32ede450   |   17 hours ago |     777.4 MB | \n | kiwenlau/hadoop-base   |                   0.1.0  |    5571bd5de58e  |    17 hours ago   |   777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-base  |    0.1.0   |   5571bd5de58e   |   17 hours ago |     777.4 MB | \n | kiwenlau/serf-dnsmasq          |            0.1.0    |  09ed89c24ee8  |    17 hours ago    |  206.7 MB | \n | index.alauda.cn/kiwenlau/serf-dnsmasq  |    0.1.0   |   09ed89c24ee8     | 17 hours ago   |   206.7 MB | \n\n- 之所以要修改镜像，是因为我默认是将镜像上传到Dockerhub, 因此Dokerfile以及shell脚本中得镜像名称都是没有alauada前缀的，sorry for this....不过改tag还是很快滴\n- 若直接下载我在DockerHub中的镜像，自然就不需要修改tag...不过Alauda镜像下载速度很快的哈~\n\n###3.下载源代码\n\n```sh\ngit clone https://github.com/kiwenlau/hadoop-cluster-docker\n```\n\n- 为了防止Github被XX, 我把代码导入到了开源中国的git仓库\n\n```sh\ngit clone http://git.oschina.net/kiwenlau/hadoop-cluster-docker\n```\n\n###4. 运行容器\n\n```sh\ncd hadoop-cluster-docker\n./start-container.sh\n\n```\n\n*运行结果*\n\n```bash\nstart master container...\nstart slave1 container...\nstart slave2 container...\nroot@master:~#\n```\n\n- 一共开启了3个容器，1个master, 2个slave\n- 开启容器后就进入了master容器root用户的家目录（/root）\n\n*查看master的root用户家目录的文件*\n\n```sh\nls\n```\n\n*运行结果*\n\n```plain\nhdfs  run-wordcount.sh\tserf_log  start-hadoop.sh  start-ssh-serf.sh\n```\n\n- start-hadoop.sh是开启hadoop的shell脚本\n- run-wordcount.sh是运行wordcount的shell脚本，可以测试镜像是否正常工作\n\n###5.测试容器是否正常启动(此时已进入master容器)\n\n*查看hadoop集群成员*\n\n```sh\nserf members\n```\n\n*运行结果*\n\n```bash\nmaster.kiwenlau.com  172.17.0.65:7946  alive\nslave1.kiwenlau.com  172.17.0.66:7946  alive\nslave2.kiwenlau.com  172.17.0.67:7946  alive\n```\n\n- 若结果缺少节点，可以稍等片刻，再执行“serf members”命令。因为serf agent需要时间发现所有节点。\n\n*测试ssh*\n\n```sh\nssh slave2.kiwenlau.com\n```\n\n*运行结果*\n\n```bash\nWarning: Permanently added 'slave2.kiwenlau.com,172.17.0.67' (ECDSA) to the list of known hosts.\nWelcome to Ubuntu 15.04 (GNU/Linux 3.13.0-53-generic x86_64)\n* Documentation:  https://help.ubuntu.com/\nThe programs included with the Ubuntu system are free software;\nthe exact distribution terms for each program are described in the\nindividual files in /usr/share/doc/*/copyright.\nUbuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\napplicable law.\nroot@slave2:~#\n```\n\n*退出slave2*\n\n```sh\nexit\n```\n\n*运行结果*\n\n```bash\nlogout\nConnection to slave2.kiwenlau.com closed.\n```\n\n- 若ssh失败，请稍等片刻再测试，因为dnsmasq的dns服务器启动需要时间。\n- 测试成功后，就可以开启Hadoop集群了！其实你也可以不进行测试，开启容器后耐心等待一分钟即可！\n\n###6. 开启hadoop\n\n```sh\n./start-hadoop.sh\n```\n\n- 上一步ssh到slave2之后，请记得回到master啊!!！\n- 运行结果太多，忽略....\n- hadoop的启动速度取决于机器性能....\n\n###7. 运行wordcount\n\n```sh\n./run-wordcount.sh\n```\n\n*运行结果*\n```bash\ninput file1.txt:\nHello Hadoop\ninput file2.txt:\nHello Docker\nwordcount output:\nDocker\t1\nHadoop\t1\nHello\t2\n```\n\n- wordcount的执行速度取决于机器性能....\n\n##四. N节点Hadoop集群搭建步骤\n\n###1. 准备工作\n\n- 参考第二部分1~3：下载镜像，修改tag，下载源代码\n- 注意，你可以不下载serf-dnsmasq, 但是请最好下载hadoop-base，因为hadoop-master是基于hadoop-base构建的\n\n###2. 重新构建hadoop-master镜像\n\n```sh\n./resize-cluster.sh 5\n```\n\n- 不要担心，1分钟就能搞定\n- 你可以为resize-cluster.sh脚本设不同的正整数作为参数数1, 2, 3, 4, 5, 6...\n\n###3. 启动容器\n\n```sh\n./start-container.sh 5\n```\n\n- 你可以为resize-cluster.sh脚本设不同的正整数作为参数数1, 2, 3, 4, 5, 6...\n- 这个参数呢，最好还是得和上一步的参数一致:)\n- 这个参数如果比上一步的参数大，你多启动的节点，Hadoop不认识它们..\n- 这个参数如果比上一步的参数小，Hadoop觉得少启动的节点挂掉了..\n\n###4. 测试工作\n\n- 参考第三部分5~7：测试容器，开启Hadoop，运行wordcount\n- 请注意，若节点增加，请务必先测试容器，然后再开启Hadoop, 因为serf可能还没有发现所有节点，而dnsmasq的DNS服务器表示还没有配置好服务\n- 测试等待时间取决于机器性能....\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/](http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/)\n***","source":"_posts/150608-hadoop-cluster-docker.md","raw":"title: 基于Docker搭建多节点Hadopp集群\n\ndate: 2015-06-08 12:44:40\n\ntags: [Hadoop, Docker]\n\n---\n\nGitHub: [kiwenlau/hadoop-cluster-docker](https://github.com/kiwenlau)\n\n可以直接进入第三部分，快速在本机搭建一个3个节点的Hadoop集群\n\n\n##一. 项目简介\n\n直接用机器搭建Hadoop集群是一个相当痛苦的过程，尤其对初学者来说。他们还没开始跑wordcount，可能就被这个问题折腾的体无完肤了。\n\n我的目标是将Hadoop集群运行在Docker容器中，使Hadoop开发者能够快速便捷地在本机搭建多节点的Hadoop集群。其实这个想法已经有了不少实现，但是都不是很理想，他们或者镜像太大，或者使用太慢，或者使用了第三方工具使得使用起来过于复杂...下表为一些已知的Hadoop on Docker项目以及其存在的问题。\n\n\n|                  项目                             | 镜像大小    |        问题                        |\n| : ------------- ---------------| : ------ | :-------------------- |\n|sequenceiq/hadoop-docker:latest  |1.491GB    | 镜像太大，只支持单个节点|\n|sequenceiq/hadoop-docker:2.7.0  |1.76 GB     |           同上                           |\n|sequenceiq/hadoop-docker:2.6.0  |1.624GB    |         同上                            |\n|sequenceiq/ambari:latest               |1.782GB     |  镜像太大，使用太慢|\n|sequenceiq/ambari:2.0.0               |4.804GB     |          同上                           |\n|sequenceiq/ambari:latest:1.70      |4.761GB    |         同上                         |\n|alvinhenrick/hadoop-mutinode     |4.331GB    |镜像太大，构建太慢，增加节点麻烦，有bug\n\n我的项目参考了alvinhenrick/hadoop-mutinode项目，不过我做了大量的优化和重构。alvinhenrick/hadoop-mutinode项目的Github主页以及作者所写的博客地址：[GitHub](https://github.com/alvinhenrick/hadoop-mutinode)，[博客](http://alvinhenrick.com/2014/07/16/hadoop-yarn-multinode-cluster-with-docker/)\n\n下面两个表是alvinhenrick/hadoop-mutinode项目与我的kiwenlau/hadoop-cluster-docker项目的参数对比\n\n|镜像名称\t                                 |构建时间\t       | 镜像层数   | 镜像大小 |\n| :-------------------------| :---------- | :------- | :------- |\n|alvinhenrick/serf                     |258.213s       | 21\t        | 239.4MB |\n|alvinhenrick/hadoop-base\t| 2236.055s    | 58\t        | 4.328GB |\n|alvinhenrick/hadoop-dn\t        | 51.959s        | 74\t        | 4.331GB |\n|alvinhenrick/hadoop-nn-dn    | 49.548s       |  84           | 4.331GB |\n\n|镜像名称\t                                 |构建时间\t       | 镜像层数   | 镜像大小 |\n| :-------------------------| :---------- | :------- | :------- |\n| kiwenlau/serf-dnsmasq          | 509.46s        |  8\t        | 206.6 MB |\n|kiwenlau/hadoop-base\t         | 400.29s\t        |  7\t        | 775.4 MB |\n|kiwenlau/hadoop-master         | 5.41s            |  9\t        | 775.4 MB |\n|kiwenlau/hadoop-slave\t         | 2.41s\t        |  8 \t        | 775.4 MB |\n\n可知，我主要优化了这样几点\n- 更小的镜像大小\n- 更快的构造时间\n- 更少的镜像层数\n\n####更快更方便地改变Hadoop集群节点数目\n\n另外，alvinhenrick/hadoop-mutinode项目增加节点时需要手动修改Hadoop配置文件然后重新构建hadoop-nn-dn镜像,然后修改容器启动脚本，才能实现增加节点的功能。而我通过shell脚本实现自动话，不到1分钟可以重新构建hadoop-master镜像，然后立即运行！！！本项目默认启动3个节点的Hadoop集群，支持任意节点数的hadoop集群。\n\n另外，启动hadoop, 运行wordcount以及重新构建镜像都采用了shell脚本实现自动化。这样使得整个项目的使用以及开发都变得非常方便快捷:)\n\n####开发测试环境\n\n- 操作系统：ubuntu 14.04 和 ubuntu 12.04\n- 内核版本: 3.13.0-32-generic\n- Docker版本：1.5.0 和1.6.2\n\n####硬盘不够，内存不够，尤其是内核版本过低会导致运行失败:(\n\n##二. 镜像简介\n\n###本项目一共开发了4个镜像\n\n- serf-dnsmasq\n- hadoop-base\n- hadoop-master\n- hadoop-slave\n\n###serf-dnsmasq镜像\n\n- 基于ubuntu:15.04 (选它是因为它最小，不是因为它最新...)\n- 安装serf: serf是一个分布式的机器节点管理工具。它可以动态地发现所有hadoop集群节点。\n- 安装dnsmasq: dnsmasq作为轻量级的dns服务器。它可以为hadoop集群提供域名解析服务。\n\n容器启动时，master节点的IP会传给所有slave节点。serf会在container启动后立即启动。slave节点上的serf agent会马上发现master节点（master IP它们都知道嘛），master节点就马上发现了所有slave节点。然后它们之间通过互相交换信息，所有节点就能知道其他所有节点的存在了！(Everyone will know Everyone). serf发现新的节点时，就会重新配置dnsmasq,然后重启dnsmasq. 所以dnsmasq就能够解析集群的所有节点的域名啦。这个过程随着节点的增加会耗时更久，因此，若配置的Hadoop节点比较多，则在启动容器后需要测试serf是否发现了所有节点，dns是否能够解析所有节点域名。稍等片刻才能启动Hadoop。这个解决方案是由SequenceIQ公司提出的，该公司专注于将Hadoop运行在Docker中。请参考这个PPT：[Docker-based Hadoop Provisioning](http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning)\n\n###hadoop-base镜像\n\n- 基于serf-dnsmasq镜像\n- 安装JDK(openjdk)\n- 安装openssh-server, 配置无密码ssh\n- 安装vim：介样就可以愉快地在容器中敲代码了:)\n- 安装Hadoop 2.3.0: 安装编译过的hadoop （2.5.2， 2.6.0， 2.7.0 都比2.3.0大，所以我懒得升级了）\n\n编译Hadoop的步骤请参考我的博客：[[Hadoop 2.30 在Ubuntu 14.04 中编译](http://www.cnblogs.com/kiwenlau/p/4227204.html)](http://www.cnblogs.com/kiwenlau/p/4227204.html)\n\n如果需要重新开发我的hadoop-base, 需要下载编译过的hadoop-2.3.0安装包，放到hadoop-cluster-docker/hadoop-base/files目录内。我编译的64位hadoop-2.3.0下载地址：[hadoop-2.3.0](http://pan.baidu.com/s/1sjFRaFz)\n\n另外，我还编译了64位的hadoop 2.5.2, 2.6.0, 2.7.0, 其下载地址如下：\n\n- [hadoop-2.3.0](http://pan.baidu.com/s/1sjFRaFz)\n- [hadoop-2.5.2](http://pan.baidu.com/s/1jGw24aa)\n- [hadoop-2.6.0](http://pan.baidu.com/s/1eQgvF2M)\n- [hadoop-2.7.0]( http://pan.baidu.com/s/1c0HD0Nu)\n\n###hadoop-master镜像\n\n- 基于hadoop-base镜像\n- 配置hadoop的master节点\n- 格式化namenode\n\n这一步需要配置slaves文件，而slaves文件需要列出所有节点的域名或者IP。因此，Hadoop节点数目不同时，slaves文件自然也不一样。因此，更改Hadoop集群节点数目时，需要修改slaves文件然后重新构建hadoop-master镜像。我编写了一个resize-cluster.sh脚本自动化这一过程。仅需给定节点数目作为脚本参数就可以轻松实现Hadoop集群节点数目的更改。由于hadoop-master镜像仅仅做一些配置工作，也无需下载任何文件，整个过程非常快，1分钟就足够了。\n\n###hadoop-slave镜像\n\n- 基于hadoop-base镜像\n- 配置hadoop的slave节点\n\n###镜像大小分析\n\n下表为sudo docker images的运行结果\n\n|REPOSITORY       |   TAG    |  IMAGE ID       | CREATED     |   VIRTUAL SIZE |\n| ------------- | ------- | ---------- | ---------- | ------- |\n|index.alauda.cn/kiwenlau/hadoop-slave   | 0.1.0|    d63869855c03 |   17 hours ago  |  777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-master|    0.1.0   | 7c9d32ede450  |  17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-base  |    0.1.0   | 5571bd5de58e    |17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/serf-dnsmasq  |  0.1.0    |09ed89c24ee8   | 17 hours ago  |  206.7 MB |\n|ubuntu     |                               15.04   | bd94ae587483  |  3 weeks ago    |131.3 MB |\n\n\n易知以下几个结论：\n- serf-dnsmasq镜像在ubuntu:15.04镜像的基础上增加了75.4MB\n- hadoop-base镜像在serf-dnsmasq镜像的基础上增加了570.7MB\n- hadoop-master和hadoop-slave镜像在hadoop-base镜像的基础上大小几乎没有增加\n\n下表为docker history index.alauda.cn/kiwenlau/hadoop-base:0.1.0命令的部分运行结果\n\n|IMAGE      |    CREATED        |    CREATED BY         |                             SIZE\n |  -----  | ---------------  | ---------------  |  -----------  | \n|2039b9b81146 |   44 hours ago   |       /bin/sh -c #(nop) ADD   multi:a93c971a49514e787  |  158.5 MB | \n | cdb620312f30    |  44 hours ago   |       /bin/sh -c apt-get install -y openjdk-7-jdk    |  324.6 MB  | \n | da7d10c790c1   |   44 hours ago      |    /bin/sh -c apt-get install -y openssh-server   |   87.58 MB  | \n |  c65cb568defc    |  44 hours ago     |     /bin/sh -c curl -Lso serf.zip https://dl.bint  |  14.46 MB  | \n | 3e22b3d72e33     | 44 hours ago       |   /bin/sh -c apt-get update && apt-get install     |  60.89 MB   | \n |  b68f8c8d2140    |  3 weeks ago     |     /bin/sh -c #(nop) ADD file:d90f7467c470bfa9a3  |   131.3 MB  | \n\n可知\n- 基础镜像ubuntu:15.04为131.3MB\n- 安装openjdk需要324.6MB\n- 安装hadoop需要158.5MB\n- ubuntu,openjdk与hadoop均为镜像所必须，三者一共占了:614.4MB\n\n### 因此，我所开发的hadoop镜像以及接近最小，优化空间已经很小了\n\n下图显示了项目的Docker镜像结构：\n\n![](/image/150608/image architecture.jpg \"Image Architecture\")\n\n##三. 3节点Hadoop集群搭建步骤\n\n###1. 拉取镜像\n\n```sh\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-master:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-slave:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-base:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/serf-dnsmasq:0.1.0\n```\n\n- 3~5分钟OK~\n\n*查看下载的镜像*\n\n```sh\nsudo docker images\n```\n\n*运行结果*\n\n|REPOSITORY    |    TAG  |    IMAGE ID      |  CREATED  |      VIRTUAL SIZE |\n |  ----------  |  -----------  |  ---------  |   --------  |  \n| index.alauda.cn/kiwenlau/hadoop-slave   | 0.1.0    |d63869855c03  |  17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-master   |   0.1.0 |     7c9d32ede450   |   17 hours ago  |    777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-base |       0.1.0     | 5571bd5de58e   |   17 hours ago  |    777.4 MB | \n |  index.alauda.cn/kiwenlau/serf-dnsmasq   |   0.1.0   |   09ed89c24ee8     | 17 hours ago     |  206.7 MB | \n\n- hadoop-base镜像是基于serf-dnsmasq镜像的，hadoop-slave镜像和hadoop-master镜像都是基于hadoop-base镜像\n- 所以其实4个镜像一共也就777.4MB:)\n\n###2. 修改镜像tag\n\n```sh\nsudo docker tag d63869855c03 kiwenlau/hadoop-slave:0.1.0\nsudo docker tag 7c9d32ede450 kiwenlau/hadoop-master:0.1.0\nsudo docker tag 5571bd5de58e kiwenlau/hadoop-base:0.1.0\nsudo docker tag 09ed89c24ee8 kiwenlau/serf-dnsmasq:0.1.0\n```\n\n*查看修改tag后镜像*\n\n```sh\nsudo docker images\n```\n\n*运行结果*\n\n| REPOSITORY   |   TAG   |    IMAGE ID    |    CREATED    |      VIRTUAL SIZE  | \n |  ----------  |  -----  |  ---------  |  ----------  |  -----------  | \n| index.alauda.cn/kiwenlau/hadoop-slave  |    0.1.0   |   d63869855c03    |  17 hours ago   |   777.4 MB\n | kiwenlau/hadoop-slave    |                  0.1.0   |   d63869855c03   |   17 hours ago  |    777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-master  |  0.1.0 |     7c9d32ede450   |   17 hours ago |     777.4 MB | \n | kiwenlau/hadoop-master  |                  0.1.0   |   7c9d32ede450   |   17 hours ago |     777.4 MB | \n | kiwenlau/hadoop-base   |                   0.1.0  |    5571bd5de58e  |    17 hours ago   |   777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-base  |    0.1.0   |   5571bd5de58e   |   17 hours ago |     777.4 MB | \n | kiwenlau/serf-dnsmasq          |            0.1.0    |  09ed89c24ee8  |    17 hours ago    |  206.7 MB | \n | index.alauda.cn/kiwenlau/serf-dnsmasq  |    0.1.0   |   09ed89c24ee8     | 17 hours ago   |   206.7 MB | \n\n- 之所以要修改镜像，是因为我默认是将镜像上传到Dockerhub, 因此Dokerfile以及shell脚本中得镜像名称都是没有alauada前缀的，sorry for this....不过改tag还是很快滴\n- 若直接下载我在DockerHub中的镜像，自然就不需要修改tag...不过Alauda镜像下载速度很快的哈~\n\n###3.下载源代码\n\n```sh\ngit clone https://github.com/kiwenlau/hadoop-cluster-docker\n```\n\n- 为了防止Github被XX, 我把代码导入到了开源中国的git仓库\n\n```sh\ngit clone http://git.oschina.net/kiwenlau/hadoop-cluster-docker\n```\n\n###4. 运行容器\n\n```sh\ncd hadoop-cluster-docker\n./start-container.sh\n\n```\n\n*运行结果*\n\n```bash\nstart master container...\nstart slave1 container...\nstart slave2 container...\nroot@master:~#\n```\n\n- 一共开启了3个容器，1个master, 2个slave\n- 开启容器后就进入了master容器root用户的家目录（/root）\n\n*查看master的root用户家目录的文件*\n\n```sh\nls\n```\n\n*运行结果*\n\n```plain\nhdfs  run-wordcount.sh\tserf_log  start-hadoop.sh  start-ssh-serf.sh\n```\n\n- start-hadoop.sh是开启hadoop的shell脚本\n- run-wordcount.sh是运行wordcount的shell脚本，可以测试镜像是否正常工作\n\n###5.测试容器是否正常启动(此时已进入master容器)\n\n*查看hadoop集群成员*\n\n```sh\nserf members\n```\n\n*运行结果*\n\n```bash\nmaster.kiwenlau.com  172.17.0.65:7946  alive\nslave1.kiwenlau.com  172.17.0.66:7946  alive\nslave2.kiwenlau.com  172.17.0.67:7946  alive\n```\n\n- 若结果缺少节点，可以稍等片刻，再执行“serf members”命令。因为serf agent需要时间发现所有节点。\n\n*测试ssh*\n\n```sh\nssh slave2.kiwenlau.com\n```\n\n*运行结果*\n\n```bash\nWarning: Permanently added 'slave2.kiwenlau.com,172.17.0.67' (ECDSA) to the list of known hosts.\nWelcome to Ubuntu 15.04 (GNU/Linux 3.13.0-53-generic x86_64)\n* Documentation:  https://help.ubuntu.com/\nThe programs included with the Ubuntu system are free software;\nthe exact distribution terms for each program are described in the\nindividual files in /usr/share/doc/*/copyright.\nUbuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\napplicable law.\nroot@slave2:~#\n```\n\n*退出slave2*\n\n```sh\nexit\n```\n\n*运行结果*\n\n```bash\nlogout\nConnection to slave2.kiwenlau.com closed.\n```\n\n- 若ssh失败，请稍等片刻再测试，因为dnsmasq的dns服务器启动需要时间。\n- 测试成功后，就可以开启Hadoop集群了！其实你也可以不进行测试，开启容器后耐心等待一分钟即可！\n\n###6. 开启hadoop\n\n```sh\n./start-hadoop.sh\n```\n\n- 上一步ssh到slave2之后，请记得回到master啊!!！\n- 运行结果太多，忽略....\n- hadoop的启动速度取决于机器性能....\n\n###7. 运行wordcount\n\n```sh\n./run-wordcount.sh\n```\n\n*运行结果*\n```bash\ninput file1.txt:\nHello Hadoop\ninput file2.txt:\nHello Docker\nwordcount output:\nDocker\t1\nHadoop\t1\nHello\t2\n```\n\n- wordcount的执行速度取决于机器性能....\n\n##四. N节点Hadoop集群搭建步骤\n\n###1. 准备工作\n\n- 参考第二部分1~3：下载镜像，修改tag，下载源代码\n- 注意，你可以不下载serf-dnsmasq, 但是请最好下载hadoop-base，因为hadoop-master是基于hadoop-base构建的\n\n###2. 重新构建hadoop-master镜像\n\n```sh\n./resize-cluster.sh 5\n```\n\n- 不要担心，1分钟就能搞定\n- 你可以为resize-cluster.sh脚本设不同的正整数作为参数数1, 2, 3, 4, 5, 6...\n\n###3. 启动容器\n\n```sh\n./start-container.sh 5\n```\n\n- 你可以为resize-cluster.sh脚本设不同的正整数作为参数数1, 2, 3, 4, 5, 6...\n- 这个参数呢，最好还是得和上一步的参数一致:)\n- 这个参数如果比上一步的参数大，你多启动的节点，Hadoop不认识它们..\n- 这个参数如果比上一步的参数小，Hadoop觉得少启动的节点挂掉了..\n\n###4. 测试工作\n\n- 参考第三部分5~7：测试容器，开启Hadoop，运行wordcount\n- 请注意，若节点增加，请务必先测试容器，然后再开启Hadoop, 因为serf可能还没有发现所有节点，而dnsmasq的DNS服务器表示还没有配置好服务\n- 测试等待时间取决于机器性能....\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/](http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/)\n***","slug":"150608-hadoop-cluster-docker","published":1,"updated":"2015-12-07T19:07:11.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cij3k03l5000f848s0ni1hp2q"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cij3k03kq0001848sjsr6jhiy","tag_id":"cij3k03ku0002848s5faqjba3","_id":"cij3k03kv0004848s7vx18k29"},{"post_id":"cij3k03kq0001848sjsr6jhiy","tag_id":"cij3k03kv0003848sxilil4nx","_id":"cij3k03kw0005848skugy5hui"},{"post_id":"cij3k03l00006848sijsqn889","tag_id":"cij3k03ku0002848s5faqjba3","_id":"cij3k03l20007848shlv9bjtq"},{"post_id":"cij3k03l00006848sijsqn889","tag_id":"cij3k03kv0003848sxilil4nx","_id":"cij3k03l20008848syrb2y40r"},{"post_id":"cij3k03l30009848so7xx5yhs","tag_id":"cij3k03ku0002848s5faqjba3","_id":"cij3k03l3000c848si8efr2i7"},{"post_id":"cij3k03l30009848so7xx5yhs","tag_id":"cij3k03l3000a848sb76tpg2y","_id":"cij3k03l4000d848s0ttvoc4q"},{"post_id":"cij3k03l30009848so7xx5yhs","tag_id":"cij3k03l3000b848sbmgc3yb3","_id":"cij3k03l4000e848s0ntpw4kq"},{"post_id":"cij3k03l5000f848s0ni1hp2q","tag_id":"cij3k03l5000g848sbcuijhnd","_id":"cij3k03l5000h848sraojgrq0"},{"post_id":"cij3k03l5000f848s0ni1hp2q","tag_id":"cij3k03ku0002848s5faqjba3","_id":"cij3k03l5000i848s597emyg4"}],"Tag":[{"name":"Docker","_id":"cij3k03ku0002848s5faqjba3"},{"name":"Kubernetes","_id":"cij3k03kv0003848sxilil4nx"},{"name":"Mesos","_id":"cij3k03l3000a848sb76tpg2y"},{"name":"Marathon","_id":"cij3k03l3000b848sbmgc3yb3"},{"name":"Hadoop","_id":"cij3k03l5000g848sbcuijhnd"}]}}