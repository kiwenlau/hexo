{"meta":{"version":1,"warehouse":"1.0.3"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0},{"_id":"source/image/160619/vagrant-vm.png","path":"image/160619/vagrant-vm.png","modified":0},{"_id":"source/image/160619/vagrant-vm.graffle","path":"image/160619/vagrant-vm.graffle","modified":0},{"_id":"source/image/160612/hadoop-cluster-docker.png","path":"image/160612/hadoop-cluster-docker.png","modified":0},{"_id":"source/image/160612/hadoop-cluster-docker.graffle","path":"image/160612/hadoop-cluster-docker.graffle","modified":0},{"_id":"source/image/160605/hadoop-docker.png","path":"image/160605/hadoop-docker.png","modified":0},{"_id":"source/image/160605/hadoop-docker.graffle","path":"image/160605/hadoop-docker.graffle","modified":0},{"_id":"source/image/160529/ubuntu-hadoop.png","path":"image/160529/ubuntu-hadoop.png","modified":0},{"_id":"source/image/160529/ubuntu-hadoop.graffle","path":"image/160529/ubuntu-hadoop.graffle","modified":0},{"_id":"source/image/160109/single-kubernetes-docker.png","path":"image/160109/single-kubernetes-docker.png","modified":0},{"_id":"source/image/160109/kubernetes-shell-supervisor.png","path":"image/160109/kubernetes-shell-supervisor.png","modified":0},{"_id":"source/image/151128/single-kubernetes-docker.png","path":"image/151128/single-kubernetes-docker.png","modified":0},{"_id":"source/image/150918/kiwenlau_single_mesos.graffle","path":"image/150918/kiwenlau_single_mesos.graffle","modified":0},{"_id":"source/image/150918/hello.png","path":"image/150918/hello.png","modified":0},{"_id":"source/image/150918/architecuture.png","path":"image/150918/architecuture.png","modified":0},{"_id":"source/image/150918/Mesos.png","path":"image/150918/Mesos.png","modified":0},{"_id":"source/image/150918/Marathon.png","path":"image/150918/Marathon.png","modified":0},{"_id":"source/image/150608/image architecture.jpg","path":"image/150608/image architecture.jpg","modified":0},{"_id":"themes/light/source/kiwenlau.jpg","path":"kiwenlau.jpg","modified":0},{"_id":"themes/light/source/kiwenlau.ico","path":"kiwenlau.ico","modified":0},{"_id":"themes/light/source/js/jquery.imagesloaded.min.js","path":"js/jquery.imagesloaded.min.js","modified":0},{"_id":"themes/light/source/js/gallery.js","path":"js/gallery.js","modified":0},{"_id":"themes/light/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0},{"_id":"themes/light/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0},{"_id":"themes/light/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0},{"_id":"themes/light/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0},{"_id":"themes/light/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0},{"_id":"themes/light/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0},{"_id":"themes/light/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0},{"_id":"themes/light/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0},{"_id":"themes/light/source/css/style.styl","path":"css/style.styl","modified":0},{"_id":"themes/light/source/css/font/fontawesome-webfont.woff","path":"css/font/fontawesome-webfont.woff","modified":0},{"_id":"themes/light/source/css/font/fontawesome-webfont.ttf","path":"css/font/fontawesome-webfont.ttf","modified":0},{"_id":"themes/light/source/css/font/fontawesome-webfont.svg","path":"css/font/fontawesome-webfont.svg","modified":0},{"_id":"themes/light/source/css/font/fontawesome-webfont.eot","path":"css/font/fontawesome-webfont.eot","modified":0}],"Cache":[{"_id":"source/CNAME","shasum":"abcdc15ac03b8fdb087e2ae5c120ef20dd63dcd1","modified":1433732308000},{"_id":"source/_posts/150918-single-mesos-docker.md","shasum":"a86f56923a28db17c8943d3ab1b142c8d1c4d8d4","modified":1463498333000},{"_id":"source/_posts/160529-compile-hadoop-ubuntu.md","shasum":"d2369c039d39d19c377ba060aa29d5fd9c44f7b7","modified":1464501443000},{"_id":"source/_posts/160605-compile-hadoop-docker.md","shasum":"19c719bfcdbf04243f8ff0ce4cd7b117e3940734","modified":1465742279000},{"_id":"source/_posts/160612-hadoop-cluster-docker-update.md","shasum":"e9634f9f7d81d667e40b4e62340bf648b1c96c4d","modified":1465832813000},{"_id":"source/_posts/160619-vagrant-virtual-machine.md","shasum":"8afa2efe7ef5fcff3b6f4428946ad6e86d81a709","modified":1466304739000},{"_id":"source/google6c2fe14874348911.html","shasum":"13b1c75c0ffea7dd64b61e99c75e328e81812700","modified":1433934386000},{"_id":"source/_posts/151128-single-kubernetes-docker.md","shasum":"d50109520c854232cab1a02daab7ed1f680eb792","modified":1463498338000},{"_id":"source/_posts/160109-multiple-processes--docker-container.md","shasum":"bd7eb602b5fcc64ba42c100e1575e29862fa4227","modified":1463496628000},{"_id":"source/_posts/150608-hadoop-cluster-docker.md","shasum":"830f1c96409dc4c976482b93d47fccc4dc06a312","modified":1465137813000},{"_id":"source/image/150918/kiwenlau_single_mesos.graffle","shasum":"3fcf8e0540f83077eac1114e5b431e64886f19ca","modified":1442407988000},{"_id":"source/image/150608/image architecture.jpg","shasum":"450f2dffe1127f102221a9b3c934eb6054876d85","modified":1433822708000},{"_id":"source/image/151128/single-kubernetes-docker.png","shasum":"75ee08265d42589d56dd36ac764d40521af2fe8c","modified":1448502361000},{"_id":"source/image/150918/architecuture.png","shasum":"43b51d2b17bb54c7c11530d40f1068a558ebde10","modified":1442408879000},{"_id":"source/image/160529/ubuntu-hadoop.png","shasum":"bf8cffa36c93a4ae827741a3d7acb66eb435c867","modified":1464501013000},{"_id":"source/image/160109/kubernetes-shell-supervisor.png","shasum":"4dac32b9fb7c178e97b78dfca383380c74689f56","modified":1452343660000},{"_id":"source/image/160109/single-kubernetes-docker.png","shasum":"4f4a736bc38089dd10f54f2fc07fe999f2f57169","modified":1452333614000},{"_id":"source/image/160612/hadoop-cluster-docker.graffle","shasum":"7cdcea1f7fa811264243cd63cdde203b9a7c3d28","modified":1465828785000},{"_id":"source/image/160612/hadoop-cluster-docker.png","shasum":"d38295b19d7629215917511cd113c78769bde759","modified":1465828547000},{"_id":"source/image/160619/vagrant-vm.graffle","shasum":"0357e87cb247241c4092ba31faa2101acf83db32","modified":1466268780000},{"_id":"source/image/150918/Marathon.png","shasum":"58abd8a1ba30eedef47649093a37291e29f21e00","modified":1442402866000},{"_id":"source/image/150918/hello.png","shasum":"732ba80324b66a3ff47fafbde89f4c61dfb04a08","modified":1442471270000},{"_id":"source/image/160619/vagrant-vm.png","shasum":"299bba0e9557c4fbafaa620429b18fbae24f1522","modified":1466268780000},{"_id":"themes/light/source/css/_base/utils.styl","shasum":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1433597960000},{"_id":"themes/light/LICENSE","shasum":"c6f301bc722f0af3a55267a36c1c147aeddc6e46","modified":1433597960000},{"_id":"themes/light/README.md","shasum":"aa189c7ff03c60d8fceb009f5fca1a61d8a0ecdf","modified":1433597960000},{"_id":"themes/light/_config.yml","shasum":"7034c507f4e76134b61e085b2cba66b42f1f45ca","modified":1464369639000},{"_id":"themes/light/languages/de.yml","shasum":"e076c7f2eb29ebcfb04d94861bf3063c4b08078c","modified":1433597960000},{"_id":"themes/light/languages/default.yml","shasum":"fd7397be7789b43c1c163ab4faf106318811c2a8","modified":1433597960000},{"_id":"themes/light/languages/es.yml","shasum":"de273af604b27812cfd4195e7b7f28ceff2734b3","modified":1433597960000},{"_id":"themes/light/languages/ru.yml","shasum":"35aadf8fdd28aaff8a1c8f50e80201dcf8ce0604","modified":1433597960000},{"_id":"themes/light/languages/zh-CN.yml","shasum":"16979cea6653f9ed345b6ed2c09823d75a622ded","modified":1464369792000},{"_id":"themes/light/languages/zh-TW.yml","shasum":"bcfd502bf073d4c451c13d05d2571f814d1766c9","modified":1464369404000},{"_id":"themes/light/layout/_partial/after_footer.ejs","shasum":"e3fc00ab06a8e41051602b65ef8a4f968a4bf2bb","modified":1433941216000},{"_id":"themes/light/layout/_partial/archive.ejs","shasum":"7e4f7c2909b1b90241424ea2ff8e7b4761d8360f","modified":1433597960000},{"_id":"themes/light/layout/_partial/article.ejs","shasum":"bcae2ea030e69242d938c33094fc60207bf25e22","modified":1433609506000},{"_id":"themes/light/layout/_partial/comment.ejs","shasum":"c5409a6920f96816506b4e0d5dfae13449021654","modified":1433609464000},{"_id":"themes/light/layout/_partial/facebook_comment.ejs","shasum":"3fdc1d0ce9177825e7417635fbc545a35d528d04","modified":1433597960000},{"_id":"themes/light/layout/_partial/footer.ejs","shasum":"61224149335ae515b0c23e27070c11b05d78749d","modified":1434024998000},{"_id":"themes/light/layout/_partial/google_analytics.ejs","shasum":"7cf0d1f93051bda510d49dab7f684b9d7c6ba58f","modified":1433597960000},{"_id":"themes/light/layout/_partial/head.ejs","shasum":"d892420cbd253bf8707da7affb8f489cebaae6fd","modified":1433941148000},{"_id":"themes/light/layout/_partial/header.ejs","shasum":"d9a99aca97d8b41ed907fbf5b25df05da3ffa4f6","modified":1433597960000},{"_id":"themes/light/layout/_partial/pagination.ejs","shasum":"1206b630a07444e8744365f14ddb26095c925ae1","modified":1433597960000},{"_id":"themes/light/layout/_partial/post/category.ejs","shasum":"be740939c5c2d4ffdbed9557b4e63a590058b476","modified":1433597960000},{"_id":"themes/light/layout/_partial/post/gallery.ejs","shasum":"fafc2501d7e65983b0f5c2b58151ca12e57c0574","modified":1433597960000},{"_id":"themes/light/layout/_partial/post/share.ejs","shasum":"24c04b319f1b19e887c42db961b90a7e0ab26fdc","modified":1433597960000},{"_id":"themes/light/layout/_partial/post/tag.ejs","shasum":"095418df66a27a28cbab16d7cb0d16001b0e23f1","modified":1433597960000},{"_id":"themes/light/layout/_partial/post/title.ejs","shasum":"d7fbc575d35ae68f9045a382c651450e4131f335","modified":1433597960000},{"_id":"themes/light/layout/_partial/sidebar.ejs","shasum":"caf351797a18d03d8ee945ceb9f83785c50c09f9","modified":1433597960000},{"_id":"themes/light/layout/_widget/category.ejs","shasum":"8a2b90dc29661371f060f710668929c3588e15e4","modified":1433597960000},{"_id":"themes/light/layout/_widget/link.ejs","shasum":"736f9534b44a5597c6d4a17710d4fa5dbfb5a966","modified":1466153504000},{"_id":"themes/light/layout/_widget/photo.ejs","shasum":"a985f891ccad6a54352c38de34e90e14005c3e04","modified":1442554487000},{"_id":"themes/light/layout/_widget/recent_posts.ejs","shasum":"8f2f3963bd568c681d7585bb8099fb1b3e1d4c81","modified":1442555459000},{"_id":"themes/light/layout/_widget/search.ejs","shasum":"55c707f3aa7453c305c41898ad22556edd213830","modified":1433597960000},{"_id":"themes/light/layout/_widget/tag.ejs","shasum":"1914db78bea49c333067d79fe7ad9567d2b08d00","modified":1433597960000},{"_id":"themes/light/layout/_widget/tagcloud.ejs","shasum":"673b598903e1b2b9d9ea31dc79bab65ef3984348","modified":1433929542000},{"_id":"source/image/160529/ubuntu-hadoop.graffle","shasum":"f3440aec0c14d1638dfe9e15d760ef02b5784190","modified":1464501309000},{"_id":"themes/light/layout/archive.ejs","shasum":"a18842e3d719fe3ca9b977a6995f8facc75c8673","modified":1433597960000},{"_id":"themes/light/layout/category.ejs","shasum":"9b740fc33f6f028df60f0bc4312bf3ebd03aa8ea","modified":1433597960000},{"_id":"themes/light/layout/index.ejs","shasum":"e569d8fe0741a24efb89e44781f9e616da17e036","modified":1433597960000},{"_id":"themes/light/layout/layout.ejs","shasum":"72da76881ebf00e71d7cc196f377e37a17ec7a6f","modified":1433597960000},{"_id":"themes/light/layout/page.ejs","shasum":"70cbc9854655773cc6ba84eecaaf330fed430465","modified":1433597960000},{"_id":"themes/light/layout/post.ejs","shasum":"70cbc9854655773cc6ba84eecaaf330fed430465","modified":1433597960000},{"_id":"themes/light/source/css/_base/layout.styl","shasum":"1b58c21aa48a8f9f7f811af681ac182dd058e23d","modified":1433597960000},{"_id":"themes/light/layout/tag.ejs","shasum":"45150a2365768b6b67880193c9264ad2bb4814db","modified":1433597960000},{"_id":"themes/light/source/css/_partial/archive.styl","shasum":"072e9b8c5ee9acf95ac7cce9c34706d41e412229","modified":1433597960000},{"_id":"themes/light/source/css/_base/variable.styl","shasum":"c4e07cb7f4ec980553cd19d9d2cd8a1a44d4cd82","modified":1434027874000},{"_id":"themes/light/source/css/_partial/comment.styl","shasum":"a74254a6e713a522134cca4c644fde681c502823","modified":1434028066000},{"_id":"themes/light/source/css/_partial/footer.styl","shasum":"1757872dbdbd09295a625f13e356aa798a8bb308","modified":1433597960000},{"_id":"themes/light/source/css/_partial/header.styl","shasum":"0f932c9514d13fea70fe109242e17ee633a2b28a","modified":1434021576000},{"_id":"themes/light/source/css/_partial/index.styl","shasum":"7a8c0ec6ab99a9f8e00c9687aca29d31752424a2","modified":1433597960000},{"_id":"themes/light/source/css/_partial/sidebar.styl","shasum":"1bdd787d9dc40829dcab26e0e4543c0d2325b5b8","modified":1434022784000},{"_id":"themes/light/source/css/_partial/syntax.styl","shasum":"031903fe83aa7d9960bd084f4bb77a6671abcbfa","modified":1443169038000},{"_id":"source/image/160605/hadoop-docker.png","shasum":"2a20af3baf8c768e3170823eba14c20bae2c98a8","modified":1464712581000},{"_id":"themes/light/source/css/_partial/article.styl","shasum":"09e00a528f2524093660cf96647282aa2b98abbf","modified":1434030084000},{"_id":"themes/light/source/css/font/fontawesome-webfont.eot","shasum":"d775f599ff3f23be082e6a9604b4898718923a37","modified":1433597960000},{"_id":"source/image/160605/hadoop-docker.graffle","shasum":"20a5234845055aadce99cd447f200a25e2b7d35c","modified":1464713631000},{"_id":"themes/light/source/css/style.styl","shasum":"c03b2520e4a85b981e29516cadc0a365e6500e3d","modified":1433597960000},{"_id":"themes/light/source/fancybox/blank.gif","shasum":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1433597960000},{"_id":"themes/light/source/fancybox/fancybox_loading.gif","shasum":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1433597960000},{"_id":"themes/light/source/fancybox/fancybox_overlay.png","shasum":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1433597960000},{"_id":"themes/light/source/fancybox/fancybox_sprite.png","shasum":"17df19f97628e77be09c352bf27425faea248251","modified":1433597960000},{"_id":"themes/light/source/fancybox/fancybox_loading@2x.gif","shasum":"273b123496a42ba45c3416adb027cd99745058b0","modified":1433597960000},{"_id":"themes/light/source/fancybox/fancybox_sprite@2x.png","shasum":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1433597960000},{"_id":"themes/light/source/fancybox/jquery.fancybox.css","shasum":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1433597960000},{"_id":"themes/light/source/js/gallery.js","shasum":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed","modified":1433597960000},{"_id":"themes/light/source/js/jquery.imagesloaded.min.js","shasum":"4109837b1f6477bacc6b095a863b1b95b1b3693f","modified":1433597960000},{"_id":"themes/light/source/css/font/fontawesome-webfont.woff","shasum":"0612cddf2f835cceffccc88fd194f97367d0b024","modified":1433597960000},{"_id":"source/image/150918/Mesos.png","shasum":"c21fb25a6da8801ecb9b6b25888cb88da16790b4","modified":1442402816000},{"_id":"themes/light/source/css/font/fontawesome-webfont.ttf","shasum":"a9468f6a1fe965fbcaf5a1bd6c11705e2fc5f84c","modified":1433597960000},{"_id":"themes/light/source/fancybox/jquery.fancybox.pack.js","shasum":"53360764b429c212f424399384417ccc233bb3be","modified":1433597960000},{"_id":"themes/light/source/kiwenlau.jpg","shasum":"e163949f9617ee5a0b92ef131f6e2bb57a2c0e36","modified":1433690104000},{"_id":"themes/light/source/kiwenlau.ico","shasum":"987d9c1658c3779037d96f49d8779b9bab18440b","modified":1433759880000},{"_id":"themes/light/source/css/font/fontawesome-webfont.svg","shasum":"d162419c91b8bab3a4fd327c933a0fcf3799c251","modified":1433597960000},{"_id":"public/CNAME","modified":1466268982391,"shasum":"abcdc15ac03b8fdb087e2ae5c120ef20dd63dcd1"},{"_id":"public/image/160619/vagrant-vm.png","modified":1466268982394,"shasum":"299bba0e9557c4fbafaa620429b18fbae24f1522"},{"_id":"public/image/160619/vagrant-vm.graffle","modified":1466268982397,"shasum":"0357e87cb247241c4092ba31faa2101acf83db32"},{"_id":"public/image/160612/hadoop-cluster-docker.png","modified":1466268982398,"shasum":"d38295b19d7629215917511cd113c78769bde759"},{"_id":"public/image/160612/hadoop-cluster-docker.graffle","modified":1466268982400,"shasum":"7cdcea1f7fa811264243cd63cdde203b9a7c3d28"},{"_id":"public/image/160605/hadoop-docker.png","modified":1466268982402,"shasum":"2a20af3baf8c768e3170823eba14c20bae2c98a8"},{"_id":"public/image/160605/hadoop-docker.graffle","modified":1466268982406,"shasum":"20a5234845055aadce99cd447f200a25e2b7d35c"},{"_id":"public/image/160529/ubuntu-hadoop.png","modified":1466268982408,"shasum":"bf8cffa36c93a4ae827741a3d7acb66eb435c867"},{"_id":"public/image/160529/ubuntu-hadoop.graffle","modified":1466268982410,"shasum":"f3440aec0c14d1638dfe9e15d760ef02b5784190"},{"_id":"public/image/160109/single-kubernetes-docker.png","modified":1466268982412,"shasum":"4f4a736bc38089dd10f54f2fc07fe999f2f57169"},{"_id":"public/image/160109/kubernetes-shell-supervisor.png","modified":1466268982417,"shasum":"4dac32b9fb7c178e97b78dfca383380c74689f56"},{"_id":"public/image/151128/single-kubernetes-docker.png","modified":1466268982418,"shasum":"75ee08265d42589d56dd36ac764d40521af2fe8c"},{"_id":"public/image/150918/kiwenlau_single_mesos.graffle","modified":1466268982420,"shasum":"3fcf8e0540f83077eac1114e5b431e64886f19ca"},{"_id":"public/image/150918/hello.png","modified":1466268982422,"shasum":"732ba80324b66a3ff47fafbde89f4c61dfb04a08"},{"_id":"public/image/150918/architecuture.png","modified":1466268982423,"shasum":"43b51d2b17bb54c7c11530d40f1068a558ebde10"},{"_id":"public/image/150918/Mesos.png","modified":1466268982426,"shasum":"c21fb25a6da8801ecb9b6b25888cb88da16790b4"},{"_id":"public/image/150918/Marathon.png","modified":1466268982427,"shasum":"58abd8a1ba30eedef47649093a37291e29f21e00"},{"_id":"public/image/150608/image architecture.jpg","modified":1466268982429,"shasum":"450f2dffe1127f102221a9b3c934eb6054876d85"},{"_id":"public/kiwenlau.jpg","modified":1466268982433,"shasum":"e163949f9617ee5a0b92ef131f6e2bb57a2c0e36"},{"_id":"public/kiwenlau.ico","modified":1466268982441,"shasum":"987d9c1658c3779037d96f49d8779b9bab18440b"},{"_id":"public/js/jquery.imagesloaded.min.js","modified":1466268982443,"shasum":"4109837b1f6477bacc6b095a863b1b95b1b3693f"},{"_id":"public/js/gallery.js","modified":1466268982445,"shasum":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed"},{"_id":"public/fancybox/jquery.fancybox.pack.js","modified":1466268982447,"shasum":"53360764b429c212f424399384417ccc233bb3be"},{"_id":"public/fancybox/jquery.fancybox.css","modified":1466268982449,"shasum":"5f163444617b6cf267342f06ac166a237bb62df9"},{"_id":"public/fancybox/fancybox_sprite@2x.png","modified":1466268982452,"shasum":"30c58913f327e28f466a00f4c1ac8001b560aed8"},{"_id":"public/fancybox/fancybox_sprite.png","modified":1466268982454,"shasum":"17df19f97628e77be09c352bf27425faea248251"},{"_id":"public/fancybox/fancybox_overlay.png","modified":1466268982456,"shasum":"b3a4ee645ba494f52840ef8412015ba0f465dbe0"},{"_id":"public/fancybox/fancybox_loading@2x.gif","modified":1466268982459,"shasum":"273b123496a42ba45c3416adb027cd99745058b0"},{"_id":"public/fancybox/fancybox_loading.gif","modified":1466268982461,"shasum":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c"},{"_id":"public/fancybox/blank.gif","modified":1466268982463,"shasum":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a"},{"_id":"public/css/style.css","modified":1466268982982,"shasum":"df9188017cc02f2f3eab54caf4ca3f185f756efd"},{"_id":"public/css/font/fontawesome-webfont.woff","modified":1466268983128,"shasum":"0612cddf2f835cceffccc88fd194f97367d0b024"},{"_id":"public/css/font/fontawesome-webfont.ttf","modified":1466268983130,"shasum":"a9468f6a1fe965fbcaf5a1bd6c11705e2fc5f84c"},{"_id":"public/css/font/fontawesome-webfont.svg","modified":1466268983132,"shasum":"d162419c91b8bab3a4fd327c933a0fcf3799c251"},{"_id":"public/css/font/fontawesome-webfont.eot","modified":1466268983134,"shasum":"d775f599ff3f23be082e6a9604b4898718923a37"},{"_id":"public/google6c2fe14874348911.html","modified":1466268983136,"shasum":"12e5058baee67bcef931133e10cf113179e7d1eb"},{"_id":"public/2016/06/18/160619-vagrant-virtual-machine/index.html","modified":1466302073539,"shasum":"282f1b6c29afb95e598c817a1253a30a02fe896a"},{"_id":"public/2016/06/12/160612-hadoop-cluster-docker-update/index.html","modified":1466268983182,"shasum":"a39260bf449a7c4adef15da1beee8a1c93d76840"},{"_id":"public/2016/06/05/160605-compile-hadoop-docker/index.html","modified":1466268983196,"shasum":"8a35e739fbb1a89b77654ce4f2f6450514e5f7b2"},{"_id":"public/2016/05/29/160529-compile-hadoop-ubuntu/index.html","modified":1466268983212,"shasum":"3430f9115aa2808f20822a7c08e117d880523a96"},{"_id":"public/2016/01/09/160109-multiple-processes--docker-container/index.html","modified":1466268983226,"shasum":"703cf58f418fb7ac6351e5b21fbd4db533c1279d"},{"_id":"public/2015/11/28/151128-single-kubernetes-docker/index.html","modified":1466268983238,"shasum":"05a3c69cedd011a4a7aacdc348adb4aba2d58018"},{"_id":"public/2015/09/18/150918-single-mesos-docker/index.html","modified":1466268983247,"shasum":"c70451474f501e65e65853211e93ed05e7ea9010"},{"_id":"public/2015/06/08/150608-hadoop-cluster-docker/index.html","modified":1466268983278,"shasum":"22d727e3045569d1f41dfff4b7bc4f2f3208a87b"},{"_id":"public/archives/index.html","modified":1466268983291,"shasum":"5a1d0a0fdc70642e947110c2eb626a5d265e0a58"},{"_id":"public/archives/2015/index.html","modified":1466268983317,"shasum":"7482493e4cd68e0410d7a44f2e88a9942d135689"},{"_id":"public/archives/2015/06/index.html","modified":1466268983329,"shasum":"0e85a289333f4d62cbaa389d9dce71f54f469da3"},{"_id":"public/archives/2015/09/index.html","modified":1466268983340,"shasum":"da9de704e8135f58dd8f6fbaefd975d5cd83ae10"},{"_id":"public/archives/2015/11/index.html","modified":1466268983349,"shasum":"4659da00c4f041fdd05280b5669ab453a8f50b82"},{"_id":"public/archives/2016/index.html","modified":1466268983360,"shasum":"afd1eec9ca4e3360de21abb05beac650565c2347"},{"_id":"public/archives/2016/01/index.html","modified":1466268983380,"shasum":"3be7f838d33ce894904ab2836332ad524fa0cd3d"},{"_id":"public/archives/2016/05/index.html","modified":1466268983389,"shasum":"9d5f8108aa0eae401f6935efec803556bd1864d7"},{"_id":"public/archives/2016/06/index.html","modified":1466268983395,"shasum":"6b93fa944dce4492e51bfa7530d1411a7a06db7e"},{"_id":"public/index.html","modified":1466302073649,"shasum":"1c11b0f36997fee7e5a82ad6ae588bc6b411a406"},{"_id":"public/tags/Vagrant/index.html","modified":1466268983431,"shasum":"8698a4a067eb1de132970d350b2fba0e85ddb67b"},{"_id":"public/tags/Docker/index.html","modified":1466268983440,"shasum":"d99c73f21b4b43a58f1f01fa71167cef03b3e6c5"},{"_id":"public/tags/Hadoop/index.html","modified":1466268983448,"shasum":"47fcf210cb4b37c566f172563274de996692ffc8"},{"_id":"public/tags/Kubernetes/index.html","modified":1466268983462,"shasum":"8ece3cf727bf3a7fe939e84a22cc0994f386a296"},{"_id":"public/tags/Mesos/index.html","modified":1466268983479,"shasum":"5520b9d189b193bb2f4aafb5ad0b772c68ed16d1"},{"_id":"public/tags/Marathon/index.html","modified":1466268983491,"shasum":"f3604f080480dfd52375c556e524be3afd6d03b1"},{"_id":"public/atom.xml","modified":1466302073682,"shasum":"a50909323145a6da034a389feb9d9b86edd07147"},{"_id":"public/sitemap.xml","modified":1466302073683,"shasum":"05ce4c1b3a402663a8126ab698c129865bc15b15"}],"Category":[],"Data":[],"Page":[{"layout":"false","_content":"google-site-verification: google6c2fe14874348911.html","source":"google6c2fe14874348911.html","raw":"layout: false\n---\ngoogle-site-verification: google6c2fe14874348911.html","date":"2015-12-09T08:20:30.000Z","updated":"2015-06-10T11:06:26.000Z","path":"google6c2fe14874348911.html","title":"","comments":1,"_id":"cipleoyjl0000pkbhszofcmyt"}],"Post":[{"title":"使用Vagrant快速创建虚拟机","date":"2016-06-18T01:00:00.000Z","_content":"\n**摘要:** 手动创建虚拟机非常不方便，重装起来也很麻烦，打包成镜像的话则不易修改，很难进行版本控制，也无法移植到云端。[Vagrant](https://www.vagrantup.com/)可以将创建虚拟机的过程代码化，有效地解决了以上所提的痛点。\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-06-18](http://kiwenlau.com/2016/06/18/160618-virtual-machine/)\n\n本文所有操作是在MacBook上进行的，Windows上的操作大部分一致，但是可能会有一些小问题。\n\n![](/image/160619/vagrant-vm.png)\n\n##一. 快速入门\n\n**1. 安装[VirtulBox](https://www.virtualbox.org/wiki/Downloads)**\n\n**2. 安装[Vagrant](https://www.vagrantup.com/downloads.html)**\n\n**3. 创建虚拟机**\n\n```\nmkdir vagrant-ubuntu\ncd vagrant-ubuntu\nvagrant box add ubuntu/trusty64\nvagrant init ubuntu/trusty64\nvagrant up --provider virtualbox\nvagrant ssh\n```\n\n- **vagrant box add:** 下载创建虚拟机所依赖的**box**\n- **vagrant init:** 生成创建虚拟机的所依赖的**Vagrantfile**\n- **vagrant up:** 创建虚拟机\n- **vagrant ssh:** SSH登陆虚拟机\n\n不妨查看一下Vagrant自动生成的**Vagrantfile**, 我删除了所有注释：\n\n```\nVagrant.configure(2) do |config|\n   config.vm.box = \"ubuntu/trusty64\"\nend\n```\n**Vagrantfile**的内容非常简单，仅定义虚拟机所依赖的**Box**为**ubuntu/trusty64**。**Box**相当于虚拟机所依赖的镜像文件。因此，这里创建的虚拟机是ubuntu trusty(14.04)。如果你需要创建其他Linux发行版例如Debian或者CentOS，可以在[这里](https://atlas.hashicorp.com/boxes/search)搜索对应的**Box**.\n\nVagrant虚拟机的默认配置:\n\n- 用户/密码: vagrant/vagrant\n- 共享目录: 主机上的vagrant-ubuntu目录与虚拟机内的/vagrant目录内容实时同步\n- 内存：512MB\n- CPU: 1\n\n默认设置并不一定满足开发需求，下一小节将介绍如何进行定义配置。\n\n##二. 自定义配置\n\n**1. 修改Vagrantfile**\n\n```\nvim Vagrantfile\n```\n\n可以通过注释理解每个自定义配置的含义。\n\n```\nVagrant.configure(2) do |config|\n\n  # 设置虚拟机的Box\n  config.vm.box = \"ubuntu/trusty64\"\n  \n  # 设置虚拟机的主机名\n  config.vm.hostname=\"ubuntu\"\n  \n  # 设置虚拟机的IP\n  config.vm.network \"private_network\", ip: \"192.168.0.2\"\n  \n  # 设置主机与虚拟机的共享目录\n  config.vm.synced_folder \"~/Desktop/share\", \"/home/vagrant/share\"\n\n  # VirtaulBox相关配置\n  config.vm.provider \"virtualbox\" do |v|\n\n      # 设置虚拟机的名称\n      v.name = \"ubuntu\"\n\n      # 设置虚拟机的内存大小\n      v.memory = 2048\n\n      # 设置虚拟机的CPU个数\n      v.cpus = 1\n  end\n  \n  # 使用shell脚本进行软件安装和配置\n  config.vm.provision \"shell\", inline: <<-SHELL\n\n     # 安装Docker 1.11.0\n     apt-get update\n     apt-get install apt-transport-https ca-certificates\n     apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\n     echo deb https://apt.dockerproject.org/repo ubuntu-trusty main > /etc/apt/sources.list.d/docker.list\n     apt-get update;\n     apt-get install -y -q docker-engine=1.11.0-0~trusty\n     usermod -aG docker vagrant\n\n  SHELL\n\nend\n```\n\n**2. 在桌面上创建share目录**\n\n主机上的share目录将与虚拟机内的/home/vagrant/share目录内容实时同步\n\n```\nmkdir ~/Desktop/share\n```\n\n**3. 创建虚拟机**\n\n```\nvagrant destroy\nvagrant up --provider virtualbox\n```\n\n**4. SSH免密码登陆**\n\n使用**vagrant ssh**命令登陆虚拟机必须切换到Vagrantfile所在的目录，而直接使用虚拟机IP登陆虚拟机则更为方便:\n\n```\nssh vagrant@192.168.0.2\n```\n\n此时SSH登陆需要输入虚拟机vagrant用户的密码，即**vagrant**\n\n将主机的公钥复制到虚拟机的authorized_keys文件中即可实现SSH无密码登陆:\n\n```\ncat $HOME/.ssh/id_rsa.pub | ssh vagrant@127.0.0.1 -p 2222 'cat >> $HOME/.ssh/authorized_keys'\n```\n\n其中，2222是主机SSH登陆虚拟机的转发端口，可以通过以下命令查看:\n\n```\nvagrant ssh-config | grep Port\n  Port 2222\n```\n\n此时SSH登陆虚拟机则不再需要输入密码。\n\n**5. 关于Provision**\n\nVagrant中有下面一段内容：\n\n```\n# 使用shell脚本进行软件安装和配置\n  config.vm.provision \"shell\", inline: <<-SHELL\n\n     # 安装Docker 1.11.0\n     apt-get update\n     apt-get install apt-transport-https ca-certificates\n     apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\n     echo deb https://apt.dockerproject.org/repo ubuntu-trusty main > /etc/apt/sources.list.d/docker.list\n     apt-get update;\n     apt-get install -y -q docker-engine=1.11.0-0~trusty\n     usermod -aG docker vagrant\n\n  SHELL\n```\n\n其实就是嵌入了一段Shell脚本进行软件的安装和配置，这里我安装了[Docker](https://www.docker.com/)，当然也可以安装所需要的软件。修改此段内容之后，重新创建虚拟机需要使用**--provision**选项。\n\n```\nvagrant halt\nvagrant up --provider virtualbox --provision\n```\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2016/06/18/160618-virtual-machine/](http://kiwenlau.com/2016/06/18/160618-virtual-machine/)\n***\n\n","source":"_posts/160619-vagrant-virtual-machine.md","raw":"title: 使用Vagrant快速创建虚拟机\n\ndate: 2016-06-18 10:00\n\ntags: [Vagrant]\n\n---\n\n**摘要:** 手动创建虚拟机非常不方便，重装起来也很麻烦，打包成镜像的话则不易修改，很难进行版本控制，也无法移植到云端。[Vagrant](https://www.vagrantup.com/)可以将创建虚拟机的过程代码化，有效地解决了以上所提的痛点。\n\n<!-- more -->\n\n- 作者: [KiwenLau](http://kiwenlau.com/)\n- 日期: [2016-06-18](http://kiwenlau.com/2016/06/18/160618-virtual-machine/)\n\n本文所有操作是在MacBook上进行的，Windows上的操作大部分一致，但是可能会有一些小问题。\n\n![](/image/160619/vagrant-vm.png)\n\n##一. 快速入门\n\n**1. 安装[VirtulBox](https://www.virtualbox.org/wiki/Downloads)**\n\n**2. 安装[Vagrant](https://www.vagrantup.com/downloads.html)**\n\n**3. 创建虚拟机**\n\n```\nmkdir vagrant-ubuntu\ncd vagrant-ubuntu\nvagrant box add ubuntu/trusty64\nvagrant init ubuntu/trusty64\nvagrant up --provider virtualbox\nvagrant ssh\n```\n\n- **vagrant box add:** 下载创建虚拟机所依赖的**box**\n- **vagrant init:** 生成创建虚拟机的所依赖的**Vagrantfile**\n- **vagrant up:** 创建虚拟机\n- **vagrant ssh:** SSH登陆虚拟机\n\n不妨查看一下Vagrant自动生成的**Vagrantfile**, 我删除了所有注释：\n\n```\nVagrant.configure(2) do |config|\n   config.vm.box = \"ubuntu/trusty64\"\nend\n```\n**Vagrantfile**的内容非常简单，仅定义虚拟机所依赖的**Box**为**ubuntu/trusty64**。**Box**相当于虚拟机所依赖的镜像文件。因此，这里创建的虚拟机是ubuntu trusty(14.04)。如果你需要创建其他Linux发行版例如Debian或者CentOS，可以在[这里](https://atlas.hashicorp.com/boxes/search)搜索对应的**Box**.\n\nVagrant虚拟机的默认配置:\n\n- 用户/密码: vagrant/vagrant\n- 共享目录: 主机上的vagrant-ubuntu目录与虚拟机内的/vagrant目录内容实时同步\n- 内存：512MB\n- CPU: 1\n\n默认设置并不一定满足开发需求，下一小节将介绍如何进行定义配置。\n\n##二. 自定义配置\n\n**1. 修改Vagrantfile**\n\n```\nvim Vagrantfile\n```\n\n可以通过注释理解每个自定义配置的含义。\n\n```\nVagrant.configure(2) do |config|\n\n  # 设置虚拟机的Box\n  config.vm.box = \"ubuntu/trusty64\"\n  \n  # 设置虚拟机的主机名\n  config.vm.hostname=\"ubuntu\"\n  \n  # 设置虚拟机的IP\n  config.vm.network \"private_network\", ip: \"192.168.0.2\"\n  \n  # 设置主机与虚拟机的共享目录\n  config.vm.synced_folder \"~/Desktop/share\", \"/home/vagrant/share\"\n\n  # VirtaulBox相关配置\n  config.vm.provider \"virtualbox\" do |v|\n\n      # 设置虚拟机的名称\n      v.name = \"ubuntu\"\n\n      # 设置虚拟机的内存大小\n      v.memory = 2048\n\n      # 设置虚拟机的CPU个数\n      v.cpus = 1\n  end\n  \n  # 使用shell脚本进行软件安装和配置\n  config.vm.provision \"shell\", inline: <<-SHELL\n\n     # 安装Docker 1.11.0\n     apt-get update\n     apt-get install apt-transport-https ca-certificates\n     apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\n     echo deb https://apt.dockerproject.org/repo ubuntu-trusty main > /etc/apt/sources.list.d/docker.list\n     apt-get update;\n     apt-get install -y -q docker-engine=1.11.0-0~trusty\n     usermod -aG docker vagrant\n\n  SHELL\n\nend\n```\n\n**2. 在桌面上创建share目录**\n\n主机上的share目录将与虚拟机内的/home/vagrant/share目录内容实时同步\n\n```\nmkdir ~/Desktop/share\n```\n\n**3. 创建虚拟机**\n\n```\nvagrant destroy\nvagrant up --provider virtualbox\n```\n\n**4. SSH免密码登陆**\n\n使用**vagrant ssh**命令登陆虚拟机必须切换到Vagrantfile所在的目录，而直接使用虚拟机IP登陆虚拟机则更为方便:\n\n```\nssh vagrant@192.168.0.2\n```\n\n此时SSH登陆需要输入虚拟机vagrant用户的密码，即**vagrant**\n\n将主机的公钥复制到虚拟机的authorized_keys文件中即可实现SSH无密码登陆:\n\n```\ncat $HOME/.ssh/id_rsa.pub | ssh vagrant@127.0.0.1 -p 2222 'cat >> $HOME/.ssh/authorized_keys'\n```\n\n其中，2222是主机SSH登陆虚拟机的转发端口，可以通过以下命令查看:\n\n```\nvagrant ssh-config | grep Port\n  Port 2222\n```\n\n此时SSH登陆虚拟机则不再需要输入密码。\n\n**5. 关于Provision**\n\nVagrant中有下面一段内容：\n\n```\n# 使用shell脚本进行软件安装和配置\n  config.vm.provision \"shell\", inline: <<-SHELL\n\n     # 安装Docker 1.11.0\n     apt-get update\n     apt-get install apt-transport-https ca-certificates\n     apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\n     echo deb https://apt.dockerproject.org/repo ubuntu-trusty main > /etc/apt/sources.list.d/docker.list\n     apt-get update;\n     apt-get install -y -q docker-engine=1.11.0-0~trusty\n     usermod -aG docker vagrant\n\n  SHELL\n```\n\n其实就是嵌入了一段Shell脚本进行软件的安装和配置，这里我安装了[Docker](https://www.docker.com/)，当然也可以安装所需要的软件。修改此段内容之后，重新创建虚拟机需要使用**--provision**选项。\n\n```\nvagrant halt\nvagrant up --provider virtualbox --provision\n```\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2016/06/18/160618-virtual-machine/](http://kiwenlau.com/2016/06/18/160618-virtual-machine/)\n***\n\n","slug":"160619-vagrant-virtual-machine","published":1,"updated":"2016-06-19T02:52:19.000Z","_id":"cipleoyjq0001pkbhugjilnrv","comments":1,"layout":"post","photos":[],"link":""},{"title":"基于Docker搭建Hadoop集群之升级版","date":"2016-06-12T01:00:00.000Z","_content":"\n**摘要:** [kiwenlau/hadoop-cluster-docker](https://github.com/kiwenlau/hadoop-cluster-docker)是去年参加[Docker巨好玩](http://www.alauda.cn/2015/04/28/docker-great/)比赛开发的，得了[二等奖](http://www.alauda.cn/2015/06/11/presentation/)并赢了一块苹果手表，目前这个项目已经在GitHub上获得了236个Star，DockerHub的镜像下载次数2000+。总之，项目还算很受欢迎吧，这篇博客将介绍项目的升级版。\n\n<!-- more -->\n\n##一. 项目介绍\n\n将[Hadoop](http://hadoop.apache.org/)打包到[Docker](https://www.docker.com/)镜像中，就可以快速地在单个机器上搭建Hadoop集群，这样可以方便新手测试和学习。\n\n如下图所示，Hadoop的master和slave分别运行在不同的Docker容器中，其中hadoop-master容器中运行NameNode和ResourceManager，hadoop-slave容器中运行DataNode和NodeManager。NameNode和DataNode是Hadoop分布式文件系统HDFS的组件，负责储存输入以及输出数据，而ResourceManager和NodeManager是Hadoop集群资源管理系统YARN的组件，负责CPU和内存资源的调度。\n\n![](/image/160612/hadoop-cluster-docker.png)\n\n之前的版本使用serf/dnsmasq为Hadoop集群提供DNS服务，由于Docker网络功能更新，现在并不需要了。更新的版本中，使用以下命令为Hadoop集群创建单独的网络:\n\n```\nsudo docker network create --driver=bridge hadoop\n```\n\n然后在运行Hadoop容器时，使用\"--net=hadoop\"选项，这时所有容器将运行在hadoop网络中，它们可以通过容器名称进行通信。\n\n**项目更新要点：**\n\n- 去除serf/dnsmasq\n- 合并Master和Slave镜像\n- 使用[kiwenlau/compile-hadoop](https://github.com/kiwenlau/compile-hadoop)项目编译的Hadoo进行安装\n- 优化Hadoop配置\n\n##二. 3节点Hadoop集群搭建步骤\n\n###1. 下载Docker镜像\n\n```\nsudo docker pull kiwenlau/hadoop:1.0\n```\n\n###2. 下载GitHub仓库\n\n```\ngit clone https://github.com/kiwenlau/hadoop-cluster-docker\n```\n\n###3. 创建Hadoop网络\n\n```\nsudo docker network create --driver=bridge hadoop\n```\n\n###4. 运行Docker容器\n\n```\ncd hadoop-cluster-docker\n./start-container.sh\n```\n\n**运行结果**\n\n```\nstart hadoop-master container...\nstart hadoop-slave1 container...\nstart hadoop-slave2 container...\nroot@hadoop-master:~# \n```\n\n- 启动了3个容器，1个master, 2个slave\n- 运行后就进入了hadoop-master容器的/root目录\n\n###5. 启动hadoop\n\n```\n./start-hadoop.sh\n```\n\n###6. 运行wordcount\n\n```\n./run-wordcount.sh\n```\n\n**运行结果**\n\n```\ninput file1.txt:\nHello Hadoop\ninput file2.txt:\nHello Docker\nwordcount output:\nDocker\t1\nHadoop\t1\nHello\t2\n```\n\nHadoop网页管理地址:\n\n- NameNode: [http://192.168.59.1:50070/](http://192.168.59.1:50070/)\n- ResourceManager: [http://192.168.59.1:8088/](http://192.168.59.1:8088/)\n\n192.168.59.1为运行容器的主机的IP。\n\n##三. N节点Hadoop集群搭建步骤\n\n###1. 准备\n\n- 参考第二部分1~3：下载Docker镜像，下载GitHub仓库，以及创建Hadoop网络\n\n###2. 重新构建Docker镜像\n\n```\n./resize-cluster.sh 5\n```\n- 可以指定任意N(N>1)\n\n###3. 启动Docker容器\n\n```\n./start-container.sh 5\n```\n- 与第2步中的N保持一致。\n\n###4. 运行Hadoop\n\n- 参考第二部分5~6：启动Hadoop，并运行wordcount。\n\n##参考\n\n1. [基于Docker搭建多节点Hadoop集群](http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/)\n2. [How to Install Hadoop on Ubuntu 13.10](https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-on-ubuntu-13-10)\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2016/06/12/160612-hadoop-cluster-docker-update/](http://kiwenlau.com/2016/06/12/160612-hadoop-cluster-docker-update/)\n***\n\n","source":"_posts/160612-hadoop-cluster-docker-update.md","raw":"title: 基于Docker搭建Hadoop集群之升级版\n\ndate: 2016-06-12 10:00\n\ntags: [Docker, Hadoop]\n\n---\n\n**摘要:** [kiwenlau/hadoop-cluster-docker](https://github.com/kiwenlau/hadoop-cluster-docker)是去年参加[Docker巨好玩](http://www.alauda.cn/2015/04/28/docker-great/)比赛开发的，得了[二等奖](http://www.alauda.cn/2015/06/11/presentation/)并赢了一块苹果手表，目前这个项目已经在GitHub上获得了236个Star，DockerHub的镜像下载次数2000+。总之，项目还算很受欢迎吧，这篇博客将介绍项目的升级版。\n\n<!-- more -->\n\n##一. 项目介绍\n\n将[Hadoop](http://hadoop.apache.org/)打包到[Docker](https://www.docker.com/)镜像中，就可以快速地在单个机器上搭建Hadoop集群，这样可以方便新手测试和学习。\n\n如下图所示，Hadoop的master和slave分别运行在不同的Docker容器中，其中hadoop-master容器中运行NameNode和ResourceManager，hadoop-slave容器中运行DataNode和NodeManager。NameNode和DataNode是Hadoop分布式文件系统HDFS的组件，负责储存输入以及输出数据，而ResourceManager和NodeManager是Hadoop集群资源管理系统YARN的组件，负责CPU和内存资源的调度。\n\n![](/image/160612/hadoop-cluster-docker.png)\n\n之前的版本使用serf/dnsmasq为Hadoop集群提供DNS服务，由于Docker网络功能更新，现在并不需要了。更新的版本中，使用以下命令为Hadoop集群创建单独的网络:\n\n```\nsudo docker network create --driver=bridge hadoop\n```\n\n然后在运行Hadoop容器时，使用\"--net=hadoop\"选项，这时所有容器将运行在hadoop网络中，它们可以通过容器名称进行通信。\n\n**项目更新要点：**\n\n- 去除serf/dnsmasq\n- 合并Master和Slave镜像\n- 使用[kiwenlau/compile-hadoop](https://github.com/kiwenlau/compile-hadoop)项目编译的Hadoo进行安装\n- 优化Hadoop配置\n\n##二. 3节点Hadoop集群搭建步骤\n\n###1. 下载Docker镜像\n\n```\nsudo docker pull kiwenlau/hadoop:1.0\n```\n\n###2. 下载GitHub仓库\n\n```\ngit clone https://github.com/kiwenlau/hadoop-cluster-docker\n```\n\n###3. 创建Hadoop网络\n\n```\nsudo docker network create --driver=bridge hadoop\n```\n\n###4. 运行Docker容器\n\n```\ncd hadoop-cluster-docker\n./start-container.sh\n```\n\n**运行结果**\n\n```\nstart hadoop-master container...\nstart hadoop-slave1 container...\nstart hadoop-slave2 container...\nroot@hadoop-master:~# \n```\n\n- 启动了3个容器，1个master, 2个slave\n- 运行后就进入了hadoop-master容器的/root目录\n\n###5. 启动hadoop\n\n```\n./start-hadoop.sh\n```\n\n###6. 运行wordcount\n\n```\n./run-wordcount.sh\n```\n\n**运行结果**\n\n```\ninput file1.txt:\nHello Hadoop\ninput file2.txt:\nHello Docker\nwordcount output:\nDocker\t1\nHadoop\t1\nHello\t2\n```\n\nHadoop网页管理地址:\n\n- NameNode: [http://192.168.59.1:50070/](http://192.168.59.1:50070/)\n- ResourceManager: [http://192.168.59.1:8088/](http://192.168.59.1:8088/)\n\n192.168.59.1为运行容器的主机的IP。\n\n##三. N节点Hadoop集群搭建步骤\n\n###1. 准备\n\n- 参考第二部分1~3：下载Docker镜像，下载GitHub仓库，以及创建Hadoop网络\n\n###2. 重新构建Docker镜像\n\n```\n./resize-cluster.sh 5\n```\n- 可以指定任意N(N>1)\n\n###3. 启动Docker容器\n\n```\n./start-container.sh 5\n```\n- 与第2步中的N保持一致。\n\n###4. 运行Hadoop\n\n- 参考第二部分5~6：启动Hadoop，并运行wordcount。\n\n##参考\n\n1. [基于Docker搭建多节点Hadoop集群](http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/)\n2. [How to Install Hadoop on Ubuntu 13.10](https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-on-ubuntu-13-10)\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2016/06/12/160612-hadoop-cluster-docker-update/](http://kiwenlau.com/2016/06/12/160612-hadoop-cluster-docker-update/)\n***\n\n","slug":"160612-hadoop-cluster-docker-update","published":1,"updated":"2016-06-13T15:46:53.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipleoyjx0004pkbhdmq59qlx"},{"title":"基于Docker编译Hadoop","date":"2016-06-05T00:00:00.000Z","_content":"\n**摘要:** 将编译Hadoop所需要的依赖软件安装到Docker镜像中，然后在Docker容器中编译Hadoop，可以提高编译效率，同时避免污染主机。编译其他软件时，也可以参考这篇博客的方法。\n\n**GitHub地址:**\n- [kiwenlau/compile-hadoop](https://github.com/kiwenlau/compile-hadoop)\n\n<!-- more -->\n\n![](/image/160605/hadoop-docker.png)\n\n在前一篇博客中，我介绍了[64位Ubuntu中编译Hadoop的步骤](http://kiwenlau.com/2016/05/29/160529-compile-hadoop-ubuntu/)。这篇博客将介绍基于Docker编译Hadoop的方法。\n\n###一. 编译步骤\n\n**1. 下载Docker镜像**\n\n```\nsudo docker pull kiwenlau/compile-hadoop\n```\n\n或者自行构建Docker镜像\n\n```\nsudo docker build -t kiwenlau/compile-hadoop .\n```\n\n\n**2. 下载并解压[Hadoop源文件](http://archive.apache.org/dist/hadoop/core/)**\n\n```\nexport VERSION=2.7.2\nwget http://archive.apache.org/dist/hadoop/core/hadoop-$VERSION/hadoop-$VERSION-src.tar.gz\ntar -xzvf hadoop-$VERSION-src.tar.gz\n```\n\n**3. 运行Docker容器，在容器中编译Hadoop**\n\n```\nsudo docker run -v $(pwd)/hadoop-$VERSION-src:/hadoop-$VERSION-src kiwenlau/compile-hadoop /root/compile.sh $VERSION\n```\n\n这一步比较耗时，大概需要15~30分钟。 \n\n正确执行的结果如下:\n\n```\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 23:46.056s\n[INFO] Finished at: Tue May 31 16:40:53 UTC 2016\n[INFO] Final Memory: 210M/915M\n[INFO] ------------------------------------------------------------------------\n\n\ncomile hadoop 2.7.2 success!\n```\n\n编译好的二进制文件包位于\n\n```\nhadoop-2.7.2-src/hadoop-dist/target/hadoop-2.7.2.tar.gz\n```\n\n编译其他版本的Hadoop的步骤一致，仅需改变VERSION的值。\n\n可以使用wget命令直接下载GitHub上的Hadoop二进制包:\n\n```\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/$VERSION/hadoop-VERSION.tar.gz\n```\n\n###二. 方法总结\n\n编译其他软件时，也可以参考本文介绍的方法，具体细节可以参考源码[kiwenlau/compile-hadoop](https://github.com/kiwenlau/compile-hadoop)\n\n**1. 构建编译所需的Docker镜像**\n\n编译软件往往需要安装很多依赖，而编译不同的软件有时需要不同版本的依赖，如果直接在主机上安装这些依赖会污染主机，而且也不易重复。\n\n**2. 下载软件源码**\n\n源码不放在Docker镜像里面，可以方便编译不同版本的软件，也可以提高构建Docker镜像的效率。\n\n**3. 运行Docker容器编译软件**\n\n软件源码以数据卷(volume)的形式挂载的容器内，编译所得的可执行文件也将位于数据卷内。\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2016/06/05/160605-compile-hadoop-docker/](http://kiwenlau.com/2016/06/05/160605-compile-hadoop-docker/)\n***","source":"_posts/160605-compile-hadoop-docker.md","raw":"title: 基于Docker编译Hadoop\n\ndate: 2016-06-05 09:00\n\ntags: [Docker, Hadoop]\n\n---\n\n**摘要:** 将编译Hadoop所需要的依赖软件安装到Docker镜像中，然后在Docker容器中编译Hadoop，可以提高编译效率，同时避免污染主机。编译其他软件时，也可以参考这篇博客的方法。\n\n**GitHub地址:**\n- [kiwenlau/compile-hadoop](https://github.com/kiwenlau/compile-hadoop)\n\n<!-- more -->\n\n![](/image/160605/hadoop-docker.png)\n\n在前一篇博客中，我介绍了[64位Ubuntu中编译Hadoop的步骤](http://kiwenlau.com/2016/05/29/160529-compile-hadoop-ubuntu/)。这篇博客将介绍基于Docker编译Hadoop的方法。\n\n###一. 编译步骤\n\n**1. 下载Docker镜像**\n\n```\nsudo docker pull kiwenlau/compile-hadoop\n```\n\n或者自行构建Docker镜像\n\n```\nsudo docker build -t kiwenlau/compile-hadoop .\n```\n\n\n**2. 下载并解压[Hadoop源文件](http://archive.apache.org/dist/hadoop/core/)**\n\n```\nexport VERSION=2.7.2\nwget http://archive.apache.org/dist/hadoop/core/hadoop-$VERSION/hadoop-$VERSION-src.tar.gz\ntar -xzvf hadoop-$VERSION-src.tar.gz\n```\n\n**3. 运行Docker容器，在容器中编译Hadoop**\n\n```\nsudo docker run -v $(pwd)/hadoop-$VERSION-src:/hadoop-$VERSION-src kiwenlau/compile-hadoop /root/compile.sh $VERSION\n```\n\n这一步比较耗时，大概需要15~30分钟。 \n\n正确执行的结果如下:\n\n```\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 23:46.056s\n[INFO] Finished at: Tue May 31 16:40:53 UTC 2016\n[INFO] Final Memory: 210M/915M\n[INFO] ------------------------------------------------------------------------\n\n\ncomile hadoop 2.7.2 success!\n```\n\n编译好的二进制文件包位于\n\n```\nhadoop-2.7.2-src/hadoop-dist/target/hadoop-2.7.2.tar.gz\n```\n\n编译其他版本的Hadoop的步骤一致，仅需改变VERSION的值。\n\n可以使用wget命令直接下载GitHub上的Hadoop二进制包:\n\n```\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/$VERSION/hadoop-VERSION.tar.gz\n```\n\n###二. 方法总结\n\n编译其他软件时，也可以参考本文介绍的方法，具体细节可以参考源码[kiwenlau/compile-hadoop](https://github.com/kiwenlau/compile-hadoop)\n\n**1. 构建编译所需的Docker镜像**\n\n编译软件往往需要安装很多依赖，而编译不同的软件有时需要不同版本的依赖，如果直接在主机上安装这些依赖会污染主机，而且也不易重复。\n\n**2. 下载软件源码**\n\n源码不放在Docker镜像里面，可以方便编译不同版本的软件，也可以提高构建Docker镜像的效率。\n\n**3. 运行Docker容器编译软件**\n\n软件源码以数据卷(volume)的形式挂载的容器内，编译所得的可执行文件也将位于数据卷内。\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2016/06/05/160605-compile-hadoop-docker/](http://kiwenlau.com/2016/06/05/160605-compile-hadoop-docker/)\n***","slug":"160605-compile-hadoop-docker","published":1,"updated":"2016-06-12T14:37:59.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipleoyk00009pkbhp7i6a68p"},{"title":"64位Ubuntu中编译Hadoop的步骤","date":"2016-05-29T00:00:00.000Z","_content":"\n**摘要:** 本文介绍了在64位Ubuntu 14.04中编译Hadoop的步骤。\n\nHadoop二进制包下载地址：[百度网盘](https://pan.baidu.com/s/1hrGLqlA) [GitHub](https://github.com/kiwenlau/compile-hadoop/releases)\n\n<!-- more -->\n\n![](/image/160529/ubuntu-hadoop.png)\n\nHadoop官网提供的二进制包是在32位系统上编译的，在64系统上运行会出错：\n\n```\nWARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n```\n\n这时需要自行编译Hadoop源码。以下为编译步骤:\n\n**1. 安装依赖软件**\n\n```\nsudo apt-get update\nsudo apt-get install -y openjdk-7-jdk libprotobuf-dev protobuf-compiler maven cmake build-essential pkg-config libssl-dev zlib1g-dev llvm-gcc automake autoconf make\n```\n\n**2. 下载并解压[Hadoop源文件](http://archive.apache.org/dist/hadoop/core/)**\n\n```\nwget http://archive.apache.org/dist/hadoop/core/hadoop-2.3.0/hadoop-2.3.0-src.tar.gz\ntar -xzvf hadoop-2.3.0-src.tar.gz\n```\n\n**3. 编译Hadoop**\n\n```\ncd hadoop-2.3.0-src\nmvn package -Pdist,native -DskipTests –Dtar\n```\n\n这一步比较耗时，大概需要15~30分钟。 \n\n正确执行的结果如下:\n\n```\n[INFO] ------------------------------------------------------------------------\n\n[INFO] BUILD SUCCESS\n\n[INFO] ------------------------------------------------------------------------\n\n[INFO] Total time: 14:59.240s\n\n[INFO] Finished at: Thu Jan 15 18:51:59 JST 2015\n\n[INFO] Final Memory: 168M/435M\n\n[INFO] ------------------------------------------------------------------------\n```\n\n编译好的二进制文件包位于\n\n```\nhadoop-2.3.0-src/hadoop-dist/target/hadoop-2.3.0.tar.gz\n```\n\n编译其他版本的Hadoop的步骤一致，也可以直接下载我编译好的Hadoop:\n\n- [百度网盘](https://pan.baidu.com/s/1hrGLqlA)\n- [GitHub](https://github.com/kiwenlau/compile-hadoop/releases)\n\nLinux终端用户可以使用wget命令直接下载GitHub上的Hadoop二进制包:\n\n```\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.3.0/hadoop-2.3.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.4.0/hadoop-2.4.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.5.0/hadoop-2.5.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.6.0/hadoop-2.6.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.7.0/hadoop-2.7.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.7.2/hadoop-2.7.2.tar.gz\n```\n\n另外, 使用自行编译的Hadoop二进制包安装Hadoop时需要删除.bashrc文件与hadoop-env.sh文件中下面两行（默认不会有这两行，但是尝试解决报错时可能改写了）\n\n```\nexport HADOOP_COMMON_LIB_NATIVE_DIR=\"~/hadoop/lib/\"\nexport HADOOP_OPTS=\"$HADOOP_OPTS -Djava.library.path=~/hadoop/lib/\"\n```\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2016/05/29/160529-compile-hadoop-ubuntu/](http://kiwenlau.com/2016/05/29/160529-compile-hadoop-ubuntu/)\n***","source":"_posts/160529-compile-hadoop-ubuntu.md","raw":"title: 64位Ubuntu中编译Hadoop的步骤\n\ndate: 2016-05-29 09:00\n\ntags: [Hadoop]\n\n---\n\n**摘要:** 本文介绍了在64位Ubuntu 14.04中编译Hadoop的步骤。\n\nHadoop二进制包下载地址：[百度网盘](https://pan.baidu.com/s/1hrGLqlA) [GitHub](https://github.com/kiwenlau/compile-hadoop/releases)\n\n<!-- more -->\n\n![](/image/160529/ubuntu-hadoop.png)\n\nHadoop官网提供的二进制包是在32位系统上编译的，在64系统上运行会出错：\n\n```\nWARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n```\n\n这时需要自行编译Hadoop源码。以下为编译步骤:\n\n**1. 安装依赖软件**\n\n```\nsudo apt-get update\nsudo apt-get install -y openjdk-7-jdk libprotobuf-dev protobuf-compiler maven cmake build-essential pkg-config libssl-dev zlib1g-dev llvm-gcc automake autoconf make\n```\n\n**2. 下载并解压[Hadoop源文件](http://archive.apache.org/dist/hadoop/core/)**\n\n```\nwget http://archive.apache.org/dist/hadoop/core/hadoop-2.3.0/hadoop-2.3.0-src.tar.gz\ntar -xzvf hadoop-2.3.0-src.tar.gz\n```\n\n**3. 编译Hadoop**\n\n```\ncd hadoop-2.3.0-src\nmvn package -Pdist,native -DskipTests –Dtar\n```\n\n这一步比较耗时，大概需要15~30分钟。 \n\n正确执行的结果如下:\n\n```\n[INFO] ------------------------------------------------------------------------\n\n[INFO] BUILD SUCCESS\n\n[INFO] ------------------------------------------------------------------------\n\n[INFO] Total time: 14:59.240s\n\n[INFO] Finished at: Thu Jan 15 18:51:59 JST 2015\n\n[INFO] Final Memory: 168M/435M\n\n[INFO] ------------------------------------------------------------------------\n```\n\n编译好的二进制文件包位于\n\n```\nhadoop-2.3.0-src/hadoop-dist/target/hadoop-2.3.0.tar.gz\n```\n\n编译其他版本的Hadoop的步骤一致，也可以直接下载我编译好的Hadoop:\n\n- [百度网盘](https://pan.baidu.com/s/1hrGLqlA)\n- [GitHub](https://github.com/kiwenlau/compile-hadoop/releases)\n\nLinux终端用户可以使用wget命令直接下载GitHub上的Hadoop二进制包:\n\n```\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.3.0/hadoop-2.3.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.4.0/hadoop-2.4.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.5.0/hadoop-2.5.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.6.0/hadoop-2.6.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.7.0/hadoop-2.7.0.tar.gz\nwget https://github.com/kiwenlau/compile-hadoop/releases/download/2.7.2/hadoop-2.7.2.tar.gz\n```\n\n另外, 使用自行编译的Hadoop二进制包安装Hadoop时需要删除.bashrc文件与hadoop-env.sh文件中下面两行（默认不会有这两行，但是尝试解决报错时可能改写了）\n\n```\nexport HADOOP_COMMON_LIB_NATIVE_DIR=\"~/hadoop/lib/\"\nexport HADOOP_OPTS=\"$HADOOP_OPTS -Djava.library.path=~/hadoop/lib/\"\n```\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2016/05/29/160529-compile-hadoop-ubuntu/](http://kiwenlau.com/2016/05/29/160529-compile-hadoop-ubuntu/)\n***","slug":"160529-compile-hadoop-ubuntu","published":1,"updated":"2016-05-29T05:57:23.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipleoyk9000cpkbhqvrhyio1"},{"title":"如何运行多进程Docker容器？","date":"2016-01-09T12:00:00.000Z","_content":"\n**摘要:** 本文介绍了两种在Docker容器中运行多个进程的方法: **shell脚本**和**supervisor**。\n\n**GitHub地址:**\n- [kiwenlau/kubernetes-shell](https://github.com/kiwenlau/kubernetes-shell)\n- [kiwenlau/kubernetes-supervisor](https://github.com/kiwenlau/kubernetes-supervisor)\n\n<!-- more -->\n\n\n##简介\n\n一般来说，Docker容器比较适合运行单个进程。例如，项目\"**使用多个Docker容器运行Kubernetes**\"，Kubernetes的各个组件分别运行在各个容器之中，每个容器只运行单个进程。\n\n然而，很多时候我们需要在Docker容器中运行多个进程。例如，项目\"**使用单个Docker容器运行Kubernetes**\"，kubernetes的各个组件均运行在同一个容器中，该容器中运行了多个进程。那么，**如何运行多进程Docker容器？**\n\n一种方法是使用**Shell脚本**，另一种方法是使用进程管理工具[Supervisor](http://supervisord.org/)。[kiwenlau/kubernetes-shell](https://github.com/kiwenlau/kubernetes-shell)和[kiwenlau/kubernetes-supervisor](https://github.com/kiwenlau/kubernetes-supervisor)分别采用了这两种方法，用于启动多个进程来运行Kubernetes的各个组件，从而实现\"**使用单个Docker容器运行Kubernetes**\"。下面我将分别介绍两种不同方法。\n\n##使用Shell脚本运行多进程Docker容器\n\n这个方法大家应该会比较熟悉，使用Shell脚本依次启动Kubernetes的各个组件即可。以下为**start-kubernetes.sh**\n\n```\n#!/bin/bash\n\n# start docker daemon\ndocker daemon > /var/log/docker.log 2>&1 &\n\n# start etcd\netcd --data-dir=/var/etcd/data > /var/log/etcd.log 2>&1 &\n\n# wait for ectd to setup\nsleep 5\n\n# start apiserver\nkube-apiserver --service-cluster-ip-range=10.0.0.1/24 --insecure-bind-address=0.0.0.0 --etcd_servers=http://127.0.0.1:4001 > /var/log/kube-apiserver.log 2>&1 &\n\n# wait for apiserver to setup\nsleep 5\n\n# start controller manager, sheduler, kubelet and proxy\nkube-controller-manager --master=http://0.0.0.0:8080 > /var/log/kube-controller-manager.log 2>&1 &\nkube-scheduler --master=http://0.0.0.0:8080 > /var/log/kube-scheduler.log 2>&1 &\nkubelet --api_servers=http://0.0.0.0:8080 --address=0.0.0.0 --cluster_dns=10.0.0.10 --cluster_domain=\"kubernetes.local\" --pod-infra-container-image=\"kiwenlau/pause:0.8.0\"  > /var/log/kubelet.log 2>&1 &\nkube-proxy --master=http://0.0.0.0:8080 > /var/log/kube-proxy.log 2>&1 &\n\n# just keep this script running\nwhile [[ true ]]; do\n\tsleep 1\ndone\n```\n\n然后在Dockerfile中，将**start-kubernetes.sh**指定为Docker容器默认执行的命令即可:\n\n```\nCMD [\"start-kubernetes.sh\"]\n```\n\n**需要注意**的一点在于，**start-kubernetes.sh**脚本将作为Docker容器的1号进程运行，必须始终保持运行。因为**Docker容器仅在1号进程运行时保持运行**，换言之，Docker容器将在1号进程退出后**Exited**。由于Kubernetes的各个组件都以后台进程方式执行，我在脚本末尾添加了死循环，以保持**start-kubernetes.sh**脚本始终处于运行状态。\n\n```\n# just keep this script running\nwhile [[ true ]]; do\n\tsleep 1\ndone\n```\n\n##使用supervisor运行多进程Docker容器\n\n[Supervisor](http://supervisord.org/)是进程管理工具。这时，需要编写supervisor的配置文件**kubernetes.conf**:\n\n```\n[supervisord]\nnodaemon=true\n\n[program:etcd]\ncommand=etcd --data-dir=/var/etcd/data\nautorestart=true\nstdout_logfile=/var/log/etcd.stdout.log\nstderr_logfile=/var/log/etcd.stderr.log\n\n[program:kube-apiserver]\ncommand=kube-apiserver --service-cluster-ip-range=10.0.0.1/24 --insecure-bind-address=0.0.0.0 --etcd_servers=http://127.0.0.1:4001\nautorestart=true\nstdout_logfile=/var/log/kube-apiserver.stdout.log\nstderr_logfile=/var/log/kube-apiserver.stderr.log\n\n[program:kube-controller-manager]\ncommand=kube-controller-manager --master=http://0.0.0.0:8080\nautorestart=true\nstdout_logfile=/var/log/controller-manager.stdout.log\nstderr_logfile=/var/log/controller-manager.stderr.log\n\n[program:kube-scheduler]\ncommand=kube-scheduler --master=http://0.0.0.0:8080\nautorestart=true\nstdout_logfile=/var/log/kube-scheduler.stdout.log\nstderr_logfile=/var/log/kube-scheduler.stderr.log\n\n[program:kubelet]\ncommand=kubelet --api_servers=http://0.0.0.0:8080 --address=0.0.0.0 --cluster_dns=10.0.0.10 --cluster_domain=\"kubernetes.local\" --pod-infra-container-image=\"kiwenlau/pause:0.8.0\"\nautorestart=true\nstdout_logfile=/var/log/kubelet.stdout.log\nstderr_logfile=/var/log/kubelet.stderr.log\n\n[program:kube-proxy]\ncommand=kube-proxy --master=http://0.0.0.0:8080\nautorestart=true\nstdout_logfile=/var/log/kube-proxy.stdout.log\nstderr_logfile=/var/log/kube-proxy.stderr.log\n\n[program:docker]\ncommand=docker daemon\nautorestart=true\nstdout_logfile=/var/log/docker.stdout.log\nstderr_logfile=/var/log/docker.stderr.log\n```\n\n可知，将Kubernetes的各个组件的启动命令设为command即可。autorestart参数设为true，意味着supervisor将负责重启意外退出的组件。stdout_logfile和stderr_logfile参数则可以用于设置命令的标准输出文件和标准错误输出文件。\n\n然后在Dockerfile中，将**supervisord**指定为Docker容器默认执行的命令即可:\n\n```\nCMD [\"supervisord\", \"-c\", \"/etc/supervisor/conf.d/kubernetes.conf\"]\n```\n\n此时, supervisord是Docker容器中的1号进程，也需要始终保持运行状态。nodaemon设为true时，表示supervisor保持前台运行而非在后台运行。若supervisor在后台运行，则Docker容器也会在执行supervisord命令后立即Exited.\n\n```\n[supervisord]\nnodaemon=true\n```\n\n\n\n##总结\n\n使用Shell脚本运行多进程Docker容器，优势是大家比较熟悉。由于需要保持Docker容器的1号进程始终运行，这一点比较容易出错。若要实现进程意外退出后自动重启的话，使用shell脚本比较麻烦。\n\n使用supervisor运行多进程Docker容器，非常方便。另外，保持1号进程保持运行，以及进程意外退出后自动重启，实现起来都很简单。\n\n\n##使用多个Docker容器运行Kubernetes\n\n**GitHub地址**\n\n- [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n下图显示了我在Ubuntu主机上运行单机版Kubernetes的架构。可知，我一共运行了7个容器，分别运行Kubernetes的各个组件。\n\n![](/image/160109/single-kubernetes-docker.png)\n\n##使用单个Docker容器运行Kubernetes\n\n**GitHub地址:**\n\n- [kiwenlau/kubernetes-shell](https://github.com/kiwenlau/kubernetes-shell)\n- [kiwenlau/kubernetes-supervisor](https://github.com/kiwenlau/kubernetes-supervisor)\n\n该项目中，我将kubernetes的所有组件：etcd, controller manager, apiserver, scheduler, kubelet, proxy以及docker daemon均运行在同一个Docker容器之中。\n\n容器启动时，各个组件由shell脚本或者supervisor启动。\n\n![](/image/160109/kubernetes-shell-supervisor.png)\n\n##参考\n1. [Using Supervisor with Docker](https://docs.docker.com/engine/articles/using_supervisord/)\n2. [How To Install and Manage Supervisor on Ubuntu and Debian VPS](https://www.digitalocean.com/community/tutorials/how-to-install-and-manage-supervisor-on-ubuntu-and-debian-vps)\n3. [基于Docker搭建单机版Kuberntes](http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/)\n4. [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2016/01/09/160109-multiple-processes--docker-container/](http://kiwenlau.com/2016/01/09/160109-multiple-processes--docker-container/)\n***\n\n\n\n\n\n\n\n\n","source":"_posts/160109-multiple-processes--docker-container.md","raw":"title: 如何运行多进程Docker容器？\n\ndate: 2016-01-09 21:00:00\n\ntags: [Docker, Kubernetes]\n\n---\n\n**摘要:** 本文介绍了两种在Docker容器中运行多个进程的方法: **shell脚本**和**supervisor**。\n\n**GitHub地址:**\n- [kiwenlau/kubernetes-shell](https://github.com/kiwenlau/kubernetes-shell)\n- [kiwenlau/kubernetes-supervisor](https://github.com/kiwenlau/kubernetes-supervisor)\n\n<!-- more -->\n\n\n##简介\n\n一般来说，Docker容器比较适合运行单个进程。例如，项目\"**使用多个Docker容器运行Kubernetes**\"，Kubernetes的各个组件分别运行在各个容器之中，每个容器只运行单个进程。\n\n然而，很多时候我们需要在Docker容器中运行多个进程。例如，项目\"**使用单个Docker容器运行Kubernetes**\"，kubernetes的各个组件均运行在同一个容器中，该容器中运行了多个进程。那么，**如何运行多进程Docker容器？**\n\n一种方法是使用**Shell脚本**，另一种方法是使用进程管理工具[Supervisor](http://supervisord.org/)。[kiwenlau/kubernetes-shell](https://github.com/kiwenlau/kubernetes-shell)和[kiwenlau/kubernetes-supervisor](https://github.com/kiwenlau/kubernetes-supervisor)分别采用了这两种方法，用于启动多个进程来运行Kubernetes的各个组件，从而实现\"**使用单个Docker容器运行Kubernetes**\"。下面我将分别介绍两种不同方法。\n\n##使用Shell脚本运行多进程Docker容器\n\n这个方法大家应该会比较熟悉，使用Shell脚本依次启动Kubernetes的各个组件即可。以下为**start-kubernetes.sh**\n\n```\n#!/bin/bash\n\n# start docker daemon\ndocker daemon > /var/log/docker.log 2>&1 &\n\n# start etcd\netcd --data-dir=/var/etcd/data > /var/log/etcd.log 2>&1 &\n\n# wait for ectd to setup\nsleep 5\n\n# start apiserver\nkube-apiserver --service-cluster-ip-range=10.0.0.1/24 --insecure-bind-address=0.0.0.0 --etcd_servers=http://127.0.0.1:4001 > /var/log/kube-apiserver.log 2>&1 &\n\n# wait for apiserver to setup\nsleep 5\n\n# start controller manager, sheduler, kubelet and proxy\nkube-controller-manager --master=http://0.0.0.0:8080 > /var/log/kube-controller-manager.log 2>&1 &\nkube-scheduler --master=http://0.0.0.0:8080 > /var/log/kube-scheduler.log 2>&1 &\nkubelet --api_servers=http://0.0.0.0:8080 --address=0.0.0.0 --cluster_dns=10.0.0.10 --cluster_domain=\"kubernetes.local\" --pod-infra-container-image=\"kiwenlau/pause:0.8.0\"  > /var/log/kubelet.log 2>&1 &\nkube-proxy --master=http://0.0.0.0:8080 > /var/log/kube-proxy.log 2>&1 &\n\n# just keep this script running\nwhile [[ true ]]; do\n\tsleep 1\ndone\n```\n\n然后在Dockerfile中，将**start-kubernetes.sh**指定为Docker容器默认执行的命令即可:\n\n```\nCMD [\"start-kubernetes.sh\"]\n```\n\n**需要注意**的一点在于，**start-kubernetes.sh**脚本将作为Docker容器的1号进程运行，必须始终保持运行。因为**Docker容器仅在1号进程运行时保持运行**，换言之，Docker容器将在1号进程退出后**Exited**。由于Kubernetes的各个组件都以后台进程方式执行，我在脚本末尾添加了死循环，以保持**start-kubernetes.sh**脚本始终处于运行状态。\n\n```\n# just keep this script running\nwhile [[ true ]]; do\n\tsleep 1\ndone\n```\n\n##使用supervisor运行多进程Docker容器\n\n[Supervisor](http://supervisord.org/)是进程管理工具。这时，需要编写supervisor的配置文件**kubernetes.conf**:\n\n```\n[supervisord]\nnodaemon=true\n\n[program:etcd]\ncommand=etcd --data-dir=/var/etcd/data\nautorestart=true\nstdout_logfile=/var/log/etcd.stdout.log\nstderr_logfile=/var/log/etcd.stderr.log\n\n[program:kube-apiserver]\ncommand=kube-apiserver --service-cluster-ip-range=10.0.0.1/24 --insecure-bind-address=0.0.0.0 --etcd_servers=http://127.0.0.1:4001\nautorestart=true\nstdout_logfile=/var/log/kube-apiserver.stdout.log\nstderr_logfile=/var/log/kube-apiserver.stderr.log\n\n[program:kube-controller-manager]\ncommand=kube-controller-manager --master=http://0.0.0.0:8080\nautorestart=true\nstdout_logfile=/var/log/controller-manager.stdout.log\nstderr_logfile=/var/log/controller-manager.stderr.log\n\n[program:kube-scheduler]\ncommand=kube-scheduler --master=http://0.0.0.0:8080\nautorestart=true\nstdout_logfile=/var/log/kube-scheduler.stdout.log\nstderr_logfile=/var/log/kube-scheduler.stderr.log\n\n[program:kubelet]\ncommand=kubelet --api_servers=http://0.0.0.0:8080 --address=0.0.0.0 --cluster_dns=10.0.0.10 --cluster_domain=\"kubernetes.local\" --pod-infra-container-image=\"kiwenlau/pause:0.8.0\"\nautorestart=true\nstdout_logfile=/var/log/kubelet.stdout.log\nstderr_logfile=/var/log/kubelet.stderr.log\n\n[program:kube-proxy]\ncommand=kube-proxy --master=http://0.0.0.0:8080\nautorestart=true\nstdout_logfile=/var/log/kube-proxy.stdout.log\nstderr_logfile=/var/log/kube-proxy.stderr.log\n\n[program:docker]\ncommand=docker daemon\nautorestart=true\nstdout_logfile=/var/log/docker.stdout.log\nstderr_logfile=/var/log/docker.stderr.log\n```\n\n可知，将Kubernetes的各个组件的启动命令设为command即可。autorestart参数设为true，意味着supervisor将负责重启意外退出的组件。stdout_logfile和stderr_logfile参数则可以用于设置命令的标准输出文件和标准错误输出文件。\n\n然后在Dockerfile中，将**supervisord**指定为Docker容器默认执行的命令即可:\n\n```\nCMD [\"supervisord\", \"-c\", \"/etc/supervisor/conf.d/kubernetes.conf\"]\n```\n\n此时, supervisord是Docker容器中的1号进程，也需要始终保持运行状态。nodaemon设为true时，表示supervisor保持前台运行而非在后台运行。若supervisor在后台运行，则Docker容器也会在执行supervisord命令后立即Exited.\n\n```\n[supervisord]\nnodaemon=true\n```\n\n\n\n##总结\n\n使用Shell脚本运行多进程Docker容器，优势是大家比较熟悉。由于需要保持Docker容器的1号进程始终运行，这一点比较容易出错。若要实现进程意外退出后自动重启的话，使用shell脚本比较麻烦。\n\n使用supervisor运行多进程Docker容器，非常方便。另外，保持1号进程保持运行，以及进程意外退出后自动重启，实现起来都很简单。\n\n\n##使用多个Docker容器运行Kubernetes\n\n**GitHub地址**\n\n- [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n下图显示了我在Ubuntu主机上运行单机版Kubernetes的架构。可知，我一共运行了7个容器，分别运行Kubernetes的各个组件。\n\n![](/image/160109/single-kubernetes-docker.png)\n\n##使用单个Docker容器运行Kubernetes\n\n**GitHub地址:**\n\n- [kiwenlau/kubernetes-shell](https://github.com/kiwenlau/kubernetes-shell)\n- [kiwenlau/kubernetes-supervisor](https://github.com/kiwenlau/kubernetes-supervisor)\n\n该项目中，我将kubernetes的所有组件：etcd, controller manager, apiserver, scheduler, kubelet, proxy以及docker daemon均运行在同一个Docker容器之中。\n\n容器启动时，各个组件由shell脚本或者supervisor启动。\n\n![](/image/160109/kubernetes-shell-supervisor.png)\n\n##参考\n1. [Using Supervisor with Docker](https://docs.docker.com/engine/articles/using_supervisord/)\n2. [How To Install and Manage Supervisor on Ubuntu and Debian VPS](https://www.digitalocean.com/community/tutorials/how-to-install-and-manage-supervisor-on-ubuntu-and-debian-vps)\n3. [基于Docker搭建单机版Kuberntes](http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/)\n4. [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2016/01/09/160109-multiple-processes--docker-container/](http://kiwenlau.com/2016/01/09/160109-multiple-processes--docker-container/)\n***\n\n\n\n\n\n\n\n\n","slug":"160109-multiple-processes--docker-container","published":1,"updated":"2016-05-17T14:50:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipleoykc000epkbhfhry929z"},{"title":"基于Docker搭建单机版Kuberntes","date":"2015-11-28T00:00:00.000Z","_content":"\n**摘要:** 本文介绍了基于Docker搭建单机版Kuberntes的方法，Kubernetes的所有组件均运行于单个容器中。\n\n**GitHub地址:**\n- [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n<!-- more -->\n\n##1. Kubernetes简介\n\n2006年，Google工程师Rohit Seth发起了Cgroups内核项目。Cgroups是容器实现CPU，内存等资源隔离的基础，由此可见Google其实很早就开始涉足容器技术。而事实上，Google内部使用容器技术已经长达十年，目前谷歌所有业务包括搜索，Gmail，MapReduce等均运行在容器之中。Google内部使用的集群管理系统--Borg，堪称其容器技术的瑞士军刀。\n\n2014年，Google发起了开源容器集群管理系统--Kubernetes，其设计之初就吸取了Borg的经验和教训，并原生支持了Docker。因此，Kubernetees与较早的集群管理系统Mesos和YARN相比，对容器技术尤其是Docker的支持更加原生，同时提供了更强大的机制实现资源调度，负载均衡，高可用等底层功能，使开发者可以专注于开发应用。\n\n与其他集群系统一致，Kubernetes也采用了Master/Slave结构。下表显示了Kubernetes的各个组件及其功能。\n\n| 角色     | 组件               | 功能                                           |\n| ------- |:-----------------: | :--------------------------------------------:|\n| Master  | apiserver          | 提供RESTful接口                                |\n| Master  | scheduler          | 负责调度，将pod分配到Slave节点                   |\n| Master  | controller-manager | 负责Master的其他功能                           |\n| Master  | etde               | 储存配置信息，节点信息，pod信息等                 |\n| Slave   | kubelet            | 负责管理Pod、容器和容器镜像                       |\n| Slave   | proxy              | 将访问Service的请求转发给对应的Pod，做一些负载均衡  |\n| 客户端   | kubectl            | 命令行工具，向apiserver发起创建Pod等请求          |\n\n\n##2. kiwenlau/kubernetes镜像简介\n\n下图显示了我在Ubuntu主机上运行单机版Kubernetes的架构。可知，我一共运行了7个容器，分别运行Kubernetes的各个组件。事实上，Kuberenetes未来的开发目标正是将Kubernetes的各个组件运行到容器之中，这样可以方便Kubernetes的部署和升级。现在我将Kubernetes的各个组件全部运行在容器中必然存在很多问题且很多问题是未知的，因此这个项目仅做学习测试而不宜部署到生产环境中。Kubernetes各个组件容器之间的通信通过docker link实现，其中apiserver与ectd的4001端口进行通信，scheduler，controller-manager，kubelet，proxy以及kubectl与apiserver的8080端口进行通信。\n\n![](/image/151128/single-kubernetes-docker.png)\n\n集群的大致运行流程是这样的: 用户通过kubectl命令向apiserver发起创建Pod的请求; scheduler将创建Pod的任务分配给kubelet；kubelet中包含了一个docker命令行工具，该工具会向Docker deamon发起创建容器的请求; Docker deamon负责下载镜像然后创建容器。\n\n我将Docker deamon运行在Ubuntu主机上，因此Docker daemon所创建的应用容器与Kubernetes各个组件的容器均运行在Ubuntu主机上。docker socket采用volume的形式挂载到kubelet容器内，因此kubelet中的docker命令行工具可以直接与主机上的Docker daemon进行通信。\n\n我是直接将kubernetes发布的各个组件的二进制可执行文件安装在/usr/local/bin目录下，因此，修改Dockerfile中的Kubernetes下载链接的版本号，就可以快速构建其他版本的Kubernetes镜像。另外，仅需修改网络配置，就可以很方便地在多个节点上部署Kubernetes。\n\nkiwenlau/kubernetes:1.0.7镜像版本信息:\n\n- ubuntu: 14.04\n- Kubernetes: 1.0.7\n- ectd: 2.2.1\n\nUbuntu主机版本信息:\n\n- ubuntu: 14.04.3 LTS\n- kernel: 3.16.0-30-generic\n- docker: 1.9.1\n\n\n\n##3. 运行步骤\n\n**1. 安装Docker**\n\nubuntu 14.04上安装Docker: \n\n```\ncurl -fLsS https://get.docker.com/ | sh\n```\n\n其他系统请参考: [https://docs.docker.com/](https://docs.docker.com/)\n\n**2. 下载Docker镜像**\n\n我将kiwenlau/kubernetes:1.07以及其他用到的Docker镜像都放在[灵雀云](http://www.alauda.cn/)\n\n```\nsudo docker pull index.alauda.cn/kiwenlau/kubernetes:1.0.7\nsudo docker pull index.alauda.cn/kiwenlau/etcd:v2.2.1\nsudo docker pull index.alauda.cn/kiwenlau/nginx:1.9.7\nsudo docker pull index.alauda.cn/kiwenlau/pause:0.8.0\n```\n\n**3. 启动Kubernetes**\n\n```sh\ngit clone https://github.com/kiwenlau/single-kubernetes-docker\ncd single-kubernetes-docker/\nsudo chmod +x start-kubernetes-alauda.sh stop-kubernetes.sh\nsudo ./start-kubernetes-alauda.sh\n```\n\n运行结束后进入kubectl容器。容器主机名为kubeclt。可以通过\"exit\"命令退出容器返回到主机，然后可以通过\"sudo docker exec -it kubectl bash\"命令再次进入kubectl容器。\n\n\n**4. 测试Kubernetes**\n\n运行测试脚本，该脚本会启动一个nginx pod。\n\n```\nchmod +x test-kubernetes-alauda.sh\n./test-kubernetes-alauda.sh \n```\n\n输出\n\n```\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n<style>\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n</style>\n</head>\n<body>\n<h1>Welcome to nginx!</h1>\n<p>If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.</p>\n\n<p>For online documentation and support please refer to\n<a href=\"http://nginx.org/\">nginx.org</a>.<br/>\nCommercial support is available at\n<a href=\"http://nginx.com/\">nginx.com</a>.</p>\n\n<p><em>Thank you for using nginx.</em></p>\n</body>\n</html>\n```\n\n\n##3. 参考\n1. [meteorhacks/hyperkube](https://github.com/meteorhacks/hyperkube)\n2. [meteorhacks/kube-init](https://github.com/meteorhacks/kube-init)\n3. [Kubernetes: The Future of Cloud Hosting](https://meteorhacks.com/learn-kubernetes-the-future-of-the-cloud)\n4. [Kubernetes 架构浅析](http://weibo.com/p/1001603912843031387951?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io)\n5. [An Introduction to Kubernetes](https://www.digitalocean.com/community/tutorials/an-introduction-to-kubernetes)\n\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/](http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/)\n***\n\n\n\n\n\n\n\n\n","source":"_posts/151128-single-kubernetes-docker.md","raw":"title: 基于Docker搭建单机版Kuberntes\n\ndate: 2015-11-28 09:00:00\n\ntags: [Docker, Kubernetes]\n\n---\n\n**摘要:** 本文介绍了基于Docker搭建单机版Kuberntes的方法，Kubernetes的所有组件均运行于单个容器中。\n\n**GitHub地址:**\n- [kiwenlau/single-kubernetes-docker](https://github.com/kiwenlau/single-kubernetes-docker)\n\n<!-- more -->\n\n##1. Kubernetes简介\n\n2006年，Google工程师Rohit Seth发起了Cgroups内核项目。Cgroups是容器实现CPU，内存等资源隔离的基础，由此可见Google其实很早就开始涉足容器技术。而事实上，Google内部使用容器技术已经长达十年，目前谷歌所有业务包括搜索，Gmail，MapReduce等均运行在容器之中。Google内部使用的集群管理系统--Borg，堪称其容器技术的瑞士军刀。\n\n2014年，Google发起了开源容器集群管理系统--Kubernetes，其设计之初就吸取了Borg的经验和教训，并原生支持了Docker。因此，Kubernetees与较早的集群管理系统Mesos和YARN相比，对容器技术尤其是Docker的支持更加原生，同时提供了更强大的机制实现资源调度，负载均衡，高可用等底层功能，使开发者可以专注于开发应用。\n\n与其他集群系统一致，Kubernetes也采用了Master/Slave结构。下表显示了Kubernetes的各个组件及其功能。\n\n| 角色     | 组件               | 功能                                           |\n| ------- |:-----------------: | :--------------------------------------------:|\n| Master  | apiserver          | 提供RESTful接口                                |\n| Master  | scheduler          | 负责调度，将pod分配到Slave节点                   |\n| Master  | controller-manager | 负责Master的其他功能                           |\n| Master  | etde               | 储存配置信息，节点信息，pod信息等                 |\n| Slave   | kubelet            | 负责管理Pod、容器和容器镜像                       |\n| Slave   | proxy              | 将访问Service的请求转发给对应的Pod，做一些负载均衡  |\n| 客户端   | kubectl            | 命令行工具，向apiserver发起创建Pod等请求          |\n\n\n##2. kiwenlau/kubernetes镜像简介\n\n下图显示了我在Ubuntu主机上运行单机版Kubernetes的架构。可知，我一共运行了7个容器，分别运行Kubernetes的各个组件。事实上，Kuberenetes未来的开发目标正是将Kubernetes的各个组件运行到容器之中，这样可以方便Kubernetes的部署和升级。现在我将Kubernetes的各个组件全部运行在容器中必然存在很多问题且很多问题是未知的，因此这个项目仅做学习测试而不宜部署到生产环境中。Kubernetes各个组件容器之间的通信通过docker link实现，其中apiserver与ectd的4001端口进行通信，scheduler，controller-manager，kubelet，proxy以及kubectl与apiserver的8080端口进行通信。\n\n![](/image/151128/single-kubernetes-docker.png)\n\n集群的大致运行流程是这样的: 用户通过kubectl命令向apiserver发起创建Pod的请求; scheduler将创建Pod的任务分配给kubelet；kubelet中包含了一个docker命令行工具，该工具会向Docker deamon发起创建容器的请求; Docker deamon负责下载镜像然后创建容器。\n\n我将Docker deamon运行在Ubuntu主机上，因此Docker daemon所创建的应用容器与Kubernetes各个组件的容器均运行在Ubuntu主机上。docker socket采用volume的形式挂载到kubelet容器内，因此kubelet中的docker命令行工具可以直接与主机上的Docker daemon进行通信。\n\n我是直接将kubernetes发布的各个组件的二进制可执行文件安装在/usr/local/bin目录下，因此，修改Dockerfile中的Kubernetes下载链接的版本号，就可以快速构建其他版本的Kubernetes镜像。另外，仅需修改网络配置，就可以很方便地在多个节点上部署Kubernetes。\n\nkiwenlau/kubernetes:1.0.7镜像版本信息:\n\n- ubuntu: 14.04\n- Kubernetes: 1.0.7\n- ectd: 2.2.1\n\nUbuntu主机版本信息:\n\n- ubuntu: 14.04.3 LTS\n- kernel: 3.16.0-30-generic\n- docker: 1.9.1\n\n\n\n##3. 运行步骤\n\n**1. 安装Docker**\n\nubuntu 14.04上安装Docker: \n\n```\ncurl -fLsS https://get.docker.com/ | sh\n```\n\n其他系统请参考: [https://docs.docker.com/](https://docs.docker.com/)\n\n**2. 下载Docker镜像**\n\n我将kiwenlau/kubernetes:1.07以及其他用到的Docker镜像都放在[灵雀云](http://www.alauda.cn/)\n\n```\nsudo docker pull index.alauda.cn/kiwenlau/kubernetes:1.0.7\nsudo docker pull index.alauda.cn/kiwenlau/etcd:v2.2.1\nsudo docker pull index.alauda.cn/kiwenlau/nginx:1.9.7\nsudo docker pull index.alauda.cn/kiwenlau/pause:0.8.0\n```\n\n**3. 启动Kubernetes**\n\n```sh\ngit clone https://github.com/kiwenlau/single-kubernetes-docker\ncd single-kubernetes-docker/\nsudo chmod +x start-kubernetes-alauda.sh stop-kubernetes.sh\nsudo ./start-kubernetes-alauda.sh\n```\n\n运行结束后进入kubectl容器。容器主机名为kubeclt。可以通过\"exit\"命令退出容器返回到主机，然后可以通过\"sudo docker exec -it kubectl bash\"命令再次进入kubectl容器。\n\n\n**4. 测试Kubernetes**\n\n运行测试脚本，该脚本会启动一个nginx pod。\n\n```\nchmod +x test-kubernetes-alauda.sh\n./test-kubernetes-alauda.sh \n```\n\n输出\n\n```\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n<style>\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n</style>\n</head>\n<body>\n<h1>Welcome to nginx!</h1>\n<p>If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.</p>\n\n<p>For online documentation and support please refer to\n<a href=\"http://nginx.org/\">nginx.org</a>.<br/>\nCommercial support is available at\n<a href=\"http://nginx.com/\">nginx.com</a>.</p>\n\n<p><em>Thank you for using nginx.</em></p>\n</body>\n</html>\n```\n\n\n##3. 参考\n1. [meteorhacks/hyperkube](https://github.com/meteorhacks/hyperkube)\n2. [meteorhacks/kube-init](https://github.com/meteorhacks/kube-init)\n3. [Kubernetes: The Future of Cloud Hosting](https://meteorhacks.com/learn-kubernetes-the-future-of-the-cloud)\n4. [Kubernetes 架构浅析](http://weibo.com/p/1001603912843031387951?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io)\n5. [An Introduction to Kubernetes](https://www.digitalocean.com/community/tutorials/an-introduction-to-kubernetes)\n\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/](http://kiwenlau.com/2015/11/28/151128-single-kubernetes-docker/)\n***\n\n\n\n\n\n\n\n\n","slug":"151128-single-kubernetes-docker","published":1,"updated":"2016-05-17T15:18:58.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipleoyke000ipkbhti3cuger"},{"title":"基于Docker搭建单机版Mesos/Marathon","date":"2015-09-18T03:00:00.000Z","_content":"\n**摘要:** 本文介绍了基于Docker搭建单机版Mesos/Marathon的方法，Mesos/Marathon的所有组件均运行于单个容器中。\n\n**GitHub地址:**\n- [kiwenlau/single-mesos-docker](https://github.com/kiwenlau/single-mesos-docker)\n\n<!-- more -->\n\n##一. 简介\n\n[Mesos](http://mesos.apache.org)是集群资源管理系统，[Marathon](http://mesosphere.github.io/marathon)是运行在Mesos之上的集群计算架构。将Mesos和Marathon打包到[Docker](https://www.docker.com/)镜像中，开发者便可以在本机上快速搭建Mesos/Marathon集群，进行学习和测试。\n\n**kiwenlau/single-mesos**镜像非常简单。Docker容器运行在Ubuntu主机之上，Mesos和Marathon运行在该容器之中。具体来讲，Docker容器中运行了一个Mesos Master和一个Mesos Slave，以及Marathon和[ZooKeeper](https://zookeeper.apache.org/)。集群架构如下图：\n\n![](/image/150918/architecuture.png)\n\n\n##二. 搭建Mesos/Marathon集群\n\n**1. 下载Docker镜像:**\n\n```sh\nsudo docker pull kiwenlau/single-mesos:3.0\n```\n\n**2. 运行Docker容器:**\n\n```sh\nsudo docker run -p 5050:5050 -p 8080:8080 --name mesos -it -w /root kiwenlau/single-mesos:3.0\n```\n\ndocker run命令运行成功后即进入容器内部，以下为输出：\n\n```bash\nStart ZooKeeper...\nStart Mesos master...\nStart Mesos slave...\nStart Marathon...\n```\n\n\n##三. 测试Mesos/Marathon集群\n\n**1. 通过curl命令调用Marathon的REST API, 创建一个hello程序：**\n\n```sh\ncurl -v -H \"Content-Type: application/json\" -X POST --data \"@hello.json\" http://127.0.0.1:8080/v2/apps\n```\n\n下面为hello.json。由cmd可知，该程序每隔1秒往output.txt文件中写入hello。\n\n```bash\n{\n  \"id\": \"hello\",\n  \"cmd\": \"while [ true ] ; do echo hello >> /root/output.txt; sleep 1; done\",\n  \"cpus\": 0.1,\n  \"mem\": 10.0,\n  \"instances\": 1\n}\n```\n\ncurl执行结果:\n\n```bash\n* Hostname was NOT found in DNS cache\n*   Trying 127.0.0.1...\n* Connected to 127.0.0.1 (127.0.0.1) port 8080 (#0)\n> POST /v2/apps HTTP/1.1\n> User-Agent: curl/7.35.0\n> Host: 127.0.0.1:8080\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 139\n> \n* upload completely sent off: 139 out of 139 bytes\n< HTTP/1.1 201 Created\n< X-Marathon-Leader: http://ec054cabb9af:8080\n< Cache-Control: no-cache, no-store, must-revalidate\n< Pragma: no-cache\n< Expires: 0\n< Location: http://127.0.0.1:8080/v2/apps/hello\n< Content-Type: application/json; qs=2\n< Transfer-Encoding: chunked\n* Server Jetty(8.y.z-SNAPSHOT) is not blacklisted\n< Server: Jetty(8.y.z-SNAPSHOT)\n< \n* Connection #0 to host 127.0.0.1 left intact\n{\"id\":\"/hello\",\"cmd\":\"while [ true ] ; do echo hello >> /root/output.txt; sleep 1; done\",\"args\":null,\"user\":null,\"env\":{},\"instances\":1,\"cpus\":0.1,\"mem\":10.0,\"disk\":0.0,\"executor\":\"\",\"constraints\":[],\"uris\":[],\"storeUrls\":[],\"ports\":[0],\"requirePorts\":false,\"backoffFactor\":1.15,\"container\":null,\"healthChecks\":[],\"dependencies\":[],\"upgradeStrategy\":{\"minimumHealthCapacity\":1.0,\"maximumOverCapacity\":1.0},\"labels\":{},\"acceptedResourceRoles\":null,\"version\":\"2015-09-16T11:22:27.967Z\",\"deployments\":[{\"id\":\"2cd2fdd4-e5f9-4088-895f-7976349b7a19\"}],\"tasks\":[],\"tasksStaged\":0,\"tasksRunning\":0,\"tasksHealthy\":0,\"tasksUnhealthy\":0,\"backoffSeconds\":1,\"maxLaunchDelaySeconds\":3600}\n```\n\n**2. 查看hello程序的运行结果：**\n\n```sh\ntail -f output.txt\n```\n当你看到终端不断输出\"hello\"时说明运行成功。\n\n**3. 使用浏览器查看Mesos和Marathon的网页管理界面**\n\n**注意**将IP替换运行Docker容器的主机IP地址\n\nMesos网页管理界面地址：[http://192.168.59.10:5050](http://192.168.59.10:5050)\n\nMesos网页管理界面如图，可知hello程序正在运行：\n\n![](/image/150918/Mesos.png)\n\nMarathon网页管理界面地址：[http://192.168.59.10:8080](http://192.168.59.10:8080)\n\nMarathon网页管理界面如图，可知hello程序正在运行：\n\n![](/image/150918/Marathon.png)\n\n**4. 通过Marathon网页管理界面创建测试程序**\n\n在Marathon的网页管理界面上点击\"New APP\"，在弹框中配置测试程序。ID为\"hello\", Command为\"echo hello >> /root/output.txt\", 然后点击\"Create\"即可。如下图：\n\n![](/image/150918/hello.png)\n\n\n##四. 存在的问题\n\n其实，参考[Setting up a Single Node Mesosphere Cluster](https://open.mesosphere.com/getting-started/developer/single-node-install/)，可以很快地在ubuntu主机上直接搭建一个单节点的Mesos/Marathon集群。但是，当我安装该教程的步骤将Mesos/Marathon集群打包到Docker镜像中时，遇到了一个比较奇怪的问题。\n\n在Docker容器中使用**\"sudo service mesos-master start\"**和**\"sudo service mesos-slave start\"**命令启动Mesos Master和Mesos Slave时，出现**\"mesos-master: unrecognized service\"**和**\"mesos-slave: unrecognized service\"**错误。但是，我在ubuntu主机上安装Mesos/Marathon集群后，使用同样的命令启动Mesos并没有问题。后来，我是通过直接执行mesos-master和mesos-slave命令启动Mesos，命令如下：\n\n```sh\n/usr/sbin/mesos-master --zk=zk://127.0.0.1:2181/mesos --quorum=1 --work_dir=/var/lib/mesos --log_dir=/log/mesos  \n```\n\n```sh\n/usr/sbin/mesos-slave --master=zk://127.0.0.1:2181/mesos --log_dir=/log/mesos\n```\n\n由这个问题可知，虽然在Docker容器几乎可以运行任意程序，似乎和Ubuntu主机没有区别。但是事实上，**Docker容器与ubuntu主机并非完全一致**，而且这些细节的不同点比较坑。这一点很值得探讨，可以让大家在使用Docker时少走些弯路。对于提到的问题，虽然是解决了，然而我仍然不清楚其中的原因:(\n\n\n##五. Docker镜像备份\n\n我将Docker镜像上传到了灵雀云（Alaudo）的Docker仓库，可以通过以下命令下载和运行：\n\n```sh\nsudo docker pull index.alauda.cn/kiwenlau/single-mesos:3.0\n```\n\n```sh\nsudo docker run -p 5050:5050 -p 8080:8080 --name mesos -it -w /root index.alauda.cn/kiwenlau/single-mesos:3.0\n```\n\n##六. 参考\n\n1. [Setting up a Single Node Mesosphere Cluster](https://open.mesosphere.com/getting-started/developer/single-node-install/)\n2. [Setting up a Cluster on Mesos and Marathon](https://open.mesosphere.com/getting-started/datacenter/install/#master-setup)\n3. [An Introduction to Mesosphere](https://www.digitalocean.com/community/tutorials/an-introduction-to-mesosphere)\n4. [How To Configure a Production-Ready Mesosphere Cluster on Ubuntu 14.04](https://www.digitalocean.com/community/tutorials/how-to-configure-a-production-ready-mesosphere-cluster-on-ubuntu-14-04)\n5. [Deploy a Mesos Cluster with 7 Commands Using Docker](https://medium.com/@gargar454/deploy-a-mesos-cluster-with-7-commands-using-docker-57951e020586)\n6. [sekka1/mesosphere-docker](https://github.com/sekka1/mesosphere-docker)\n7. [Marathon: Application Basics](http://mesosphere.github.io/marathon/docs/application-basics.html)\n8. [Marathon: REST API](http://mesosphere.github.io/marathon/docs/rest-api.html)\n\n***\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文URL地址：\n\n[http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/](http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/)\n***\n\n\n\n\n\n","source":"_posts/150918-single-mesos-docker.md","raw":"title: 基于Docker搭建单机版Mesos/Marathon\n\ndate: 2015-09-18 12:00:00\n\ntags: [Docker,Mesos,Marathon]\n\n---\n\n**摘要:** 本文介绍了基于Docker搭建单机版Mesos/Marathon的方法，Mesos/Marathon的所有组件均运行于单个容器中。\n\n**GitHub地址:**\n- [kiwenlau/single-mesos-docker](https://github.com/kiwenlau/single-mesos-docker)\n\n<!-- more -->\n\n##一. 简介\n\n[Mesos](http://mesos.apache.org)是集群资源管理系统，[Marathon](http://mesosphere.github.io/marathon)是运行在Mesos之上的集群计算架构。将Mesos和Marathon打包到[Docker](https://www.docker.com/)镜像中，开发者便可以在本机上快速搭建Mesos/Marathon集群，进行学习和测试。\n\n**kiwenlau/single-mesos**镜像非常简单。Docker容器运行在Ubuntu主机之上，Mesos和Marathon运行在该容器之中。具体来讲，Docker容器中运行了一个Mesos Master和一个Mesos Slave，以及Marathon和[ZooKeeper](https://zookeeper.apache.org/)。集群架构如下图：\n\n![](/image/150918/architecuture.png)\n\n\n##二. 搭建Mesos/Marathon集群\n\n**1. 下载Docker镜像:**\n\n```sh\nsudo docker pull kiwenlau/single-mesos:3.0\n```\n\n**2. 运行Docker容器:**\n\n```sh\nsudo docker run -p 5050:5050 -p 8080:8080 --name mesos -it -w /root kiwenlau/single-mesos:3.0\n```\n\ndocker run命令运行成功后即进入容器内部，以下为输出：\n\n```bash\nStart ZooKeeper...\nStart Mesos master...\nStart Mesos slave...\nStart Marathon...\n```\n\n\n##三. 测试Mesos/Marathon集群\n\n**1. 通过curl命令调用Marathon的REST API, 创建一个hello程序：**\n\n```sh\ncurl -v -H \"Content-Type: application/json\" -X POST --data \"@hello.json\" http://127.0.0.1:8080/v2/apps\n```\n\n下面为hello.json。由cmd可知，该程序每隔1秒往output.txt文件中写入hello。\n\n```bash\n{\n  \"id\": \"hello\",\n  \"cmd\": \"while [ true ] ; do echo hello >> /root/output.txt; sleep 1; done\",\n  \"cpus\": 0.1,\n  \"mem\": 10.0,\n  \"instances\": 1\n}\n```\n\ncurl执行结果:\n\n```bash\n* Hostname was NOT found in DNS cache\n*   Trying 127.0.0.1...\n* Connected to 127.0.0.1 (127.0.0.1) port 8080 (#0)\n> POST /v2/apps HTTP/1.1\n> User-Agent: curl/7.35.0\n> Host: 127.0.0.1:8080\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 139\n> \n* upload completely sent off: 139 out of 139 bytes\n< HTTP/1.1 201 Created\n< X-Marathon-Leader: http://ec054cabb9af:8080\n< Cache-Control: no-cache, no-store, must-revalidate\n< Pragma: no-cache\n< Expires: 0\n< Location: http://127.0.0.1:8080/v2/apps/hello\n< Content-Type: application/json; qs=2\n< Transfer-Encoding: chunked\n* Server Jetty(8.y.z-SNAPSHOT) is not blacklisted\n< Server: Jetty(8.y.z-SNAPSHOT)\n< \n* Connection #0 to host 127.0.0.1 left intact\n{\"id\":\"/hello\",\"cmd\":\"while [ true ] ; do echo hello >> /root/output.txt; sleep 1; done\",\"args\":null,\"user\":null,\"env\":{},\"instances\":1,\"cpus\":0.1,\"mem\":10.0,\"disk\":0.0,\"executor\":\"\",\"constraints\":[],\"uris\":[],\"storeUrls\":[],\"ports\":[0],\"requirePorts\":false,\"backoffFactor\":1.15,\"container\":null,\"healthChecks\":[],\"dependencies\":[],\"upgradeStrategy\":{\"minimumHealthCapacity\":1.0,\"maximumOverCapacity\":1.0},\"labels\":{},\"acceptedResourceRoles\":null,\"version\":\"2015-09-16T11:22:27.967Z\",\"deployments\":[{\"id\":\"2cd2fdd4-e5f9-4088-895f-7976349b7a19\"}],\"tasks\":[],\"tasksStaged\":0,\"tasksRunning\":0,\"tasksHealthy\":0,\"tasksUnhealthy\":0,\"backoffSeconds\":1,\"maxLaunchDelaySeconds\":3600}\n```\n\n**2. 查看hello程序的运行结果：**\n\n```sh\ntail -f output.txt\n```\n当你看到终端不断输出\"hello\"时说明运行成功。\n\n**3. 使用浏览器查看Mesos和Marathon的网页管理界面**\n\n**注意**将IP替换运行Docker容器的主机IP地址\n\nMesos网页管理界面地址：[http://192.168.59.10:5050](http://192.168.59.10:5050)\n\nMesos网页管理界面如图，可知hello程序正在运行：\n\n![](/image/150918/Mesos.png)\n\nMarathon网页管理界面地址：[http://192.168.59.10:8080](http://192.168.59.10:8080)\n\nMarathon网页管理界面如图，可知hello程序正在运行：\n\n![](/image/150918/Marathon.png)\n\n**4. 通过Marathon网页管理界面创建测试程序**\n\n在Marathon的网页管理界面上点击\"New APP\"，在弹框中配置测试程序。ID为\"hello\", Command为\"echo hello >> /root/output.txt\", 然后点击\"Create\"即可。如下图：\n\n![](/image/150918/hello.png)\n\n\n##四. 存在的问题\n\n其实，参考[Setting up a Single Node Mesosphere Cluster](https://open.mesosphere.com/getting-started/developer/single-node-install/)，可以很快地在ubuntu主机上直接搭建一个单节点的Mesos/Marathon集群。但是，当我安装该教程的步骤将Mesos/Marathon集群打包到Docker镜像中时，遇到了一个比较奇怪的问题。\n\n在Docker容器中使用**\"sudo service mesos-master start\"**和**\"sudo service mesos-slave start\"**命令启动Mesos Master和Mesos Slave时，出现**\"mesos-master: unrecognized service\"**和**\"mesos-slave: unrecognized service\"**错误。但是，我在ubuntu主机上安装Mesos/Marathon集群后，使用同样的命令启动Mesos并没有问题。后来，我是通过直接执行mesos-master和mesos-slave命令启动Mesos，命令如下：\n\n```sh\n/usr/sbin/mesos-master --zk=zk://127.0.0.1:2181/mesos --quorum=1 --work_dir=/var/lib/mesos --log_dir=/log/mesos  \n```\n\n```sh\n/usr/sbin/mesos-slave --master=zk://127.0.0.1:2181/mesos --log_dir=/log/mesos\n```\n\n由这个问题可知，虽然在Docker容器几乎可以运行任意程序，似乎和Ubuntu主机没有区别。但是事实上，**Docker容器与ubuntu主机并非完全一致**，而且这些细节的不同点比较坑。这一点很值得探讨，可以让大家在使用Docker时少走些弯路。对于提到的问题，虽然是解决了，然而我仍然不清楚其中的原因:(\n\n\n##五. Docker镜像备份\n\n我将Docker镜像上传到了灵雀云（Alaudo）的Docker仓库，可以通过以下命令下载和运行：\n\n```sh\nsudo docker pull index.alauda.cn/kiwenlau/single-mesos:3.0\n```\n\n```sh\nsudo docker run -p 5050:5050 -p 8080:8080 --name mesos -it -w /root index.alauda.cn/kiwenlau/single-mesos:3.0\n```\n\n##六. 参考\n\n1. [Setting up a Single Node Mesosphere Cluster](https://open.mesosphere.com/getting-started/developer/single-node-install/)\n2. [Setting up a Cluster on Mesos and Marathon](https://open.mesosphere.com/getting-started/datacenter/install/#master-setup)\n3. [An Introduction to Mesosphere](https://www.digitalocean.com/community/tutorials/an-introduction-to-mesosphere)\n4. [How To Configure a Production-Ready Mesosphere Cluster on Ubuntu 14.04](https://www.digitalocean.com/community/tutorials/how-to-configure-a-production-ready-mesosphere-cluster-on-ubuntu-14-04)\n5. [Deploy a Mesos Cluster with 7 Commands Using Docker](https://medium.com/@gargar454/deploy-a-mesos-cluster-with-7-commands-using-docker-57951e020586)\n6. [sekka1/mesosphere-docker](https://github.com/sekka1/mesosphere-docker)\n7. [Marathon: Application Basics](http://mesosphere.github.io/marathon/docs/application-basics.html)\n8. [Marathon: REST API](http://mesosphere.github.io/marathon/docs/rest-api.html)\n\n***\n**版权声明**\n\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文URL地址：\n\n[http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/](http://kiwenlau.com/2015/09/18/150918-single-mesos-docker/)\n***\n\n\n\n\n\n","slug":"150918-single-mesos-docker","published":1,"updated":"2016-05-17T15:18:53.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipleoykk000lpkbhibtl23j7"},{"title":"基于Docker搭建多节点Hadoop集群","date":"2015-06-08T03:44:40.000Z","_content":"\n**摘要:** 本文介绍了基于Docker在单机上搭建多节点Hadopp集群方法，Hadoop的Master和Slave分别运行在不同容器中。\n\n**GitHub地址:**\n- [kiwenlau/hadoop-cluster-docker](https://github.com/kiwenlau)\n\n<!-- more -->\n\n可以直接进入第三部分，快速在本机搭建一个3个节点的Hadoop集群\n\n\n##一. 项目简介\n\n直接用机器搭建Hadoop集群是一个相当痛苦的过程，尤其对初学者来说。他们还没开始跑wordcount，可能就被这个问题折腾的体无完肤了。\n\n我的目标是将Hadoop集群运行在Docker容器中，使Hadoop开发者能够快速便捷地在本机搭建多节点的Hadoop集群。其实这个想法已经有了不少实现，但是都不是很理想，他们或者镜像太大，或者使用太慢，或者使用了第三方工具使得使用起来过于复杂...下表为一些已知的Hadoop on Docker项目以及其存在的问题。\n\n\n|                  项目                             | 镜像大小    |        问题                        |\n| : ------------- ---------------| : ------ | :-------------------- |\n|sequenceiq/hadoop-docker:latest  |1.491GB    | 镜像太大，只支持单个节点|\n|sequenceiq/hadoop-docker:2.7.0  |1.76 GB     |           同上                           |\n|sequenceiq/hadoop-docker:2.6.0  |1.624GB    |         同上                            |\n|sequenceiq/ambari:latest               |1.782GB     |  镜像太大，使用太慢|\n|sequenceiq/ambari:2.0.0               |4.804GB     |          同上                           |\n|sequenceiq/ambari:latest:1.70      |4.761GB    |         同上                         |\n|alvinhenrick/hadoop-mutinode     |4.331GB    |镜像太大，构建太慢，增加节点麻烦，有bug\n\n我的项目参考了alvinhenrick/hadoop-mutinode项目，不过我做了大量的优化和重构。alvinhenrick/hadoop-mutinode项目的Github主页以及作者所写的博客地址：[GitHub](https://github.com/alvinhenrick/hadoop-mutinode)，[博客](http://alvinhenrick.com/2014/07/16/hadoop-yarn-multinode-cluster-with-docker/)\n\n下面两个表是alvinhenrick/hadoop-mutinode项目与我的kiwenlau/hadoop-cluster-docker项目的参数对比\n\n|镜像名称\t                                 |构建时间\t       | 镜像层数   | 镜像大小 |\n| :-------------------------| :---------- | :------- | :------- |\n|alvinhenrick/serf                     |258.213s       | 21\t        | 239.4MB |\n|alvinhenrick/hadoop-base\t| 2236.055s    | 58\t        | 4.328GB |\n|alvinhenrick/hadoop-dn\t        | 51.959s        | 74\t        | 4.331GB |\n|alvinhenrick/hadoop-nn-dn    | 49.548s       |  84           | 4.331GB |\n\n|镜像名称\t                                 |构建时间\t       | 镜像层数   | 镜像大小 |\n| :-------------------------| :---------- | :------- | :------- |\n| kiwenlau/serf-dnsmasq          | 509.46s        |  8\t        | 206.6 MB |\n|kiwenlau/hadoop-base\t         | 400.29s\t        |  7\t        | 775.4 MB |\n|kiwenlau/hadoop-master         | 5.41s            |  9\t        | 775.4 MB |\n|kiwenlau/hadoop-slave\t         | 2.41s\t        |  8 \t        | 775.4 MB |\n\n可知，我主要优化了这样几点\n- 更小的镜像大小\n- 更快的构造时间\n- 更少的镜像层数\n\n####更快更方便地改变Hadoop集群节点数目\n\n另外，alvinhenrick/hadoop-mutinode项目增加节点时需要手动修改Hadoop配置文件然后重新构建hadoop-nn-dn镜像,然后修改容器启动脚本，才能实现增加节点的功能。而我通过shell脚本实现自动话，不到1分钟可以重新构建hadoop-master镜像，然后立即运行！！！本项目默认启动3个节点的Hadoop集群，支持任意节点数的hadoop集群。\n\n另外，启动hadoop, 运行wordcount以及重新构建镜像都采用了shell脚本实现自动化。这样使得整个项目的使用以及开发都变得非常方便快捷:)\n\n####开发测试环境\n\n- 操作系统：ubuntu 14.04 和 ubuntu 12.04\n- 内核版本: 3.13.0-32-generic\n- Docker版本：1.5.0 和1.6.2\n\n####硬盘不够，内存不够，尤其是内核版本过低会导致运行失败:(\n\n##二. 镜像简介\n\n###本项目一共开发了4个镜像\n\n- serf-dnsmasq\n- hadoop-base\n- hadoop-master\n- hadoop-slave\n\n###serf-dnsmasq镜像\n\n- 基于ubuntu:15.04 (选它是因为它最小，不是因为它最新...)\n- 安装serf: serf是一个分布式的机器节点管理工具。它可以动态地发现所有hadoop集群节点。\n- 安装dnsmasq: dnsmasq作为轻量级的dns服务器。它可以为hadoop集群提供域名解析服务。\n\n容器启动时，master节点的IP会传给所有slave节点。serf会在container启动后立即启动。slave节点上的serf agent会马上发现master节点（master IP它们都知道嘛），master节点就马上发现了所有slave节点。然后它们之间通过互相交换信息，所有节点就能知道其他所有节点的存在了！(Everyone will know Everyone). serf发现新的节点时，就会重新配置dnsmasq,然后重启dnsmasq. 所以dnsmasq就能够解析集群的所有节点的域名啦。这个过程随着节点的增加会耗时更久，因此，若配置的Hadoop节点比较多，则在启动容器后需要测试serf是否发现了所有节点，dns是否能够解析所有节点域名。稍等片刻才能启动Hadoop。这个解决方案是由SequenceIQ公司提出的，该公司专注于将Hadoop运行在Docker中。请参考这个PPT：[Docker-based Hadoop Provisioning](http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning)\n\n###hadoop-base镜像\n\n- 基于serf-dnsmasq镜像\n- 安装JDK(openjdk)\n- 安装openssh-server, 配置无密码ssh\n- 安装vim：介样就可以愉快地在容器中敲代码了:)\n- 安装Hadoop 2.3.0: 安装编译过的hadoop （2.5.2， 2.6.0， 2.7.0 都比2.3.0大，所以我懒得升级了）\n\n编译Hadoop的步骤请参考我的博客：[[Hadoop 2.30 在Ubuntu 14.04 中编译](http://www.cnblogs.com/kiwenlau/p/4227204.html)](http://www.cnblogs.com/kiwenlau/p/4227204.html)\n\n如果需要重新开发我的hadoop-base, 需要下载编译过的hadoop-2.3.0安装包，放到hadoop-cluster-docker/hadoop-base/files目录内。我编译的64位hadoop-2.3.0下载地址：[hadoop-2.3.0](http://pan.baidu.com/s/1sjFRaFz)\n\n另外，我还编译了64位的hadoop 2.5.2, 2.6.0, 2.7.0, 其下载地址如下：\n\n- [hadoop-2.3.0](http://pan.baidu.com/s/1sjFRaFz)\n- [hadoop-2.5.2](http://pan.baidu.com/s/1jGw24aa)\n- [hadoop-2.6.0](http://pan.baidu.com/s/1eQgvF2M)\n- [hadoop-2.7.0]( http://pan.baidu.com/s/1c0HD0Nu)\n\n###hadoop-master镜像\n\n- 基于hadoop-base镜像\n- 配置hadoop的master节点\n- 格式化namenode\n\n这一步需要配置slaves文件，而slaves文件需要列出所有节点的域名或者IP。因此，Hadoop节点数目不同时，slaves文件自然也不一样。因此，更改Hadoop集群节点数目时，需要修改slaves文件然后重新构建hadoop-master镜像。我编写了一个resize-cluster.sh脚本自动化这一过程。仅需给定节点数目作为脚本参数就可以轻松实现Hadoop集群节点数目的更改。由于hadoop-master镜像仅仅做一些配置工作，也无需下载任何文件，整个过程非常快，1分钟就足够了。\n\n###hadoop-slave镜像\n\n- 基于hadoop-base镜像\n- 配置hadoop的slave节点\n\n###镜像大小分析\n\n下表为sudo docker images的运行结果\n\n|REPOSITORY       |   TAG    |  IMAGE ID       | CREATED     |   VIRTUAL SIZE |\n| ------------- | ------- | ---------- | ---------- | ------- |\n|index.alauda.cn/kiwenlau/hadoop-slave   | 0.1.0|    d63869855c03 |   17 hours ago  |  777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-master|    0.1.0   | 7c9d32ede450  |  17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-base  |    0.1.0   | 5571bd5de58e    |17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/serf-dnsmasq  |  0.1.0    |09ed89c24ee8   | 17 hours ago  |  206.7 MB |\n|ubuntu     |                               15.04   | bd94ae587483  |  3 weeks ago    |131.3 MB |\n\n\n易知以下几个结论：\n- serf-dnsmasq镜像在ubuntu:15.04镜像的基础上增加了75.4MB\n- hadoop-base镜像在serf-dnsmasq镜像的基础上增加了570.7MB\n- hadoop-master和hadoop-slave镜像在hadoop-base镜像的基础上大小几乎没有增加\n\n下表为docker history index.alauda.cn/kiwenlau/hadoop-base:0.1.0命令的部分运行结果\n\n|IMAGE      |    CREATED        |    CREATED BY         |                             SIZE\n |  -----  | ---------------  | ---------------  |  -----------  | \n|2039b9b81146 |   44 hours ago   |       /bin/sh -c #(nop) ADD   multi:a93c971a49514e787  |  158.5 MB | \n | cdb620312f30    |  44 hours ago   |       /bin/sh -c apt-get install -y openjdk-7-jdk    |  324.6 MB  | \n | da7d10c790c1   |   44 hours ago      |    /bin/sh -c apt-get install -y openssh-server   |   87.58 MB  | \n |  c65cb568defc    |  44 hours ago     |     /bin/sh -c curl -Lso serf.zip https://dl.bint  |  14.46 MB  | \n | 3e22b3d72e33     | 44 hours ago       |   /bin/sh -c apt-get update && apt-get install     |  60.89 MB   | \n |  b68f8c8d2140    |  3 weeks ago     |     /bin/sh -c #(nop) ADD file:d90f7467c470bfa9a3  |   131.3 MB  | \n\n可知\n- 基础镜像ubuntu:15.04为131.3MB\n- 安装openjdk需要324.6MB\n- 安装hadoop需要158.5MB\n- ubuntu,openjdk与hadoop均为镜像所必须，三者一共占了:614.4MB\n\n### 因此，我所开发的hadoop镜像以及接近最小，优化空间已经很小了\n\n下图显示了项目的Docker镜像结构：\n\n![](/image/150608/image architecture.jpg \"Image Architecture\")\n\n##三. 3节点Hadoop集群搭建步骤\n\n###1. 拉取镜像\n\n```sh\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-master:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-slave:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-base:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/serf-dnsmasq:0.1.0\n```\n\n- 3~5分钟OK~\n\n*查看下载的镜像*\n\n```sh\nsudo docker images\n```\n\n*运行结果*\n\n|REPOSITORY    |    TAG  |    IMAGE ID      |  CREATED  |      VIRTUAL SIZE |\n |  ----------  |  -----------  |  ---------  |   --------  |  \n| index.alauda.cn/kiwenlau/hadoop-slave   | 0.1.0    |d63869855c03  |  17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-master   |   0.1.0 |     7c9d32ede450   |   17 hours ago  |    777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-base |       0.1.0     | 5571bd5de58e   |   17 hours ago  |    777.4 MB | \n |  index.alauda.cn/kiwenlau/serf-dnsmasq   |   0.1.0   |   09ed89c24ee8     | 17 hours ago     |  206.7 MB | \n\n- hadoop-base镜像是基于serf-dnsmasq镜像的，hadoop-slave镜像和hadoop-master镜像都是基于hadoop-base镜像\n- 所以其实4个镜像一共也就777.4MB:)\n\n###2. 修改镜像tag\n\n```sh\nsudo docker tag d63869855c03 kiwenlau/hadoop-slave:0.1.0\nsudo docker tag 7c9d32ede450 kiwenlau/hadoop-master:0.1.0\nsudo docker tag 5571bd5de58e kiwenlau/hadoop-base:0.1.0\nsudo docker tag 09ed89c24ee8 kiwenlau/serf-dnsmasq:0.1.0\n```\n\n*查看修改tag后镜像*\n\n```sh\nsudo docker images\n```\n\n*运行结果*\n\n| REPOSITORY   |   TAG   |    IMAGE ID    |    CREATED    |      VIRTUAL SIZE  | \n |  ----------  |  -----  |  ---------  |  ----------  |  -----------  | \n| index.alauda.cn/kiwenlau/hadoop-slave  |    0.1.0   |   d63869855c03    |  17 hours ago   |   777.4 MB\n | kiwenlau/hadoop-slave    |                  0.1.0   |   d63869855c03   |   17 hours ago  |    777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-master  |  0.1.0 |     7c9d32ede450   |   17 hours ago |     777.4 MB | \n | kiwenlau/hadoop-master  |                  0.1.0   |   7c9d32ede450   |   17 hours ago |     777.4 MB | \n | kiwenlau/hadoop-base   |                   0.1.0  |    5571bd5de58e  |    17 hours ago   |   777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-base  |    0.1.0   |   5571bd5de58e   |   17 hours ago |     777.4 MB | \n | kiwenlau/serf-dnsmasq          |            0.1.0    |  09ed89c24ee8  |    17 hours ago    |  206.7 MB | \n | index.alauda.cn/kiwenlau/serf-dnsmasq  |    0.1.0   |   09ed89c24ee8     | 17 hours ago   |   206.7 MB | \n\n- 之所以要修改镜像，是因为我默认是将镜像上传到Dockerhub, 因此Dokerfile以及shell脚本中得镜像名称都是没有alauada前缀的，sorry for this....不过改tag还是很快滴\n- 若直接下载我在DockerHub中的镜像，自然就不需要修改tag...不过Alauda镜像下载速度很快的哈~\n\n###3.下载源代码\n\n```sh\ngit clone https://github.com/kiwenlau/hadoop-cluster-docker\n```\n\n- 为了防止Github被XX, 我把代码导入到了开源中国的git仓库\n\n```sh\ngit clone http://git.oschina.net/kiwenlau/hadoop-cluster-docker\n```\n\n###4. 运行容器\n\n```sh\ncd hadoop-cluster-docker\n./start-container.sh\n\n```\n\n*运行结果*\n\n```bash\nstart master container...\nstart slave1 container...\nstart slave2 container...\nroot@master:~#\n```\n\n- 一共开启了3个容器，1个master, 2个slave\n- 开启容器后就进入了master容器root用户的家目录（/root）\n\n*查看master的root用户家目录的文件*\n\n```sh\nls\n```\n\n*运行结果*\n\n```plain\nhdfs  run-wordcount.sh\tserf_log  start-hadoop.sh  start-ssh-serf.sh\n```\n\n- start-hadoop.sh是开启hadoop的shell脚本\n- run-wordcount.sh是运行wordcount的shell脚本，可以测试镜像是否正常工作\n\n###5.测试容器是否正常启动(此时已进入master容器)\n\n*查看hadoop集群成员*\n\n```sh\nserf members\n```\n\n*运行结果*\n\n```bash\nmaster.kiwenlau.com  172.17.0.65:7946  alive\nslave1.kiwenlau.com  172.17.0.66:7946  alive\nslave2.kiwenlau.com  172.17.0.67:7946  alive\n```\n\n- 若结果缺少节点，可以稍等片刻，再执行“serf members”命令。因为serf agent需要时间发现所有节点。\n\n*测试ssh*\n\n```sh\nssh slave2.kiwenlau.com\n```\n\n*运行结果*\n\n```bash\nWarning: Permanently added 'slave2.kiwenlau.com,172.17.0.67' (ECDSA) to the list of known hosts.\nWelcome to Ubuntu 15.04 (GNU/Linux 3.13.0-53-generic x86_64)\n* Documentation:  https://help.ubuntu.com/\nThe programs included with the Ubuntu system are free software;\nthe exact distribution terms for each program are described in the\nindividual files in /usr/share/doc/*/copyright.\nUbuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\napplicable law.\nroot@slave2:~#\n```\n\n*退出slave2*\n\n```sh\nexit\n```\n\n*运行结果*\n\n```bash\nlogout\nConnection to slave2.kiwenlau.com closed.\n```\n\n- 若ssh失败，请稍等片刻再测试，因为dnsmasq的dns服务器启动需要时间。\n- 测试成功后，就可以开启Hadoop集群了！其实你也可以不进行测试，开启容器后耐心等待一分钟即可！\n\n###6. 开启hadoop\n\n```sh\n./start-hadoop.sh\n```\n\n- 上一步ssh到slave2之后，请记得回到master啊!!！\n- 运行结果太多，忽略....\n- hadoop的启动速度取决于机器性能....\n\n###7. 运行wordcount\n\n```sh\n./run-wordcount.sh\n```\n\n*运行结果*\n```bash\ninput file1.txt:\nHello Hadoop\ninput file2.txt:\nHello Docker\nwordcount output:\nDocker\t1\nHadoop\t1\nHello\t2\n```\n\n- wordcount的执行速度取决于机器性能....\n\n##四. N节点Hadoop集群搭建步骤\n\n###1. 准备工作\n\n- 参考第二部分1~3：下载镜像，修改tag，下载源代码\n- 注意，你可以不下载serf-dnsmasq, 但是请最好下载hadoop-base，因为hadoop-master是基于hadoop-base构建的\n\n###2. 重新构建hadoop-master镜像\n\n```sh\n./resize-cluster.sh 5\n```\n\n- 不要担心，1分钟就能搞定\n- 你可以为resize-cluster.sh脚本设不同的正整数作为参数数1, 2, 3, 4, 5, 6...\n\n###3. 启动容器\n\n```sh\n./start-container.sh 5\n```\n\n- 你可以为resize-cluster.sh脚本设不同的正整数作为参数数1, 2, 3, 4, 5, 6...\n- 这个参数呢，最好还是得和上一步的参数一致:)\n- 这个参数如果比上一步的参数大，你多启动的节点，Hadoop不认识它们..\n- 这个参数如果比上一步的参数小，Hadoop觉得少启动的节点挂掉了..\n\n###4. 测试工作\n\n- 参考第三部分5~7：测试容器，开启Hadoop，运行wordcount\n- 请注意，若节点增加，请务必先测试容器，然后再开启Hadoop, 因为serf可能还没有发现所有节点，而dnsmasq的DNS服务器表示还没有配置好服务\n- 测试等待时间取决于机器性能....\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/](http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/)\n***","source":"_posts/150608-hadoop-cluster-docker.md","raw":"title: 基于Docker搭建多节点Hadoop集群\n\ndate: 2015-06-08 12:44:40\n\ntags: [Hadoop, Docker]\n\n---\n\n**摘要:** 本文介绍了基于Docker在单机上搭建多节点Hadopp集群方法，Hadoop的Master和Slave分别运行在不同容器中。\n\n**GitHub地址:**\n- [kiwenlau/hadoop-cluster-docker](https://github.com/kiwenlau)\n\n<!-- more -->\n\n可以直接进入第三部分，快速在本机搭建一个3个节点的Hadoop集群\n\n\n##一. 项目简介\n\n直接用机器搭建Hadoop集群是一个相当痛苦的过程，尤其对初学者来说。他们还没开始跑wordcount，可能就被这个问题折腾的体无完肤了。\n\n我的目标是将Hadoop集群运行在Docker容器中，使Hadoop开发者能够快速便捷地在本机搭建多节点的Hadoop集群。其实这个想法已经有了不少实现，但是都不是很理想，他们或者镜像太大，或者使用太慢，或者使用了第三方工具使得使用起来过于复杂...下表为一些已知的Hadoop on Docker项目以及其存在的问题。\n\n\n|                  项目                             | 镜像大小    |        问题                        |\n| : ------------- ---------------| : ------ | :-------------------- |\n|sequenceiq/hadoop-docker:latest  |1.491GB    | 镜像太大，只支持单个节点|\n|sequenceiq/hadoop-docker:2.7.0  |1.76 GB     |           同上                           |\n|sequenceiq/hadoop-docker:2.6.0  |1.624GB    |         同上                            |\n|sequenceiq/ambari:latest               |1.782GB     |  镜像太大，使用太慢|\n|sequenceiq/ambari:2.0.0               |4.804GB     |          同上                           |\n|sequenceiq/ambari:latest:1.70      |4.761GB    |         同上                         |\n|alvinhenrick/hadoop-mutinode     |4.331GB    |镜像太大，构建太慢，增加节点麻烦，有bug\n\n我的项目参考了alvinhenrick/hadoop-mutinode项目，不过我做了大量的优化和重构。alvinhenrick/hadoop-mutinode项目的Github主页以及作者所写的博客地址：[GitHub](https://github.com/alvinhenrick/hadoop-mutinode)，[博客](http://alvinhenrick.com/2014/07/16/hadoop-yarn-multinode-cluster-with-docker/)\n\n下面两个表是alvinhenrick/hadoop-mutinode项目与我的kiwenlau/hadoop-cluster-docker项目的参数对比\n\n|镜像名称\t                                 |构建时间\t       | 镜像层数   | 镜像大小 |\n| :-------------------------| :---------- | :------- | :------- |\n|alvinhenrick/serf                     |258.213s       | 21\t        | 239.4MB |\n|alvinhenrick/hadoop-base\t| 2236.055s    | 58\t        | 4.328GB |\n|alvinhenrick/hadoop-dn\t        | 51.959s        | 74\t        | 4.331GB |\n|alvinhenrick/hadoop-nn-dn    | 49.548s       |  84           | 4.331GB |\n\n|镜像名称\t                                 |构建时间\t       | 镜像层数   | 镜像大小 |\n| :-------------------------| :---------- | :------- | :------- |\n| kiwenlau/serf-dnsmasq          | 509.46s        |  8\t        | 206.6 MB |\n|kiwenlau/hadoop-base\t         | 400.29s\t        |  7\t        | 775.4 MB |\n|kiwenlau/hadoop-master         | 5.41s            |  9\t        | 775.4 MB |\n|kiwenlau/hadoop-slave\t         | 2.41s\t        |  8 \t        | 775.4 MB |\n\n可知，我主要优化了这样几点\n- 更小的镜像大小\n- 更快的构造时间\n- 更少的镜像层数\n\n####更快更方便地改变Hadoop集群节点数目\n\n另外，alvinhenrick/hadoop-mutinode项目增加节点时需要手动修改Hadoop配置文件然后重新构建hadoop-nn-dn镜像,然后修改容器启动脚本，才能实现增加节点的功能。而我通过shell脚本实现自动话，不到1分钟可以重新构建hadoop-master镜像，然后立即运行！！！本项目默认启动3个节点的Hadoop集群，支持任意节点数的hadoop集群。\n\n另外，启动hadoop, 运行wordcount以及重新构建镜像都采用了shell脚本实现自动化。这样使得整个项目的使用以及开发都变得非常方便快捷:)\n\n####开发测试环境\n\n- 操作系统：ubuntu 14.04 和 ubuntu 12.04\n- 内核版本: 3.13.0-32-generic\n- Docker版本：1.5.0 和1.6.2\n\n####硬盘不够，内存不够，尤其是内核版本过低会导致运行失败:(\n\n##二. 镜像简介\n\n###本项目一共开发了4个镜像\n\n- serf-dnsmasq\n- hadoop-base\n- hadoop-master\n- hadoop-slave\n\n###serf-dnsmasq镜像\n\n- 基于ubuntu:15.04 (选它是因为它最小，不是因为它最新...)\n- 安装serf: serf是一个分布式的机器节点管理工具。它可以动态地发现所有hadoop集群节点。\n- 安装dnsmasq: dnsmasq作为轻量级的dns服务器。它可以为hadoop集群提供域名解析服务。\n\n容器启动时，master节点的IP会传给所有slave节点。serf会在container启动后立即启动。slave节点上的serf agent会马上发现master节点（master IP它们都知道嘛），master节点就马上发现了所有slave节点。然后它们之间通过互相交换信息，所有节点就能知道其他所有节点的存在了！(Everyone will know Everyone). serf发现新的节点时，就会重新配置dnsmasq,然后重启dnsmasq. 所以dnsmasq就能够解析集群的所有节点的域名啦。这个过程随着节点的增加会耗时更久，因此，若配置的Hadoop节点比较多，则在启动容器后需要测试serf是否发现了所有节点，dns是否能够解析所有节点域名。稍等片刻才能启动Hadoop。这个解决方案是由SequenceIQ公司提出的，该公司专注于将Hadoop运行在Docker中。请参考这个PPT：[Docker-based Hadoop Provisioning](http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning)\n\n###hadoop-base镜像\n\n- 基于serf-dnsmasq镜像\n- 安装JDK(openjdk)\n- 安装openssh-server, 配置无密码ssh\n- 安装vim：介样就可以愉快地在容器中敲代码了:)\n- 安装Hadoop 2.3.0: 安装编译过的hadoop （2.5.2， 2.6.0， 2.7.0 都比2.3.0大，所以我懒得升级了）\n\n编译Hadoop的步骤请参考我的博客：[[Hadoop 2.30 在Ubuntu 14.04 中编译](http://www.cnblogs.com/kiwenlau/p/4227204.html)](http://www.cnblogs.com/kiwenlau/p/4227204.html)\n\n如果需要重新开发我的hadoop-base, 需要下载编译过的hadoop-2.3.0安装包，放到hadoop-cluster-docker/hadoop-base/files目录内。我编译的64位hadoop-2.3.0下载地址：[hadoop-2.3.0](http://pan.baidu.com/s/1sjFRaFz)\n\n另外，我还编译了64位的hadoop 2.5.2, 2.6.0, 2.7.0, 其下载地址如下：\n\n- [hadoop-2.3.0](http://pan.baidu.com/s/1sjFRaFz)\n- [hadoop-2.5.2](http://pan.baidu.com/s/1jGw24aa)\n- [hadoop-2.6.0](http://pan.baidu.com/s/1eQgvF2M)\n- [hadoop-2.7.0]( http://pan.baidu.com/s/1c0HD0Nu)\n\n###hadoop-master镜像\n\n- 基于hadoop-base镜像\n- 配置hadoop的master节点\n- 格式化namenode\n\n这一步需要配置slaves文件，而slaves文件需要列出所有节点的域名或者IP。因此，Hadoop节点数目不同时，slaves文件自然也不一样。因此，更改Hadoop集群节点数目时，需要修改slaves文件然后重新构建hadoop-master镜像。我编写了一个resize-cluster.sh脚本自动化这一过程。仅需给定节点数目作为脚本参数就可以轻松实现Hadoop集群节点数目的更改。由于hadoop-master镜像仅仅做一些配置工作，也无需下载任何文件，整个过程非常快，1分钟就足够了。\n\n###hadoop-slave镜像\n\n- 基于hadoop-base镜像\n- 配置hadoop的slave节点\n\n###镜像大小分析\n\n下表为sudo docker images的运行结果\n\n|REPOSITORY       |   TAG    |  IMAGE ID       | CREATED     |   VIRTUAL SIZE |\n| ------------- | ------- | ---------- | ---------- | ------- |\n|index.alauda.cn/kiwenlau/hadoop-slave   | 0.1.0|    d63869855c03 |   17 hours ago  |  777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-master|    0.1.0   | 7c9d32ede450  |  17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-base  |    0.1.0   | 5571bd5de58e    |17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/serf-dnsmasq  |  0.1.0    |09ed89c24ee8   | 17 hours ago  |  206.7 MB |\n|ubuntu     |                               15.04   | bd94ae587483  |  3 weeks ago    |131.3 MB |\n\n\n易知以下几个结论：\n- serf-dnsmasq镜像在ubuntu:15.04镜像的基础上增加了75.4MB\n- hadoop-base镜像在serf-dnsmasq镜像的基础上增加了570.7MB\n- hadoop-master和hadoop-slave镜像在hadoop-base镜像的基础上大小几乎没有增加\n\n下表为docker history index.alauda.cn/kiwenlau/hadoop-base:0.1.0命令的部分运行结果\n\n|IMAGE      |    CREATED        |    CREATED BY         |                             SIZE\n |  -----  | ---------------  | ---------------  |  -----------  | \n|2039b9b81146 |   44 hours ago   |       /bin/sh -c #(nop) ADD   multi:a93c971a49514e787  |  158.5 MB | \n | cdb620312f30    |  44 hours ago   |       /bin/sh -c apt-get install -y openjdk-7-jdk    |  324.6 MB  | \n | da7d10c790c1   |   44 hours ago      |    /bin/sh -c apt-get install -y openssh-server   |   87.58 MB  | \n |  c65cb568defc    |  44 hours ago     |     /bin/sh -c curl -Lso serf.zip https://dl.bint  |  14.46 MB  | \n | 3e22b3d72e33     | 44 hours ago       |   /bin/sh -c apt-get update && apt-get install     |  60.89 MB   | \n |  b68f8c8d2140    |  3 weeks ago     |     /bin/sh -c #(nop) ADD file:d90f7467c470bfa9a3  |   131.3 MB  | \n\n可知\n- 基础镜像ubuntu:15.04为131.3MB\n- 安装openjdk需要324.6MB\n- 安装hadoop需要158.5MB\n- ubuntu,openjdk与hadoop均为镜像所必须，三者一共占了:614.4MB\n\n### 因此，我所开发的hadoop镜像以及接近最小，优化空间已经很小了\n\n下图显示了项目的Docker镜像结构：\n\n![](/image/150608/image architecture.jpg \"Image Architecture\")\n\n##三. 3节点Hadoop集群搭建步骤\n\n###1. 拉取镜像\n\n```sh\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-master:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-slave:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/hadoop-base:0.1.0\nsudo docker pull index.alauda.cn/kiwenlau/serf-dnsmasq:0.1.0\n```\n\n- 3~5分钟OK~\n\n*查看下载的镜像*\n\n```sh\nsudo docker images\n```\n\n*运行结果*\n\n|REPOSITORY    |    TAG  |    IMAGE ID      |  CREATED  |      VIRTUAL SIZE |\n |  ----------  |  -----------  |  ---------  |   --------  |  \n| index.alauda.cn/kiwenlau/hadoop-slave   | 0.1.0    |d63869855c03  |  17 hours ago   | 777.4 MB|\n|index.alauda.cn/kiwenlau/hadoop-master   |   0.1.0 |     7c9d32ede450   |   17 hours ago  |    777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-base |       0.1.0     | 5571bd5de58e   |   17 hours ago  |    777.4 MB | \n |  index.alauda.cn/kiwenlau/serf-dnsmasq   |   0.1.0   |   09ed89c24ee8     | 17 hours ago     |  206.7 MB | \n\n- hadoop-base镜像是基于serf-dnsmasq镜像的，hadoop-slave镜像和hadoop-master镜像都是基于hadoop-base镜像\n- 所以其实4个镜像一共也就777.4MB:)\n\n###2. 修改镜像tag\n\n```sh\nsudo docker tag d63869855c03 kiwenlau/hadoop-slave:0.1.0\nsudo docker tag 7c9d32ede450 kiwenlau/hadoop-master:0.1.0\nsudo docker tag 5571bd5de58e kiwenlau/hadoop-base:0.1.0\nsudo docker tag 09ed89c24ee8 kiwenlau/serf-dnsmasq:0.1.0\n```\n\n*查看修改tag后镜像*\n\n```sh\nsudo docker images\n```\n\n*运行结果*\n\n| REPOSITORY   |   TAG   |    IMAGE ID    |    CREATED    |      VIRTUAL SIZE  | \n |  ----------  |  -----  |  ---------  |  ----------  |  -----------  | \n| index.alauda.cn/kiwenlau/hadoop-slave  |    0.1.0   |   d63869855c03    |  17 hours ago   |   777.4 MB\n | kiwenlau/hadoop-slave    |                  0.1.0   |   d63869855c03   |   17 hours ago  |    777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-master  |  0.1.0 |     7c9d32ede450   |   17 hours ago |     777.4 MB | \n | kiwenlau/hadoop-master  |                  0.1.0   |   7c9d32ede450   |   17 hours ago |     777.4 MB | \n | kiwenlau/hadoop-base   |                   0.1.0  |    5571bd5de58e  |    17 hours ago   |   777.4 MB | \n | index.alauda.cn/kiwenlau/hadoop-base  |    0.1.0   |   5571bd5de58e   |   17 hours ago |     777.4 MB | \n | kiwenlau/serf-dnsmasq          |            0.1.0    |  09ed89c24ee8  |    17 hours ago    |  206.7 MB | \n | index.alauda.cn/kiwenlau/serf-dnsmasq  |    0.1.0   |   09ed89c24ee8     | 17 hours ago   |   206.7 MB | \n\n- 之所以要修改镜像，是因为我默认是将镜像上传到Dockerhub, 因此Dokerfile以及shell脚本中得镜像名称都是没有alauada前缀的，sorry for this....不过改tag还是很快滴\n- 若直接下载我在DockerHub中的镜像，自然就不需要修改tag...不过Alauda镜像下载速度很快的哈~\n\n###3.下载源代码\n\n```sh\ngit clone https://github.com/kiwenlau/hadoop-cluster-docker\n```\n\n- 为了防止Github被XX, 我把代码导入到了开源中国的git仓库\n\n```sh\ngit clone http://git.oschina.net/kiwenlau/hadoop-cluster-docker\n```\n\n###4. 运行容器\n\n```sh\ncd hadoop-cluster-docker\n./start-container.sh\n\n```\n\n*运行结果*\n\n```bash\nstart master container...\nstart slave1 container...\nstart slave2 container...\nroot@master:~#\n```\n\n- 一共开启了3个容器，1个master, 2个slave\n- 开启容器后就进入了master容器root用户的家目录（/root）\n\n*查看master的root用户家目录的文件*\n\n```sh\nls\n```\n\n*运行结果*\n\n```plain\nhdfs  run-wordcount.sh\tserf_log  start-hadoop.sh  start-ssh-serf.sh\n```\n\n- start-hadoop.sh是开启hadoop的shell脚本\n- run-wordcount.sh是运行wordcount的shell脚本，可以测试镜像是否正常工作\n\n###5.测试容器是否正常启动(此时已进入master容器)\n\n*查看hadoop集群成员*\n\n```sh\nserf members\n```\n\n*运行结果*\n\n```bash\nmaster.kiwenlau.com  172.17.0.65:7946  alive\nslave1.kiwenlau.com  172.17.0.66:7946  alive\nslave2.kiwenlau.com  172.17.0.67:7946  alive\n```\n\n- 若结果缺少节点，可以稍等片刻，再执行“serf members”命令。因为serf agent需要时间发现所有节点。\n\n*测试ssh*\n\n```sh\nssh slave2.kiwenlau.com\n```\n\n*运行结果*\n\n```bash\nWarning: Permanently added 'slave2.kiwenlau.com,172.17.0.67' (ECDSA) to the list of known hosts.\nWelcome to Ubuntu 15.04 (GNU/Linux 3.13.0-53-generic x86_64)\n* Documentation:  https://help.ubuntu.com/\nThe programs included with the Ubuntu system are free software;\nthe exact distribution terms for each program are described in the\nindividual files in /usr/share/doc/*/copyright.\nUbuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\napplicable law.\nroot@slave2:~#\n```\n\n*退出slave2*\n\n```sh\nexit\n```\n\n*运行结果*\n\n```bash\nlogout\nConnection to slave2.kiwenlau.com closed.\n```\n\n- 若ssh失败，请稍等片刻再测试，因为dnsmasq的dns服务器启动需要时间。\n- 测试成功后，就可以开启Hadoop集群了！其实你也可以不进行测试，开启容器后耐心等待一分钟即可！\n\n###6. 开启hadoop\n\n```sh\n./start-hadoop.sh\n```\n\n- 上一步ssh到slave2之后，请记得回到master啊!!！\n- 运行结果太多，忽略....\n- hadoop的启动速度取决于机器性能....\n\n###7. 运行wordcount\n\n```sh\n./run-wordcount.sh\n```\n\n*运行结果*\n```bash\ninput file1.txt:\nHello Hadoop\ninput file2.txt:\nHello Docker\nwordcount output:\nDocker\t1\nHadoop\t1\nHello\t2\n```\n\n- wordcount的执行速度取决于机器性能....\n\n##四. N节点Hadoop集群搭建步骤\n\n###1. 准备工作\n\n- 参考第二部分1~3：下载镜像，修改tag，下载源代码\n- 注意，你可以不下载serf-dnsmasq, 但是请最好下载hadoop-base，因为hadoop-master是基于hadoop-base构建的\n\n###2. 重新构建hadoop-master镜像\n\n```sh\n./resize-cluster.sh 5\n```\n\n- 不要担心，1分钟就能搞定\n- 你可以为resize-cluster.sh脚本设不同的正整数作为参数数1, 2, 3, 4, 5, 6...\n\n###3. 启动容器\n\n```sh\n./start-container.sh 5\n```\n\n- 你可以为resize-cluster.sh脚本设不同的正整数作为参数数1, 2, 3, 4, 5, 6...\n- 这个参数呢，最好还是得和上一步的参数一致:)\n- 这个参数如果比上一步的参数大，你多启动的节点，Hadoop不认识它们..\n- 这个参数如果比上一步的参数小，Hadoop觉得少启动的节点挂掉了..\n\n###4. 测试工作\n\n- 参考第三部分5~7：测试容器，开启Hadoop，运行wordcount\n- 请注意，若节点增加，请务必先测试容器，然后再开启Hadoop, 因为serf可能还没有发现所有节点，而dnsmasq的DNS服务器表示还没有配置好服务\n- 测试等待时间取决于机器性能....\n\n***\n**版权声明**\n转载时请注明作者[KiwenLau](http://kiwenlau.com/)以及本文地址：\n[http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/](http://kiwenlau.com/2015/06/08/150608-hadoop-cluster-docker/)\n***","slug":"150608-hadoop-cluster-docker","published":1,"updated":"2016-06-05T14:43:33.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cipleoykn000rpkbhbj6wxqku"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cipleoyjq0001pkbhugjilnrv","tag_id":"cipleoyjt0002pkbhjeuuiuyr","_id":"cipleoyjv0003pkbh0cjmx4u8"},{"post_id":"cipleoyjx0004pkbhdmq59qlx","tag_id":"cipleoyjy0005pkbhdg9mwwgf","_id":"cipleoyjz0007pkbhru7jp6h7"},{"post_id":"cipleoyjx0004pkbhdmq59qlx","tag_id":"cipleoyjy0006pkbh9bflnxv4","_id":"cipleoyjz0008pkbhxyly78bn"},{"post_id":"cipleoyk00009pkbhp7i6a68p","tag_id":"cipleoyjy0005pkbhdg9mwwgf","_id":"cipleoyk1000apkbh0gufcvug"},{"post_id":"cipleoyk00009pkbhp7i6a68p","tag_id":"cipleoyjy0006pkbh9bflnxv4","_id":"cipleoyk1000bpkbhtextk50x"},{"post_id":"cipleoyk9000cpkbhqvrhyio1","tag_id":"cipleoyjy0006pkbh9bflnxv4","_id":"cipleoyka000dpkbh7xgx387e"},{"post_id":"cipleoykc000epkbhfhry929z","tag_id":"cipleoyjy0005pkbhdg9mwwgf","_id":"cipleoykc000gpkbhihy6ekan"},{"post_id":"cipleoykc000epkbhfhry929z","tag_id":"cipleoykc000fpkbhfnf81glg","_id":"cipleoykc000hpkbhbkce6w0k"},{"post_id":"cipleoyke000ipkbhti3cuger","tag_id":"cipleoyjy0005pkbhdg9mwwgf","_id":"cipleoykg000jpkbh24x6ky35"},{"post_id":"cipleoyke000ipkbhti3cuger","tag_id":"cipleoykc000fpkbhfnf81glg","_id":"cipleoykh000kpkbh4lhycyuy"},{"post_id":"cipleoykk000lpkbhibtl23j7","tag_id":"cipleoyjy0005pkbhdg9mwwgf","_id":"cipleoykm000opkbhjwh3t7v2"},{"post_id":"cipleoykk000lpkbhibtl23j7","tag_id":"cipleoykl000mpkbhyxg3b5ey","_id":"cipleoykm000ppkbhsm4sftps"},{"post_id":"cipleoykk000lpkbhibtl23j7","tag_id":"cipleoykm000npkbhu4vi30z0","_id":"cipleoykm000qpkbhuvyn3x83"},{"post_id":"cipleoykn000rpkbhbj6wxqku","tag_id":"cipleoyjy0006pkbh9bflnxv4","_id":"cipleoyko000spkbhw4xqajwo"},{"post_id":"cipleoykn000rpkbhbj6wxqku","tag_id":"cipleoyjy0005pkbhdg9mwwgf","_id":"cipleoyko000tpkbhqf6i8ad6"}],"Tag":[{"name":"Vagrant","_id":"cipleoyjt0002pkbhjeuuiuyr"},{"name":"Docker","_id":"cipleoyjy0005pkbhdg9mwwgf"},{"name":"Hadoop","_id":"cipleoyjy0006pkbh9bflnxv4"},{"name":"Kubernetes","_id":"cipleoykc000fpkbhfnf81glg"},{"name":"Mesos","_id":"cipleoykl000mpkbhyxg3b5ey"},{"name":"Marathon","_id":"cipleoykm000npkbhu4vi30z0"}]}}